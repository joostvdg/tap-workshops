{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HOME","text":"<p>Welcome.</p> <p>For the Tanzu Solution Engineer workshops, visit the E2E Demo Portal.</p>"},{"location":"#todo","title":"TODO","text":"<ul> <li>ask people for public keys -&gt; so we can upload them to the jump hosts</li> <li>generate google sheet with pre-defined groups</li> <li></li> </ul>"},{"location":"#dockerhub-proxy","title":"DockerHub Proxy","text":"<ul> <li>In Harbor -&gt; <code>harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/</code></li> </ul> <p>So instead of <code>alpine</code> -&gt; <code>harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/library/alpine</code></p> <pre><code>docker pull harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/library/alpine\n</code></pre>"},{"location":"#workshops","title":"Workshops","text":"<ul> <li>Installations<ul> <li>Full &amp; OOTB Basic</li> <li>GitOps</li> </ul> </li> <li>Workloads<ul> <li>Hello World</li> <li>External Services</li> </ul> </li> <li>Supply Chains<ul> <li>Custom Supply Chain</li> <li>Upgrade OOTB Basic to OOTB Test &amp; Scanning</li> </ul> </li> <li>TAP GUI<ul> <li>View Supply Chain</li> <li>Register App</li> <li>View App Live View</li> <li>View App API</li> </ul> </li> </ul>"},{"location":"#ideas","title":"Ideas","text":"<ul> <li>Create Application From Accelerator<ul> <li>TAP GUI</li> <li>VSCode Extension</li> </ul> </li> <li>Workload in local Git server<ul> <li>Register Workload in TAP GUI -&gt; App Live View</li> </ul> </li> <li>Workload that uses external services (Bitnami Services + Crossplane)<ul> <li>Register API</li> <li>API Gateway View</li> </ul> </li> <li>Install with GitOps</li> <li>Install Multicluster</li> <li>Install Scanning &amp; Testing Supply Chain</li> <li>Create Custom Supply Chain</li> <li></li> </ul>"},{"location":"#outcome-steps","title":"Outcome Steps","text":""},{"location":"#basic","title":"Basic","text":"<p>I can install Cluster Essentials I setup wildcard certs to be configured in TAP I can configure tap-values.yaml for a Full profile installation of TAP (with OOTB Basic) I manually installed TAP I'm able to manually setup a single developer namespace I'm able to do basic troubleshooting of a TAP Installation I can configure a public Git provider for TAP GUI in TAP Values I can configure DNS for TAP Endpoints I understand Kapp Controller and Secret Gen Controller in the context of TAP I know how to configure access to private container registries in TAP Values I have installed TAP on TKGx I have configured custom CA's for TAP</p> <p>To add: I know how to configure an integration to a private Git provider in TAP GUI I know how to configure access to private SCM repositories for TAP GUI.</p>"},{"location":"#hello-world","title":"Hello-world","text":"<p>I'm able to use an accelerator to generate a Web TAP Workload I applied a TAP Web Workload and register it within TAP GUI I'm able to do basic troubleshooting of a TAP Workload I've configured App Live View in TAP GUI for a TAP Web Workload I've configured Tanzu Build Service and it works for a Spring Boot Application to produce an Image I am able to view and verify a TAP Workload displays in properly in Supply Chain view of TAP GUI I know how to configure an integration to a private Git provider in TAP GUI I know how to configure access to private repositories (SCM and Container) for TAP Workloads I have configured custom CA's for TAP</p> <p>To add: I am able to view and verify results for the Security Analysis within TAP GUI</p>"},{"location":"#external-services","title":"External Services","text":"<p>I am able to register API Documentation for a Workload in TAP GUI I am able to stand up shared services and claim resources on a TAP Run Cluster I can confgure API Auto Registration within a Supply Chain for a Workload I can configure API Scoring and Validation within TAP GUI I can configure API Portal with TAP I can install Shared Services on a TAP Cluster (Services Toolkit) I can claim a shared service on a TAP Cluster (Service Bindings) I can use a postgres resource claim with sample Spring Boot Application Pet Clinic Accelerator Expose Accerators Endpoint for use in Tanzu Accelerator Plugins</p>"},{"location":"#upgrade-basic-to-test-scanning-supply-chain","title":"Upgrade Basic To Test &amp; Scanning Supply Chain","text":"<p>I performed an upgrade of TAP I've able to configure and install the OOTB Testing and Scanning Supply Chain and verify a Workload I am able to configure a Scan Policy for a Source and Image Scan I am able to create a Tekton Task to execute Unit Tests within a Supply Chain for a Developer Namespace I've customized the OOTB Supply Chains I can create custom Tekton Tasks in Supply Chains</p> <p>Maybe? I have installed Grype in an air-gapped environment</p>"},{"location":"#gitops-install","title":"GitOps Install","text":"<p>I am able to manage a TAP installation with GitOPs</p>"},{"location":"#custom-supply-chain","title":"Custom Supply Chain","text":"<p>I've written and implemented my own Custom Supply Chain on a Cluster</p>"},{"location":"#build-view-run","title":"Build &amp; View &amp; Run","text":"<p>I understand and have implemented the TAP Multi Cluster Reference Architecture Configure TAP GUI to read resources from multiple clusters</p>"},{"location":"#create-accelerator","title":"Create Accelerator","text":"<p>I can create a custom Accelerator and register it in TAP GUI</p>"},{"location":"#maybe","title":"Maybe","text":"<p>I understand TAP GUI Catalog System I can do a Live Debug for a Workload I can do a Live Update for a Workload I can install the Dev Tools for IDEs (VSCode and IntelliJ) I am able to configure Image Signing I am able to use Namespace Provisioner to manage Tenant Onboarding I have implemented SRE best practices for a TAP installation (Monitoring/Alerting/etc.)</p>"},{"location":"#todo_1","title":"TODO","text":"<p>I'm able to install Learning Center and use the sample workshop. I understand the different Workload Types available in TAP I can update TAP values and reconcile changes on the cluster. I have relocated TAP Images to my own container registry and understand the imgpkg utility. I can use the VMware compatibility matrix I'm able to gather requirements for a TAP Installation (Use the TAP Questionaire) I've able to configure and install the OOTB Testing Supply Chain and verify a Workload I've able to configure and install the OOTB Testing and Scanning Supply Chain and verify a Workload I'm able to configure different Scanners (Trivy, Snyk) in the OOTB Supply Chains I am able to configure a Scan Policy for a Source and Image Scan I am able to create a Tekton Task to execute Unit Tests within a Supply Chain for a Developer Namespace I know how to setup Authorization and Authentication of RBAC for a TAP Cluster I know how to configure an Auth Provider for TAP GUI access I know how to configure an external Postgres database for TAP GUI I'm able to configure an external Postgres Database for Metadata Store I'm able to configure TAP GUI to read from Metadata Store. I've configured and generated Tech Docs for TAP GUI I am able to view and verify results for the Security Analysis within TAP GUI I've installed Tilt CLI I can force reconcilation of TAP changes I understand GitOps I can configure Tanzu Telemetry I can configure App SSO and consume it within a sample app generated from an Accelerator from TAP GUI. I can configure TAP Values to use External Secrets Multi Language Support (.NET, Java, Node, etc.) of TAP I can configure TAP GUI TLS I can configure Metadata Store TLS I can configure App Live View TLS Configure Accelerators to read from a Gitops Repo I have installed TAP on a Public Cloud Provider (EKS, AKS, GKE) I understand the OOTB Convention Services provided with TAP I understand how to customize Cloud Native Runtimes (Knative) domains I can customize the look and feel of TAP GUI I've implemented role based access for TAP GUI I have integrated a Supply Chain with Jenkins I can configure caching of artifacts for workload in a Supply Chain I have implemented Pull Requests in a Supply Chain to manage delivery of a workload in a config repo I have implemented automation to reconcile a Deliverable from a Build Cluster to a Run Cluster to apply workloads I have configured custom accelerators to be reconciled with TAP GUI with GitOps I've written my own Convention Service for a Supply Chain I have configured custom namespace provisioned resources I have installed TBS in an airgapped environment I have installed Grype in an air-gapped environment I can do Capacity Planning and Sizing for TAP I've configured the TAS to TAP workload adaptor I have migrated a TAS Workload to TAP I have implemented an active/active architecture with TAP I have built a custom Multi-cluster architecture for TAP (Customize the profiles) I have rotated Certificates used by TAP I have restored TAP from a Backup I have implemented SRE best practices for a TAP installation (Monitoring/Alerting/etc.) I have implemented Cert monitoring I have written my own custom buildpack for TBS I have migrated datastores used by TAP Upgrade TBS outside of TAP upgrade Cycle I have integrated Tanzu Service Mesh with TAP I have implemented GitHub Actions for TBS I can practice a failover of TAP I have installed TAP on OpenShift</p>"},{"location":"#developer-goals","title":"Developer Goals","text":"<ul> <li>I'm able to deploy an app generated from an OOTB Accelerator<ul> <li>Outcome: I'm able to understand the basics of TAP as a developer</li> </ul> </li> <li>I can use Inner Loop with an app generated from an OOTB Accelerator on TAP <ul> <li>Outcome: I'm able to iterate my development/debugging of an app as a Developer running on an Iterate Cluster</li> </ul> </li> <li>I can use Outer Loop with an app generated from an OOTB Accelerator on TAP<ul> <li>Outcome:  I'm able to understand the path to production for an app running on TAP\"</li> </ul> </li> <li>I can deploy an application that connects to Services provided on TAP. I can write custom Accelerators.</li> </ul>"},{"location":"#links","title":"Links","text":""},{"location":"#examples","title":"Examples","text":"<ul> <li>Tanzu Labs US - Custom Cartographer Supply Chains</li> <li>TAP Application Accelerator samples</li> <li>VRabbi TAP GitOps</li> </ul>"},{"location":"#example-applications","title":"Example Applications","text":"<ul> <li>Where For Dinner - Main TAP demo application</li> <li>Spring Cloud Stream</li> <li>TAP - Open Telemetry For Applications demo</li> <li>Spring Cloud Demo</li> </ul>"},{"location":"#community-links","title":"Community Links","text":"<ul> <li>VRabbi - Whats New In 1.5</li> <li>VRabbi - TAP 1.5 GitOps</li> <li>VRabbi - Service Toolkit Dynamic Provisioning</li> </ul>"},{"location":"#other-links","title":"Other Links","text":"<ul> <li>Testcontainers with Tekton</li> <li>Tanzu Developer - Getting Started With Testcontainers</li> <li>CNCF Security TAG - Software Supply Chain Best Practices</li> <li>Miro Board with US based TAP Engagements</li> <li>Cartographer Lifecyle docs (useful for leveraging Tekton TaskRun)</li> <li>What Is Supply Chain Choreography, and Why Should You Care?</li> </ul>"},{"location":"labs/","title":"Labs","text":"<p>See the index for the Labs to do in recommended order.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"apps/external-service/","title":"TAP App with External Service","text":"","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#checks","title":"Checks","text":"<ul> <li> I am able to register API Documentation for a Workload in TAP GUI</li> <li> I am able to stand up shared services and claim resources on a TAP Run Cluster</li> <li> I can confgure API Auto Registration within a Supply Chain for a Workload</li> <li> I can configure API Scoring and Validation within TAP GUI</li> <li> I can configure API Portal with TAP</li> <li> I can install Shared Services on a TAP Cluster (Services Toolkit)</li> <li> I can claim a shared service on a TAP Cluster (Service Bindings)</li> <li> I can use a postgres resource claim with sample Spring Boot Application Pet Clinic Accelerator</li> <li> Expose Accerators Endpoint for use in Tanzu Accelerator Plugins</li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#todo","title":"TODO","text":"<ul> <li>bitnami services one</li> <li>use spring boot 3</li> <li>use Java 17</li> <li>create via accelerator</li> <li>skip testcontainer -&gt; explain what can be done</li> <li>create ssh secret</li> <li>go through the steps<ul> <li>create http secret</li> <li>add</li> <li>register in TAP GUI</li> <li>show API</li> <li>verify</li> <li>update -&gt; main controller &amp; test</li> <li>run local <code>mvn test</code> if you can</li> <li>verify update</li> </ul> </li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#steps","title":"Steps","text":"<ul> <li>go to TAP GUI -&gt; Create | use VSCode Extention<ul> <li>https://tap-gui.view.h2o-2-9349.h2o.vmware.com/create</li> </ul> </li> <li> <p>Possible Choices:</p> <ul> <li>Tanzu Java Restful Web App</li> <li>AppSSO Starter Java</li> <li>Tanzu Java Web UI</li> <li>Tanzu Java Web App</li> </ul> </li> <li> <p>create project via VSCode Extention</p> </li> <li>open project (accept prompt)</li> <li>create project in Gitea</li> <li>add project to Gitea</li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#config-used","title":"Config Used","text":"<ul> <li>Project Name: tap-workload-demo-01</li> <li>Artifact Id: customer-profile</li> <li>Group Id: com.example</li> <li>Package Name: com.example.customerprofile</li> <li>Build Tool: maven</li> <li>Expose Open API Endpoint: Yes</li> <li>Update Boot 3: Yes</li> <li>Java Version: 17</li> <li>Database Type: postgres</li> <li>Database Name: customer-database</li> <li>Database Migration Tool: flyway</li> <li>Database Integration Test Type: in-memory</li> <li>Include Build Tool Wrapper: Yes</li> <li>Api System: profile-management</li> <li>Api Owner: customer-relations-department</li> <li>Api Description: Manage customer profiles</li> <li>Database Postgres Storage Class: default</li> <li>Live Update IDE Support: Yes</li> <li>Source Repository Prefix: dev.local</li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#commands","title":"Commands","text":"<pre><code>git init\ngit add .\ngit commit -m \"first commit\"\ngit remote add origin https://gitea.services.h2o-2-9349.h2o.vmware.com/gitea/demo-02.git\ngit push -u origin main\n</code></pre> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\n</code></pre> <pre><code>kubectl create secret\n</code></pre> <pre><code>tanzu apps workload create spring-boot-postgres-01 \\\n--namespace dev \\\n--git-repo ssh://git@172.16.50.201:22/gitea/spring-boot-postgres.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=spring-boot-spring-01 \\\n--label apps.tanzu.vmware.com/has-tests-needs-workspace=true \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--build-env BP_JVM_VERSION=17 \\\n--param gitops_ssh_secret=gitea-ssh \\\n--service-ref db=services.apps.tanzu.vmware.com/v1alpha1:ClassClaim:psql-1 \\\n--yes\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/hello-world/","title":"TAP Workload Demo","text":"<p>Coming soon.</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#checks","title":"Checks","text":"<ul> <li> I'm able to use an accelerator to generate a Web TAP Workload</li> <li> I applied a TAP Web Workload and register it within TAP GUI</li> <li> I'm able to do basic troubleshooting of a TAP Workload</li> <li> I've configured App Live View in TAP GUI for a TAP Web Workload</li> <li> I've configured Tanzu Build Service and it works for a Spring Boot Application to produce an Image</li> <li> I am able to view and verify a TAP Workload displays in properly in Supply Chain view of TAP GUI</li> <li> I know how to configure an integration to a private Git provider in TAP GUI</li> <li> I know how to configure access to private repositories (SCM and Container) for TAP Workloads</li> <li> I have configured custom CA's for TAP</li> </ul> <p>To add:</p> <ul> <li> I am able to view and verify results for the Security Analysis within TAP GUI</li> </ul>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#java-web","title":"Java Web","text":"","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#steps","title":"Steps","text":"<ul> <li>go to TAP GUI -&gt; Create | use VSCode Extention<ul> <li>https://tap-gui.view.h2o-2-9349.h2o.vmware.com/create</li> <li>Tanzu Java Web UI</li> </ul> </li> <li>create project in Gitea<ul> <li>make the project public (should be the default)</li> </ul> </li> <li>Push project to Gitea</li> <li>create secret for Gitea with Credentials &amp; CA Cert<ul> <li>https://fluxcd.io/flux/components/source/gitrepositories/</li> </ul> </li> <li>create workload</li> <li>(optional) copy Deliverable to Run cluster</li> <li>verify application runs</li> </ul>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#commands","title":"Commands","text":"<pre><code>git init\ngit add .\ngit commit -m \"first commit\"\ngit remote add origin https://gitea.services.h2o-2-9349.h2o.vmware.com/gitea/tap-demo-03.git\ngit push -u origin main\n</code></pre> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\n</code></pre> <ul> <li>https://fluxcd.io/flux/components/source/gitrepositories/</li> </ul> <pre><code>kubectl create secret generic https-credentials \\\n--namespace $TAP_DEVELOPER_NAMESPACE \\\n--from-file caFile=ssl/ca.crt \\\n--from-literal username=gitea \\\n--from-literal password=gitea\n</code></pre> <ul> <li>in existing env we need <code>--label apps.tanzu.vmware.com/has-tests-needs-workspace=true \\</code></li> </ul> <pre><code>export APP_NAME=tap-demo-03\n</code></pre> <pre><code>tanzu apps workload create ${APP_NAME} \\\n--namespace ${TAP_DEVELOPER_NAMESPACE} \\\n--git-repo https://gitea.services.h2o-2-9349.h2o.vmware.com/gitea/${APP_NAME}.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=${APP_NAME} \\\n--label apps.tanzu.vmware.com/has-tests-needs-workspace=true \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--param gitops_ssh_secret=https-credentials \\\n--yes\n</code></pre> <pre><code>kubectl get workload -A\n</code></pre> <pre><code>kubectl get GitRepository -A\n</code></pre> <pre><code>tanzu apps workload get ${APP_NAME} --namespace dev\n</code></pre> <pre><code>tanzu apps workload tail ${APP_NAME} --namespace dev --timestamp --since 1h\n</code></pre> <pre><code>export DELIVERABLE_FILE=\"${APP_NAME}-deliverable.yaml\"\n</code></pre> <pre><code>kubectl get configmap \"${APP_NAME}-deliverable\" -n ${TAP_DEVELOPER_NAMESPACE} \\\n-o go-template='{{.data.deliverable}}' \\\n&gt; ${DELIVERABLE_FILE}\necho \"DELIVERABLE_FILE=${DELIVERABLE_FILE}\"\n</code></pre> <ul> <li>Change to Run cluster</li> </ul> <pre><code>kubectl apply -f ${DELIVERABLE_FILE} -n apps\n</code></pre> <ul> <li>register application in TAP GUI</li> <li>use <code>./catalog/catalog-info.yaml</code></li> <li>use the raw version, for <code>tap-demo-03</code> URL=https://gitea.services.h2o-2-9349.h2o.vmware.com/gitea/tap-demo-03/raw/branch/main/catalog/catalog-info.yaml</li> </ul> <pre><code>kubectl get deliverable -n apps\n</code></pre> <pre><code>kubectl get httpproxy -A\n</code></pre> <pre><code>curl -k \"https://tap-demo-03.apps.run-01.h2o-2-9349.h2o.vmware.com/\"\n</code></pre>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/where-for-dinner/","title":"TAP Where For Dinner App","text":"","tags":["tap","kubernetes","spring","java","spring-boot","micoservice"]},{"location":"apps/where-for-dinner/#source","title":"Source","text":"<ul> <li>https://github.com/vmware-tanzu/application-accelerator-samples/blob/main/where-for-dinner/doc/TAPDeployment.md</li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","micoservice"]},{"location":"custom/accelerator/","title":"TAP Create Accelerator","text":"<p>TODO</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/","title":"TAP Basic Install","text":"<p>Important</p> <p>The Goals and Outcomes is the work of Rick Farmer.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#goals-outcomes-operator","title":"Goals &amp; Outcomes Operator","text":"<ol> <li>I can do a basic install of TAP within a customer environment</li> <li>I have a basic understanding of the core features of TAP such that I can explain these to a customer</li> <li>I\u2019m able to demo TAP deployment via an OOTB accelerator and can explain the benefits of accelerator</li> </ol>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#checks","title":"Checks","text":"<ul> <li> Know the relevant parts of the docs</li> <li> Understand the prerequisites: TAP install</li> <li> Understand the prerequisites: Jump Host</li> <li> Can configure a TAP Full Profile config file</li> <li> Can discover how to configure TAP Packages</li> <li> Understand how to configure custom CA</li> <li> Can install TAP Full Profile</li> <li> I manually installed TAP</li> <li> I'm able to use an accelerator to generate a Web TAP Workload</li> <li> I'm able to manually setup a single developer namespace</li> <li> I applied a TAP Web Workload and register it within TAP GUI</li> <li> I'm able to do basic troubleshooting of a TAP Workload</li> <li> I'm able to do basic troubleshooting of a TAP Installation</li> <li> I've configured App Live View in TAP GUI for a TAP Web Workload</li> <li> I can update TAP values and reconcile changes on the cluster</li> <li> I understand TAP GUI Catalog System</li> <li> I know how to configure an integration to a private Git provider in TAP GUI</li> <li> I know how to configure access to private container registries in TAP Values</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#optional","title":"Optional","text":"<ul> <li> I'm able to install Learning Center and use the sample workshop.</li> <li> I understand the different Workload Types available in TAP</li> <li> I can configure a public Git provider for TAP GUI in TAP Values</li> <li> I can configure DNS for TAP Endpoints</li> <li> I have relocated TAP Images to my own container registry and understand the imgpkg utility.</li> <li> I understand Kapp Controller and Secret Gen Controller in the context of TAP</li> <li> I've able to configure and install the OOTB Testing and Scanning Supply Chain and verify a Workload</li> <li> I am able to configure a Scan Policy for a Source and Image Scan</li> <li> I am able to create a Tekton Task to execute Unit Tests within a Supply Chain for a Developer Namespace</li> <li> I know how to configure an Auth Provider for TAP GUI access</li> <li> I'm able to configure TAP GUI to read from Metadata Store.</li> <li> I know how to configure access to private repositories (SCM and Container) for TAP Workloads</li> <li> I am able to register API Documentation for a Workload in TAP GUI</li> <li> I am able to view and verify results for the Security Analysis within TAP GUI</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#steps","title":"Steps","text":"<ul> <li>Verify Prerequisites are met</li> <li>Install Cluster Essentials</li> <li>Choose TAP setup (profile, installation type)</li> <li>Review TAP packages configuration options<ul> <li>e.g., TAP values schema -&gt; map value to other package -&gt; other package values schema</li> </ul> </li> <li>Install TAP Profile</li> <li>Install Test Workload</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#verify-prerequisites-are-met","title":"Verify Prerequisites are met","text":"<p>The prerequisites1 we need:</p> <ul> <li>Credentials for Tanzu Net</li> <li>Credentials for local Container Image Registry</li> <li>Accept Tanzu Application Platform EULAs</li> <li>DNS Records for the clusters</li> <li>Suitable Kubernetes clusters</li> <li>Jump Host or other machine with the required tools in place</li> <li>Relocate TAP Images to local Container Image Registry</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#required-tools","title":"Required Tools","text":"<ul> <li>kubectl</li> <li>yq</li> <li>jq</li> <li>Tanzu CLI<ul> <li>with TAP plugins</li> </ul> </li> <li>curl</li> <li>...?</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#suitable-kubernetes-clusters","title":"Suitable Kubernetes clusters","text":"<p>TAP 1.5 supports Kubernetes 1.24, 1.25 and 1.262.</p> <p>Indicative cluster resource requirements of TAP per profile: (TODO: verify these numbers)</p> Profile Memory/Node Storage/Node vCPU Total Memory Total Iterate 8GB 150GB 12 16GB Build 16GB 150GB 12 12GB View 8GB 50GB 8 8GB Run 8GB 100GB 12 8GB Full 16GB 150BG 16 20GB <p>Warning</p> <p>These are requirements to run the TAP components. This does not include the applications or builds run in the cluster.</p> <p>For example, let's look at an application in a Run cluster. If your application requires 10vCPU and 40GB memory, you add that on top of the TAP requirements. Your Run cluser now needs a minimum of 22vCPU and 48GB of memory.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#tap-images-relocated","title":"TAP Images Relocated","text":"<p>TODO: provide information on what has been relocated and to where for the LAB environment 3</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-cluster-essentials","title":"Install Cluster Essentials","text":"<p>Cluster Essentials4 essentially (pun intended) boils down to two components:</p> <ul> <li>KAPP Controller5</li> <li>SecretGen Controller6</li> </ul> <p>Please install the Cluster Essentials4 according to the docs.</p> <p>Important</p> <p>While not explicitly mentioned, for each TAP minor version (e.g., 1.4, 1.5) there is an associated Cluster Essentials4.</p> <p>A minimum version of either controller is required, although there is no explicit version of either known at this point in time (April 2023).</p> <p>So unless there is a strong reason not too, use the Cluster Essentials referenced in the TAP docs.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#configure-certificate-authority","title":"Configure Certificate Authority","text":"<p>While TAP supports configuring a Certificate Authority (CA) to be trusted by the TAP machinery (or Workloads specifically11) this applies to when TAP is installed.</p> <p>To install TAP, we need to ensure that the KAPP Controller we install with the Cluster Essentials, also trusts our custom CA.</p> <p>When using a private registry, which we do, we need to create a Secret with a pre-defined name and structure. This kapp-controller-config12 Secret is expected to have the CA as Base64 encoded PEM in the field <code>caCerts</code>. Luckily, this is done with a single line via the kubectl create secret4 command.</p> <pre><code>kubectl create secret generic kapp-controller-config \\\n--namespace kapp-controller \\\n--from-file caCerts=ca.crt\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#choose-tap-setup-profile-installation-type","title":"Choose TAP setup (profile, installation type)","text":"<p>For almost every customer, the TAP installation covers multiple environments, multiple clusters, and various profiles.</p> <p>Some customers want a Run cluster per application, others assume multi-tenancy is oke.</p> <p>It is generally recommended to assume different environments with their own purpose. For example:</p> <ul> <li>Test: contains one or more clusters, regularly re-created, used for experimentation and learning. Only used by the Platform Team.</li> <li>Staging: multiple clusters and multiple profiles, used for testing specific features and upgrades. Used by the Platform Team, and a handful of \"beta testers\".</li> <li>Production: multiple clusters and multiple profiles, including an iterate cluster for learning for end-users.</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#full-profile-for-workshop","title":"Full Profile For Workshop","text":"<p>For the sake of brevity for this workshop, we'll stick to a single cluster with the Full profile.</p> <p>While this does not represent a typical installation, it let's you go through the steps of installing and using TAP. It also shows you all the components and let's you customize and interact with them, without having to go multiple times the same process.</p> <p>The Full profile contains all the components of TAP, which is what the name implies (not always true, but this time it is).</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-type","title":"Install Type","text":"<p>Next we choose how we want to install TAP.</p> <p>There are currently three options:</p> <ol> <li>Traditional manual KAPP package install, online</li> <li>Traditional manual KAPP package install, offline</li> <li>GitOps install, beta</li> </ol> <p>It is likely that the GitOps installation type will be the default in the future.</p> <p>For now the most common (and supported) installation is the Traditional Offline install.</p> <p>It is good to understand what steps need to be taken before automating them. So this is the type we'll use for this workshop.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#version","title":"Version","text":"<p>Ideally we want to use the latest (supported) version.</p> <p>Which at this time of writing is TAP 1.5.</p> <p>TAP 1.5 requires Kubernetes 1.24, so this is currently (April 2023) not supported on TGKs based customers, as they can only go to Kubernetes 1.23.</p> <p>Assuming that in due time vSphere 8 with TGKs does support 1.24 (the Supervisor cluster already does), TAP 1.5 is a relatively safe bet.</p> <p>TAP 1.6, not yet released, will require Kubernetes 1.25, which won't be supported anytime soon with TGKs or TGKm.</p> <p>So let's stick to TAP 1.5.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#conclusion","title":"Conclusion","text":"<p>Our TAP environment will be as follows:</p> <ul> <li>TAP 1.5</li> <li>single Kubernetes cluster of 1.24</li> <li>using the Full profile</li> <li>installed via the traditional KAPP package, assuming a internet restricted environment</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-package-repository","title":"Install Package Repository","text":"<p>Now that we know what version of TAP we want to install, and how we want to install it, we install the Package Repository.</p> <p>Assumptions: * All relevant TAP packages are relocated * We have read credentials to Image Registry containing the TAP apps * We have Kubernetes cluster   * Which runs a version that TAP supports (e.g., 1.24)   * The nodes trust the CA of Image Registry</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#create-required-secrets","title":"Create Required Secrets","text":"<p>The TAP Package Repository comes from either Tanzu Network or the Registry you relocated the TAP images to.</p> <p>In either case, you need a Registry read credential in the Namespace we install the Package Repository in.</p> <pre><code>TAP_INSTALL_NAMESPACE=tap-install\nTAP_INSTALL_REGISTRY_SECRET=tap-registry\nTAP_INSTALL_REGISTRY_HOSTNAME=\nTAP_INSTALL_REGISTRY_USERNAME=\nTAP_INSTALL_REGISTRY_PASSWORD=\n</code></pre> <p>First, create the Namespace:</p> <pre><code>kubectl create namespace ${TAP_INSTALL_NAMESPACE} || true\n</code></pre> <p>And then use the Tanzu CLI to create a credential via the SecretGen Controller:</p> <pre><code>tanzu secret registry add ${TAP_INSTALL_REGISTRY_SECRET} \\\n--username ${INSTALL_REGISTRY_USERNAME} \\\n--password ${INSTALL_REGISTRY_PASSWORD} \\\n--server ${INSTALL_REGISTRY_HOSTNAME} \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--export-to-all-namespaces \\\n--yes\n</code></pre> <p>Registry Write Secret</p> <p>TAP profiles <code>Iterate</code>, <code>Full</code>, and <code>Build</code>, also need a Registry write secret.</p> <p>This secret is used to write built images of the applications going throuhg the Supply Chains. Define the appropriate environment variables:</p> <pre><code>TAP_INSTALL_NAMESPACE=tap-install\nTAP_BUILD_REGISTRY_SECRET=registry-credentials\nTAP_BUILD_REGISTRY_HOSTNAME=\nTAP_BUILD_REGISTRY_USERNAME=\nTAP_BUILD_REGISTRY_PASSWORD=\n</code></pre> <p>And then we use the Tanzu CLI to create the secret:</p> <pre><code>tanzu secret registry add ${TAP_BUILD_REGISTRY_SECRET} \\\n--username ${TAP_BUILD_REGISTRY_USERNAME} \\\n--password ${TAP_BUILD_REGISTRY_PASSWORD} \\\n--server   ${TAP_BUILD_REGISTRY_HOSTNAME} \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--export-to-all-namespaces \\\n--yes\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#create-package-repository","title":"Create Package Repository","text":"<p>To make the values less abstract, let's look at an example.</p> <p>Assume we have a internal Harbor registry, with the hostname <code>harbor.example.com</code>. We create a project in this Harbor instance called <code>tap</code>, and relocated the TAP packages to this project as <code>tap-packages</code>.</p> <p>The complete URL will now be: <code>harbor.example.com/tap/tap-packages:1.5.0</code>.</p> <p>And our environment variables will be:</p> <ul> <li><code>INSTALL_REGISTRY_REPO=tap</code></li> <li><code>INSTALL_REGISTRY_HOSTNAME=harbor.example.com</code></li> <li><code>TAP_VERSION=1.5.0</code></li> </ul> <p>Set the environment variables to values appropriate for your environment.</p> <pre><code>TAP_INSTALL_NAMESPACE=tap-install\nTAP_VERSION=1.5.0\nINSTALL_REGISTRY_REPO=tap\nINSTALL_REGISTRY_HOSTNAME=\n</code></pre> <p>And use the Tanzu CLI to create the Package Repository for TAP in the TAP install Namespace (usually <code>tap-install</code>).</p> <pre><code>tanzu package repository add tanzu-tap-repository \\\n--url ${INSTALL_REGISTRY_HOSTNAME}/${INSTALL_REGISTRY_REPO}/tap-packages:${TAP_VERSION} \\\n--namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Run the command below to verify the Package Repository is reconciled successfully:</p> <pre><code>tanzu package repository list -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>This should return something like this:</p> <pre><code>  NAME                      SOURCE                                                                            STATUS\n  tanzu-tap-repository      (imgpkg) harbor.services.h2o-2-9349.h2o.vmware.com/tap/tap-packages:1.5.0         Reconcile succeeded\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#configure-certificate-authority-bundle-for-crossplane","title":"Configure Certificate Authority Bundle For Crossplane","text":"<p>The use of external services via the Bitnami Services14 feature relies on Crossplane15.</p> <p>Unfortunately, in the <code>1.5.0</code> release of TAP, the Crossplane package does not pickup the <code>shared.ca_cert_data</code> property.</p> <p>This means we must configure Crossplane ourselves. Crossplane expects a ConfigMap with a ca-bundle property13, which we later configure when installing TAP Profile.</p> <p>First, verify the <code>crossplane-system</code> namespace exists:</p> <pre><code>kubectl create namespace crossplane-system || true\n</code></pre> <p>And then create the ConfigMap with the expected name and value.</p> <pre><code>kubectl -n crossplane-system create cm ca-bundle-config \\\n--from-file=ca-bundle=ca.crt\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#review-tap-packages-configuration-options","title":"Review TAP packages configuration options","text":"<p>Before we can install our TAP Profile as desired, we need to understand how to configure it.</p> <p>We can take a look at the available packages and specifically the TAP package itself to discover what we can configure.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#explore-packages","title":"Explore Packages","text":"<p>The first step, is to discover the packages available to us.</p> <pre><code>tanzu package available list --namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>This is a large list, so we'll limit the expected output:</p> <pre><code>  NAME                                                 DISPLAY-NAME\n  accelerator.apps.tanzu.vmware.com                    Application Accelerator for VMware Tanzu\n  api-portal.tanzu.vmware.com                          API portal\n  apis.apps.tanzu.vmware.com                           API Auto Registration for VMware Tanzu\n  ...\n</code></pre> <p>To see what we can configure with TAP, we have to take a few steps:</p> <pre><code>tanzu package available get tap.tanzu.vmware.com --namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>This returns the versions of the TAP main package available to us.</p> <p>You can also get a list of available packages and their version via <code>kubectl</code>:</p> <pre><code>kubectl get package -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Which results in something like this:</p> <pre><code>NAME                                                            PACKAGEMETADATA NAME                                  VERSION                         AGE\nantrea.tanzu.vmware.com.1.7.2+vmware.1-tkg.1-advanced           antrea.tanzu.vmware.com                               1.7.2+vmware.1-tkg.1-advanced   646h52m30s\ncapabilities.tanzu.vmware.com.0.28.1+vmware.1                   capabilities.tanzu.vmware.com                         0.28.1+vmware.1                 646h52m34s\nkube-vip-cloud-provider.tanzu.vmware.com.0.0.4+vmware.2-tkg.1   kube-vip-cloud-provider.tanzu.vmware.com              0.0.4+vmware.2-tkg.1            646h52m33s\n...\n</code></pre> <p>Eitherway, we now know the version of the TAP package available to us, <code>1.5.0</code>. We need this information if we want to see the possible values for the TAP package.</p> <p>We add the version to the package name (<code>&lt;packageName&gt;/&lt;packageVersion&gt;</code>), and then add the <code>--values-schema</code> flag.</p> <pre><code>tanzu package available get tap.tanzu.vmware.com/1.5.0 \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--values-schema\n</code></pre> <p>This results in a long list of possible values:</p> <pre><code>  KEY                                                  DEFAULT                 TYPE    DESCRIPTION\n  appliveview                                                                  object  App Live View configuration\n  contour.envoy.service.type                           LoadBalancer            string  Set to LoadBalancer by default; valid values are LoadBalancer, NodePort and\n                                                                                       ClusterIP(except for contour.infrastructure_provider=vsphere)\ncrossplane                                                                   object  Crossplane configuration\n  image_policy_webhook                                                         object  Image Policy Webhook configuration\n  learningcenter                                                               object  Learning Center configuration\n  ...\n</code></pre> <p>As you can probably tell, many of these values are not explained at this level.</p> <p>For example, the <code>TYPE</code> of <code>crossplane</code> is <code>object</code>. We will have to explore the values schema of the Crossplane package to discover what we can configure here if we need to.</p> <p>There is a lot of overlap between the packages, especially with configuration options such as a Domain name, secrets, or a custom CA.</p> <p>These are conventient configurable via the <code>shared</code> configuration key:</p> <pre><code>tanzu package available get tap.tanzu.vmware.com/1.5.0 \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--values-schema | grep shared.\n</code></pre> <p>This gives the following list:</p> <pre><code>  shared.image_registry.password                       \"\"                      string  Optional: Password for the image registry. Mutually exclusive with\n                                                                                       shared.image_registry.secret.name/namespace.\n  shared.image_registry.project_path                   \"\"                      string  Optional: Project path in the image registry server used for builder and\n  shared.image_registry.secret.name                    \"\"                      string  Optional: Secret name for the image registry credentials of\n                                                                                       shared.image_registry.username/password.\n  shared.image_registry.secret.namespace               \"\"                      string  Optional: Secret namespace for the image registry credentials. Mutually\n                                                                                       exclusive with shared.image_registry.username/password.\n  shared.image_registry.username                       \"\"                      string  Optional: Username for the image registry. Mutually exclusive with\n                                                                                       shared.image_registry.secret.name/namespace.\n  shared.ingress_domain                                \"\"                      string  Optional: Domain name to be used in service routes and hostnames for instances\n  shared.ingress_issuer                                tap-ingress-selfsigned  string  Optional: A cert-manager.io/v1/ClusterIssuer for issuing TLS certificates to TAP\n  shared.kubernetes_distribution                       \"\"                      string  Optional: Type of K8s infrastructure being used. Can be used in coordination\n  shared.kubernetes_version                            \"\"                      string  Optional: K8s version. Can be used independently or in coordination with\n  shared.activateAppLiveViewSecureAccessControl                                bool    Optional: Enable Secure Access Connection between App Live View Components\n  shared.ca_cert_data                                  \"\"                      string  Optional: PEM Encoded certificate data to trust TLS connections with a private\n</code></pre> <p>Let's dive into the Full profile next.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#full-profile-example-from-docs","title":"Full Profile Example From Docs","text":"<p>The documentation has an annotated Full profile configuration file7 example, you can expand the example below if you want to see it.</p> <p>Let's look at how we use YTT to template the Profile configuration.</p> Full Profile Example From Docs full-profile.yaml<pre><code>shared:\ningress_domain: \"INGRESS-DOMAIN\"\ningress_issuer: # Optional, can denote a cert-manager.io/v1/ClusterIssuer of your choice. Defaults to \"tap-ingress-selfsigned\".\nimage_registry:\nproject_path: \"SERVER-NAME/REPO-NAME\"\nsecret:\nname: \"KP-DEFAULT-REPO-SECRET\"\nnamespace: \"KP-DEFAULT-REPO-SECRET-NAMESPACE\"\nkubernetes_distribution: \"K8S-DISTRO\" # Only required if the distribution is OpenShift and must be used with the following kubernetes_version key.\nkubernetes_version: \"K8S-VERSION\" # Required regardless of distribution when Kubernetes version is 1.25 or later.\nca_cert_data: | # To be passed if using custom certificates.\n-----BEGIN CERTIFICATE-----\nMIIFXzCCA0egAwIBAgIJAJYm37SFocjlMA0GCSqGSIb3DQEBDQUAMEY...\n-----END CERTIFICATE-----\nceip_policy_disclosed: FALSE-OR-TRUE-VALUE # Installation fails if this is not set to true. Not a string.\n#The above keys are minimum numbers of entries needed in tap-values.yaml to get a functioning TAP Full profile installation.\n#Below are the keys which may have default values set, but can be overridden.\nprofile: full # Can take iterate, build, run, view.\nsupply_chain: basic # Can take testing, testing_scanning.\nootb_supply_chain_basic: # Based on supply_chain set above, can be changed to ootb_supply_chain_testing, ootb_supply_chain_testing_scanning.\nregistry:\nserver: \"SERVER-NAME\" # Takes the value from the shared section by default, but can be overridden by setting a different value.\nrepository: \"REPO-NAME\" # Takes the value from the shared section by default, but can be overridden by setting a different value.\ngitops:\nssh_secret: \"SSH-SECRET-KEY\" # Takes \"\" as value by default; but can be overridden by setting a different value.\ncontour:\nenvoy:\nservice:\ntype: LoadBalancer # This is set by default, but can be overridden by setting a different value.\nbuildservice:\n# Takes the value from the shared section by default, but can be overridden by setting a different value.\nkp_default_repository: \"KP-DEFAULT-REPO\"\nkp_default_repository_secret: # Takes the value from the shared section above by default, but can be overridden by setting a different value.\nname: \"KP-DEFAULT-REPO-SECRET\"\nnamespace: \"KP-DEFAULT-REPO-SECRET-NAMESPACE\"\ntap_gui:\nservice_type: ClusterIP # If the shared.ingress_domain is set as earlier, this must be set to ClusterIP.\nmetadataStoreAutoconfiguration: true # Create a service account, the Kubernetes control plane token and the requisite app_config block to enable communications between Tanzu Application Platform GUI and SCST - Store.\napp_config:\ncatalog:\nlocations:\n- type: url\ntarget: https://GIT-CATALOG-URL/catalog-info.yaml\nmetadata_store:\nns_for_export_app_cert: \"MY-DEV-NAMESPACE\"\napp_service_type: ClusterIP # Defaults to LoadBalancer. If shared.ingress_domain is set earlier, this must be set to ClusterIP.\nscanning:\nmetadataStore:\nurl: \"\" # Configuration is moved, so set this string to empty.\ngrype:\nnamespace: \"MY-DEV-NAMESPACE\"\ntargetImagePullSecret: \"TARGET-REGISTRY-CREDENTIALS-SECRET\"\n# In a single cluster, the connection between the scanning pod and the metadata store happens inside the cluster and does not pass through ingress. This is automatically configured, you do not need to provide an ingress connection to the store.\npolicy:\ntuf_enabled: false # By default, TUF initialization and keyless verification are deactivated.\ntap_telemetry:\ncustomer_entitlement_account_number: \"CUSTOMER-ENTITLEMENT-ACCOUNT-NUMBER\" # (Optional) Identify data for creating the Tanzu Application Platform usage reports.\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#full-profile-template","title":"Full Profile Template","text":"<p>Usually you will have more than one environment and possibly more than one of the same profile in the same environment. So you end up installing the same Profile more than once.</p> <p>It is recommended to use some kind of templating.</p> <p>In the world of Tanzu (and TAP), it makes sense to use YTT.</p> <p>TODO: review this by doing all the commands/use the template</p> full-profile.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: full\nshared:\ningress_domain: #@ dv.domainName\nca_cert_data: #@ dv.caCert\nimage_registry:\nproject_path: #@ kpRegistry\nsecret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nsupply_chain: basic\nootb_supply_chain_basic:\nregistry:\nserver: #@ dv.buildRegistry\nrepository: #@ dv.buildRepo\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: #@ \"appliveview.\"+dv.domainName\nappliveview:\ningressEnabled: true\nserver:\ntls:\nenabled: false\ntap_gui:\nservice_type: ClusterIP\napp_config:\nauth:\nallowGuestAccess: true\ncustomize:\ncustom_name: 'Portal McPortalFace'\norganization:\nname: 'Org McOrg Face'\ncatalog:\nlocations:\n- type: url\ntarget: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n- type: url\ntarget: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\ncrossplane:\nregistryCaBundleConfig:\nname: ca-bundle-config\nkey: ca-bundle\ncontour:\nenvoy:\nservice:\ntype: LoadBalancer\nceip_policy_disclosed: true\nexcluded_packages:\n- scanning.apps.tanzu.vmware.com\n- grype.scanning.apps.tanzu.vmware.co\n- policy.apps.tanzu.vmware.com\n</code></pre> <p>Template Explained</p> <p><code>shared</code>: values that are to be shared across any package that has the same configuration option.    It is here we configure Install Registry credentials and custom CA.</p> <p><code>buildservice</code>: if your environment has no or restricted access to the internet, you need to manage the depencies yourself.   It can re-use the repository from the shared configuration. Here we do that, and tell it to exclude the depencies as we install them ourselves.</p> <p><code>supply_chain</code>: TAP can install one Cartographer Supply Chain fully*. You specify the Supply Chain by name,   and then configure the Supply Chain by its own name**, in our case <code>basic</code> and <code>ootb_supply_chain_basic</code>.    Where we override the default Registry and Repository; this is where our built images are pushed.</p> <p><code>appliveview</code>: the App Live View component let's use view registered application's resources existing across TAP cluster in the TAP GUI.   Note, this is a TAP GUI component and that is where you will find it.</p> <p><code>tap_gui</code>: the TAP GUI is the way to discover APIs, applications, accelerators and the like. It is Backstage with a few VMware Tanzu plugins included.   It has a lot of configuration options, most of which can be ignored for an initial installation.    To use TAP GUI, you have to specify an authorization provider. In the event you do not have one (ready), you set app_config.auth.allowGuestAccess to true.</p> <p><code>crossplane</code>: Crossplane is a new package for TAP 1.5, and unfortunately does not (yet) pick up the Shared CA configuration. So we have to configure this ourselves.   TAP provides external services for use with the Service Binding Specification. By default it includes Crossplane packages for several Bitnami Helm Charts (packed as Bitnami Services).</p> <p><code>ceip_policy_disclosed</code>: if you accept the policy or not. If you set this to false, you cannot install TAP</p> <p><code>excluded_packages</code>: the packages we exclude are from the Test &amp; Scanning Supply Chain. They require some configuration, especially if you are in a restricted internet access environment.   Because we are using the Basic Supply Chain, these packages are not required and we can exclude them.</p> <ul> <li>= the Test and Test &amp; Scanning Supply Chains do miss a Tekton task, but are complete beyond that</li> </ul> <p>** = each Supply Chain is its own Carvel Package part of the TAP Packages Repository</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#tanzu-build-service","title":"Tanzu Build Service","text":"<p>TAP includes a version of Tanzu Build Service (TBS). One of the things TBS needs, is a bunch of container images for the various Build Packs and the various versions of tech stacks they support.</p> <p>A full set of depencies is ~12 GB, and TBS starts synchronizing these to the nodes once it is up and running. There are two complications with this, one, thats a lot of data to synchronize from the outside, and two, you might not be able to reach the outside.</p> <p>So what we do instad, is to relocate the TBS depencies8, in the same way as we do with the TAP packages.</p> <p>We configured this in the TAP Profile already, by specifying the following:</p> <pre><code>buildservice:\nexclude_dependencies: true\n</code></pre> <p>We cannot install the TAP TBS Dependencies (a TAP specific bundle of TBS) until we install TAP. The TAP TBS Dependencies package depends on CRDs that are installed by TAP. So we will continue this later.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tap-profile","title":"Install TAP Profile","text":"<p>At this point, we have the following:</p> <ul> <li>TAP Packages available in a local Image Registry</li> <li>TAP TBS Dependencies available in a local Image Registry</li> <li>A Namespace to install TAP into (the concention is <code>tap-install</code>)</li> <li>Read access secret to the local Image Registry to retrieve the TAP packages</li> <li>Write access secret to the local Image Registry to write application build images to</li> <li>Profile Template for our Profile of choice (<code>Full</code>)</li> </ul> <p>We now take the following steps:</p> <ol> <li>Generate a Profile config file from the YTT Template</li> <li>Install the TAP package</li> <li>Install the TBS Dependencies Package Repository</li> <li>Install the TBS Dependencies Package</li> </ol>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#generate-profile-config-file","title":"Generate Profile Config FIle","text":"<p>First, we set the required variables:</p> <p>Warning</p> <p>You should have received the address of the Build Registry and the Domain specific to your cluster!</p> <pre><code>export TAP_BUILD_REGISTRY_SECRET=registry-credentials\nexport BUILD_REGISTRY_REPO=tap-apps\nexport TBS_REPO=buildservice/tbs-full-deps\nexport CA_CERT=$(cat ssl/ca.pem)\nexport BUILD_REGISTRY=\nexport DOMAIN_NAME=\n</code></pre> <p>And then we run YTT to generate our Profile configuration file.</p> <pre><code>ytt -f full-profile.ytt.yaml \\\n-v buildRegistry=\"$BUILD_REGISTRY\" \\\n-v buildRegistrySecret=\"$BUILD_REGISTRY_SECRET\" \\\n-v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n-v tbsRepo=\"$TBS_REPO\" \\\n-v domainName=\"$DOMAIN_NAME\" \\\n-v caCert=\"${CA_CERT}\" \\\n&gt; \"tap-values-full.yml\"\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tap-package","title":"Install TAP Package","text":"<p>We are now ready to install TAP.</p> <p>In case you don't remember or your environment variables are no longer set, set them to the appropriate values:</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_VERSION=1.5.0\n</code></pre> <p>We can then install TAP9 via the Tanzu CLI.</p> <pre><code>tanzu package install tap \\\n-p tap.tanzu.vmware.com \\\n-v $TAP_VERSION \\\n--values-file tap-values-full.yml \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>You can get an overview of the Packages that are installed via the Tanzu CLI:</p> <pre><code>tanzu package installed list -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Or via <code>kubectl</code>:</p> <pre><code>kubectl get app -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>If there is an issue, you can debug the package installs via <code>kubectl</code> commands:</p> <pre><code>kubectl describe app -n ${TAP_INSTALL_NAMESPACE} tap\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#determine-tbs-version","title":"Determine TBS Version","text":"<p>Now that we have the TAP package installed, we can install the TBS dependencies.</p> <p>First, we need to verify which version of TBS TAP shipped8.</p> <pre><code>tanzu package available list buildservice.tanzu.vmware.com \\\n--namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Which for TAP <code>1.5.0</code> gives the following result:</p> <pre><code>  NAME                           VERSION  RELEASED-AT\n  buildservice.tanzu.vmware.com  1.10.8   -\n</code></pre> <p>We then set the environment variable, so we can re-use it:</p> <pre><code>export TBS_VERSION=1.10.8\n</code></pre> <p>Note</p> <p>Your environment should already have the TBS Dependencies relocated for you.</p> <p>If you want to know how to do so, you can find it documented here8</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tbs-depencies-package-repository","title":"Install TBS Depencies Package Repository","text":"<p>In case you have not yet, set the environment variables.</p> <pre><code>export TBS_REPO=buildservice/tbs-full-deps\n</code></pre> <p>And we then we can install the TBS Dependencies Package Repository:</p> <pre><code>tanzu package repository add tbs-full-deps-repository \\\n--url ${INSTALL_REGISTRY_HOSTNAME}/${TBS_REPO}:VERSION \\\n--namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>We can verify the status via the Tanzu CLI:</p> <pre><code>tanzu package repository list -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Which should now yield this:</p> <pre><code>  NAME                      SOURCE                                                                            STATUS\n  tanzu-tap-repository      (imgpkg) harbor.services.h2o-2-9349.h2o.vmware.com/tap/tap-packages:1.5.0         Reconcile succeeded\n  tbs-full-deps-repository  (imgpkg)                                                                          Reconcile succeeded\n                            harbor.services.h2o-2-9349.h2o.vmware.com/buildservice/tbs-full-deps:1.10.8\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tbs-full-dependencies-package","title":"Install TBS Full Dependencies Package","text":"<p>We can now install the TBS Full Dependencies package8:</p> <pre><code>tanzu package install full-tbs-deps \\\n-p full-tbs-deps.tanzu.vmware.com \\\n-v ${TBS_VERSION} \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>And then verify the package is installed and reconciled correctly:</p> <pre><code>tanzu package installed get full-tbs-deps -n $TAP_INSTALL_NAMESPACE\n</code></pre> <p>Which should return something like this:</p> <pre><code>NAMESPACE:          tap-install\nNAME:               full-tbs-deps\nPACKAGE-NAME:       full-tbs-deps.tanzu.vmware.com\nPACKAGE-VERSION:    1.10.8\nSTATUS:             Reconcile succeeded\nCONDITIONS:         - type: ReconcileSucceeded\n  status: \"True\"\nreason: \"\"\nmessage: \"\"\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-test-application","title":"Install Test Application","text":"<ul> <li>explain workloads</li> <li>CLI</li> <li>namespace &amp; namespace provisioner</li> <li>?</li> </ul> <p>One of the goals of TAP is to be flexible, to support the various ways people build and run applications.</p> <p>Cartographer's Supply Chains let you define any workflow you want for any kind of resource you can express in Kubernetes.</p> <p>While that is interesting, starting with the basics, we focus on the <code>batteries included</code> part. Which gives you three main workflows:</p> <ul> <li>build an application</li> <li>run an application</li> <li>both build &amp; run an application</li> </ul> <p>Each of the three Out-Of-The-Box (OOTB) Supply Chains comes with two <code>ClusterSupplyChain</code> CRs.</p> <ul> <li><code>x-image-to-url</code></li> <li><code>source-x-to-url</code></li> </ul> <p>The Source to Image journey is defined by the <code>Workload</code> CR. The Image to URL journey is defined by the <code>Deliverable</code> CR.</p> <p>When you start at the Source, the OOTB Supply Chains generate the <code>Deliverable</code> CR for you.</p> <p>To test if our TAP machinery works as intended, we'll stick to creating a <code>Workload</code> that uses the <code>source-to-url</code> Supply Chain.</p> <p>In order to the tools used by the Supply Chain to do their work, they need the appropriate Secrets and RBAC permissions. So we start with setting up a Developer Namespace.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#set-up-developer-namespace","title":"Set Up Developer Namespace","text":"<p>Starting with TAP 1.5, it includes a package called the Namespace Provisioner10.</p> <p>This let's us configure Developer Namespaces by adding Label.</p> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\n</code></pre> Via kubectlVia Manifest <pre><code>kubectl create namespace ${TAP_DEVELOPER_NAMESPACE}\nkubectl label namespace ${TAP_DEVELOPER_NAMESPACE} \\\napps.tanzu.vmware.com/tap-ns=\"\"\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt; tap-dev-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    apps.tanzu.vmware.com/tap-ns: \"\"\n    kubernetes.io/metadata.name: ${TAP_DEVELOPER_NAMESPACE}\n  name: ${TAP_DEVELOPER_NAMESPACE}\nEOF\n</code></pre> <pre><code>kubectl apply -f tap-dev-namespace.yaml\n</code></pre> <p>If we wait a few moments, we can then see the Namespace contains Secrets and RoleBindings:</p> <pre><code>kubectl get secret,rolebinding -n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>Which shows something like this:</p> <pre><code>NAME                            TYPE                             DATA   AGE\nsecret/registries-credentials   kubernetes.io/dockerconfigjson   1      25d\n\nNAME                                                            ROLE                   AGE\nrolebinding.rbac.authorization.k8s.io/default-permit-workload   ClusterRole/workload   25d\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#create-workload","title":"Create Workload","text":"<p>Now that we have a Namespace to work in, we can define a Workload.</p> <p>We can then either use the CLI or the <code>Workload</code> CR to create our test workload.</p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create smoke-app \\\n--git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=smoke-app \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--yes \\\n-n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: smoke-app\n    apps.tanzu.vmware.com/workload-type: web\n  name: smoke-app\n  namespace: ${TAP_DEVELOPER_NAMESPACE}\nspec:\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/sample-accelerators/tanzu-java-web-app.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>Use <code>kubectl wait</code> to wait for the app to be ready.</p> <pre><code>kubectl wait --for=condition=Ready Workload smoke-app --timeout=10m -n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#verify-workload","title":"Verify Workload","text":"<p>To see the logs:</p> <pre><code>tanzu apps workload tail smoke-app\n</code></pre> <p>To get the status:</p> <pre><code>tanzu apps workload get smoke-app\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#delete-workload","title":"Delete Workload","text":"<p>And then we can delete our test workload if want to.</p> <pre><code>tanzu apps workload delete smoke-app -y -n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#links","title":"Links","text":"<ol> <li> <p>TAP 1.5 - Prerequisites \u21a9</p> </li> <li> <p>TAP 1.5 - Supported Kubernetes versions \u21a9</p> </li> <li> <p>TAP 1.5 - Relocate images to a registry \u21a9</p> </li> <li> <p>TAP 1.5 - Cluster Essentials \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>KAPP Controller - Overview \u21a9</p> </li> <li> <p>SecretGen Controller - GitHub page \u21a9</p> </li> <li> <p>TAP 1.5 - Full profile example \u21a9</p> </li> <li> <p>TAP 1.5 - Handle TBS Depencies \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>TAP 1.5 - Install TAP Package \u21a9</p> </li> <li> <p>TAP 1.5 - Namespace Provisioner \u21a9</p> </li> <li> <p>TAP 1.5 - Trust CA for Workload specifically \u21a9</p> </li> <li> <p>KAPP Controller - Configure Controller to Trust custom CA \u21a9</p> </li> <li> <p>Crossplane - Configure Self-signed CA Certs support \u21a9</p> </li> <li> <p>TAP 1.5 - Bitnami Services \u21a9</p> </li> <li> <p>Crossplane - The Cloud Native Control Plane Framework \u21a9</p> </li> </ol>","tags":["tap","kubernetes","install"]},{"location":"install/gitops/","title":"TAP GitOps Install","text":"","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#goal-outcome-checks","title":"Goal, Outcome &amp; Checks","text":"<p>Goal: Complete an installation of TAP for a Customer with GitOps and namespace management ready for production usage as an Operator</p> <p>Outcome:  TAP install and updates is controlled via chages in Git only</p> <ul> <li> I am able to manage a TAP installation with GitOPs</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#raw-notes","title":"Raw Notes","text":"<ul> <li>h2o -&gt; so we use SOPS</li> <li>Prerequisites</li> <li>Accept EULAs</li> <li>Install Cluster Essentials</li> <li>Install TAP with GitOps (SOPS)</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#install-cluster-essentials","title":"Install Cluster Essentials","text":"<ul> <li>https://docs.vmware.com/en/Cluster-Essentials-for-VMware-Tanzu/1.5/cluster-essentials/deploy.html</li> <li>Login to Tanzu Network</li> <li>Download Cluster Essentials</li> <li>Upload to Jump Host</li> <li>Create Namespace</li> <li>Create ConfigMap for CA</li> </ul> <pre><code>kubectl create namespace kapp-controller\n</code></pre> <pre><code>kubectl create secret generic kapp-controller-config \\\n--namespace kapp-controller \\\n--from-file caCerts=ssl/ca.crt\n</code></pre> <pre><code>mkdir $HOME/tanzu-cluster-essentials\ntar -xvf $HOME/scripts/scripts/tanzu-cluster-essentials-linux-amd64-1.5.0.tgz -C $HOME/tanzu-cluster-essentials\n</code></pre> <pre><code>export INSTALL_BUNDLE=registry.tanzu.vmware.com/tanzu-cluster-essentials/cluster-essentials-bundle@sha256:79abddbc3b49b44fc368fede0dab93c266ff7c1fe305e2d555ed52d00361b446\nexport INSTALL_REGISTRY_HOSTNAME=registry.tanzu.vmware.com\nexport INSTALL_REGISTRY_USERNAME=TANZU-NET-USER\nexport INSTALL_REGISTRY_PASSWORD=TANZU-NET-PASSWORD\n</code></pre> <pre><code>cd $HOME/tanzu-cluster-essentials\n</code></pre> <pre><code>./install.sh --yes\n</code></pre> <p>NOTE: looks like my cluster already has the essentials installed -&gt; part of TGKm?</p>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#gitops-install","title":"GitOps Install","text":"<ul> <li>install Age -&gt; https://github.com/FiloSottile/age#installation</li> <li>create new repo in Gitea -&gt; <code>gitops-iterate-01</code></li> <li>sync it</li> <li>download Reference Implementation</li> <li>ingest reference implmentation (script)</li> <li>run cluster config init script</li> <li>update git repo</li> <li></li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#init-repo","title":"Init Repo","text":"<pre><code>mkdir -p $HOME/projects/gitops-iterate-01\ncd $HOME/projects/gitops-iterate-01\ngit config --global init.defaultBranch main\n</code></pre> <pre><code>touch README.md\ngit init\ngit checkout -b main\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin https://gitea.services.h2o-2-9349.h2o.vmware.com/gitea/gitops-iterate-01.git\ngit push -u origin main\n</code></pre> <pre><code>scp ~/Downloads/tanzu-gitops-ri-0.1.0.tgz ubuntu@10.220.13.174:/home/ubuntu\n</code></pre> <pre><code>tar xvf $HOME/tanzu-gitops-ri-0.1.0.tgz  -C $HOME/projects/gitops-iterate-01\n</code></pre> <pre><code>./setup-repo.sh iterate sops\n</code></pre> <pre><code>git add . &amp;&amp; git commit -m \"Add iterate\"\ngit push -u origin\n</code></pre> <pre><code>cd clusters/iterate\nless docs/README.md\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#create-sops-key","title":"Create SOPS Key","text":"<pre><code>mkdir -p $HOME/tmp-enc\nchmod 700 $HOME/tmp-enc\ncd $HOME/tmp-enc\n</code></pre> <pre><code>age-keygen -o key.txt\n</code></pre> <ul> <li>create <code>tap-sensitive-values.yaml</code></li> </ul> <pre><code>---\ntap_install:\nsensitive_values:\n</code></pre> <pre><code>export SOPS_AGE_RECIPIENTS=$(cat key.txt | grep \"# public key: \" | sed 's/# public key: //')\n</code></pre> <pre><code>sops --encrypt tap-sensitive-values.yaml &gt; tap-sensitive-values.sops.yaml\n</code></pre> <pre><code>export SOPS_AGE_KEY_FILE=key.txt\n</code></pre> <pre><code>sops --decrypt tap-sensitive-values.sops.yaml\n</code></pre> <pre><code>cp tap-sensitive-values.sops.yaml $HOME/projects/gitops-iterate-01/clusters/iterate/cluster-config/values/\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#prepare-values","title":"Prepare Values","text":"<ul> <li>use iterate: https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/multicluster-reference-tap-values-iterate-sample.html</li> <li>create <code>tap-non-sensitive-values.yaml</code><ul> <li>at <code>&lt;GIT-REPO-ROOT&gt;/clusters/&lt;CLUSTER-NAME&gt;/cluster-config/values/tap-non-sensitive-values.yaml</code></li> <li><code>vim clusters/iterate/cluster-config/values/tap-non-sensitive-values.yaml</code></li> </ul> </li> </ul> <pre><code>tap_install:\nvalues:\nceip_policy_disclosed: true\nexcluded_packages:\n- policy.apps.tanzu.vmware.com\n</code></pre> <pre><code>tap_install:\nvalues:\nprofile: iterate\nceip_policy_disclosed: true\nshared:\ningress_domain: \"iterate.h2o-2-9349.h2o.vmware.com\"\nca_cert_data: | # To be passed if using custom certificates\n-----BEGIN CERTIFICATE-----\nMIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\nBQAwgY4xCzAJBgNVBAYTAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAO\nBgNVBAcTB1V0cmVjaHQxFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMU\nS2Vhcm9zIFRhbnp1IFJvb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290\nIENBMB4XDTIyMDMyMzE1MzUwMFoXDTI3MDMyMjE1MzUwMFowgY4xCzAJBgNVBAYT\nAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAOBgNVBAcTB1V0cmVjaHQx\nFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMUS2Vhcm9zIFRhbnp1IFJv\nb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290IENBMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyZXDL9W2vu365m//E/w8n1M189a5mI9HcTYa\n0xZhnup58Zp72PsgzujI/fQe43JEeC+aIOcmsoDaQ/uqRi8p8phU5/poxKCbe9SM\nf1OflLD9k2dwte6OV5kcSUbVOgScKL1wGEo5mdOiTFrEp5aLBUcbUeJMYz2IqLVa\nv52H0vTzGfmrfSm/PQb+5qnCE5D88DREqKtWdWW2bCW0HhxVHk6XX/FKD2Z0FHWI\nChejeaiarXqWBI94BANbOAOmlhjjyJekT5hL1gh7BuCLbiE+A53kWnXO6Xb/eyuJ\nobr+uHLJldoJq7SFyvxrDd/8LAJD4XMCEz+3gWjYDXMH7GfPWwIDAQABo0IwQDAO\nBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUfGU50Pe9\nYTv5SFvGVOz6R7ddPcUwDQYJKoZIhvcNAQELBQADggEBAHMoNDxy9/kL4nW0Bhc5\nGn0mD8xqt+qpLGgChlsMPNR0xPW04YDotm+GmZHZg1t6vE8WPKsktcuv76d+hX4A\nuhXXGS9D0FeC6I6j6dOIW7Sbd3iAQQopwICYFL9EFA+QAINeY/Y99Lf3B11JfLU8\njN9uGHKFI0FVwHX428ObVrDi3+OCNewQ3fLmrRQe6F6q2OU899huCg+eYECWvxZR\na3SlVZmYnefbA87jI2FRHUPqxp4P2mDwj/RZxhgIobhw0zz08sqC6DW0Aj1OIJe5\nsDAm0uiUdqs7FZN2uKkLKekdTgW0QkTFEJTk5Yk9t/hOrjnHoWQfB+mLhO3vPhip\nvhs=\n-----END CERTIFICATE-----\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nkp_default_repository: \"harbor.services.h2o-2-9349.h2o.vmware.com/buildservice/tbs-full-deps\"\nsupply_chain: basic\nootb_supply_chain_basic:\nregistry:\nserver: harbor.services.h2o-2-9349.h2o.vmware.com\nrepository: tap-apps\nimage_policy_webhook:\nallow_unmatched_tags: true\ncontour:\nenvoy:\nservice:\ntype: LoadBalancer cnrs:\ndomain_name: \"iterate.h2o-2-9349.h2o.vmware.com\"\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: appliveview.view.h2o-2-9349.h2o.vmware.com\nexcluded_packages:\n- policy.apps.tanzu.vmware.com\n- scanning.apps.tanzu.vmware.com\n- grype.scanning.apps.tanzu.vmware.com\n</code></pre> <pre><code>---\ntap_install:\nsensitive_values:\nshared:\nimage_registry:\nproject_path: \"harbor.services.h2o-2-9349.h2o.vmware.com/tap\"\nusername: \"\"\npassword: ''\nbuildservice:\nkp_default_repository_username: \"\"\nkp_default_repository_password: ''\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#setup-sync","title":"Setup Sync","text":"","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#git-ssh","title":"Git SSH","text":"<pre><code>ssh-keyscan gitssh.h2o-2-9349.h2o.vmware.com &gt; gitea-known-hosts.txt\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#sync-script","title":"Sync Script","text":"<pre><code>export INSTALL_REGISTRY_HOSTNAME=harbor.services.h2o-2-9349.h2o.vmware.com\nexport INSTALL_REGISTRY_USERNAME=admin\nexport INSTALL_REGISTRY_PASSWORD='VMware123!'\nexport GIT_SSH_PRIVATE_KEY=$(cat $HOME/.ssh/id_gitea)\nexport GIT_KNOWN_HOSTS=$(ssh-keyscan 1)\nexport SOPS_AGE_KEY=$(cat $HOME/tmp-enc/key.txt)\nexport TAP_PKGR_REPO=harbor.services.h2o-2-9349.h2o.vmware.com/tap/tap-packages\n</code></pre> <pre><code>./tanzu-sync/scripts/configure.sh\n</code></pre> <pre><code>git add cluster-config/ tanzu-sync/\ngit commit -m \"Configure install of TAP 1.5.0\"\ngit push\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#deploy-script","title":"Deploy Script","text":"<p>Warning</p> <p>Run this with your Kubernetes context set to your target cluster!</p> <pre><code>kubectx tap-iterate-admin@tap-iterate\n</code></pre> <pre><code>./tanzu-sync/scripts/deploy.sh\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#setup-developer-namespaces","title":"Setup Developer Namespaces","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/install-gitops-set-up-namespaces.html</li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-about.html</li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-customize-installation.html</li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-customize-installation.html#git-install</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#define-gitops-for-namespace-provisioner","title":"Define GitOps For Namespace Provisioner","text":"<ul> <li>create <code>desired-namespaces.yaml</code></li> <li>create <code>namespaces.yaml</code></li> </ul> desired-namespaces.yaml<pre><code>#@data/values\n---\nnamespaces:\n- name: dev\n- name: qa\n</code></pre> namespaces.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#! This loop will now loop over the namespace list in\n#! in ns.yaml and will create those namespaces.\n#@ for ns in data.values.namespaces:\n---\napiVersion: v1\nkind: Namespace\nmetadata:\nname: #@ ns.name\n#@ end\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#configure-namespace-provisioner-in-tap-install-values","title":"Configure Namespace Provisioner in TAP Install Values","text":"<ul> <li>update <code>cluster-config/values/tap-non-sensitive-values.yaml</code></li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-use-case3.html#git-private</li> </ul> <pre><code>tap_install:\nvalues:\n...\nnamespace_provisioner:\ncontroller: false\ngitops_install:\nref: origin/main\nsubPath: clusters/iterate/dev-namespaces\nurl: git@gitssh.h2o-2-9349.h2o.vmware.com:gitea/gitops-iterate-01.git\nsecretRef:\nname: sync-git-ssh\nnamespace: tanzu-sync\ncreate_export: true\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#tanzu-build-service-dependencies","title":"Tanzu Build Service Dependencies","text":"<ul> <li>Inspired by VRabbi: https://vrabbi.cloud/post/tap-1-5-gitops-installation/</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#add-tbs-dependencies-package-and-repository","title":"Add TBS Dependencies Package and Repository","text":"<ul> <li>create <code>cluster-config/config/tbs-install/package-repository.yaml</code></li> <li>create <code>cluster-config/config/tbs-install/package-install.yaml</code></li> </ul> package-repository.yaml<pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\nname: tbs-full-deps-repository\nnamespace: tap-install\nannotations:\nkapp.k14s.io/change-group: pkgr\nspec:\nfetch:\nimgpkgBundle:\nimage: harbor.services.h2o-2-9349.h2o.vmware.com/buildservice/tbs-full-deps:1.10.8\n</code></pre> package-install.yaml<pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\nname: full-tbs-deps\nnamespace: tap-install\nannotations:\nkapp.k14s.io/change-group: tbs\nkapp.k14s.io/change-rule.0: \"upsert after upserting pkgi\"\nkapp.k14s.io/change-rule.1: \"delete before deleting pkgi\"\nspec:\nserviceAccountName: tap-installer-sa\npackageRef:\nrefName: full-tbs-deps.tanzu.vmware.com\nversionSelection:\nconstraints: 1.10.8\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#test-with-workload","title":"Test With Workload","text":"<p>Now that we have a Namespace to work in, we can define a Workload.</p> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#create-workload","title":"Create Workload","text":"<p>We can then either use the CLI or the <code>Workload</code> CR to create our test workload.</p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create smoke-app \\\n--git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=smoke-app \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--yes \\\n-n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: smoke-app\n    apps.tanzu.vmware.com/workload-type: web\n  name: smoke-app\n  namespace: ${TAP_DEVELOPER_NAMESPACE}\nspec:\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/sample-accelerators/tanzu-java-web-app.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>To verify the status of FluxCD checkout:</p> <pre><code>kubectl get gitrepo -A\n</code></pre> <p>Use <code>kubectl wait</code> to wait for the app to be ready.</p> <pre><code>kubectl wait --for=condition=Ready Workload smoke-app --timeout=10m -n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#verify-workload","title":"Verify Workload","text":"<p>To see the logs:</p> <pre><code>tanzu apps workload tail smoke-app\n</code></pre> <p>To get the status:</p> <pre><code>tanzu apps workload get smoke-app\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#delete-workload","title":"Delete Workload","text":"<p>And then we can delete our test workload if want to.</p> <pre><code>tanzu apps workload delete smoke-app -y -n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#links","title":"Links","text":"<ul> <li>Official Docs</li> <li>VRabbi Blog</li> <li>Mozilla SOPS</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/mutlicluster/","title":"Multicluster Install","text":"<p>TODO</p>","tags":["tap","kubernetes","install"]},{"location":"install/mutlicluster/#goal-outcome-checks","title":"Goal, Outcome &amp; Checks","text":"<p>Goal: Complete a multi-cluster installation of TAP for a Customer with GitOps and namespace management ready for production usage as an Operator</p> <p>Outcome:  I can do a full multi-cluster installation of TAP that is managed by GitOps</p> <ul> <li> I understand and have implemented the TAP Multi Cluster Reference Architecture</li> <li> Configure TAP GUI to read resources from multiple clusters</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"jumphost/","title":"Jumphost Required Tools","text":"<ul> <li>needs:</li> <li>4 cpu</li> <li>8gb ram</li> <li>100gb disk</li> <li>download Ubuntu Image from Cloud Images</li> <li>https://cloud-images.ubuntu.com/jammy/20230401/</li> <li>create VM in user-workload network from ubuntu image</li> <li>paste personal SSH pub key</li> <li>add it to additional network</li> <li>set up IP</li> <li><code>ip a</code></li> <li><code>sudo netplan set ethernets.ens224.dhcp4=true</code></li> <li><code>sudo netplan apply</code></li> <li><code>ip a</code></li> <li>remove default route to alt network</li> <li><code>sudo ip r delete default via 172.16.50.1 dev ens224</code></li> <li>https://matteo-magni.github.io/tanzugym/tkgm/bastion/</li> <li>https://cloud-images.ubuntu.com/jammy/20230401/</li> <li>tools:</li> <li>go lang</li> <li>python 3?</li> <li>net-tools</li> <li>git</li> <li>yq</li> <li>jq</li> <li>govc</li> <li>kubectl</li> <li>kubectx</li> <li>kubens</li> <li>Tanzu CLI<ul> <li>TKG Plugins</li> <li>TAP Plugins</li> </ul> </li> <li>Carvel toolsuite</li> <li>curl (&amp; httpie)</li> <li>Docker</li> <li>Kind</li> <li>cfssl</li> <li>minio client</li> <li>Cosign</li> <li>age</li> <li>sops</li> <li>optional tools:<ul> <li>java</li> <li>maven</li> </ul> </li> </ul>"},{"location":"jumphost/#kind-cluster","title":"Kind Cluster","text":"<pre><code>kind create cluster --image kindest/node:v1.24.12@sha256:1e12918b8bc3d4253bc08f640a231bb0d3b2c5a9b28aa3f2ca1aee93e1e8db16 --name tkgm\n</code></pre> <pre><code>tanzu management-cluster create h2o-9349-01  --file tkgm-management-kubevip.yaml  --use-existing-bootstrap-cluster tkgm -v 6\n</code></pre>"},{"location":"jumphost/#age","title":"Age","text":"<ul> <li>https://github.com/FiloSottile/age#installation</li> </ul> <pre><code>sudo apt install age\n</code></pre>"},{"location":"jumphost/#sops","title":"Sops","text":"<ul> <li>https://github.com/mozilla/sops/releases</li> </ul> <pre><code>http --download https://github.com/mozilla/sops/releases/download/v3.7.3/sops_3.7.3_amd64.deb\nsudo apt install ./sops_3.7.3_amd64.deb\n</code></pre>"},{"location":"jumphost/#git","title":"Git","text":"<pre><code>sudo apt install net-tools git\n</code></pre> <pre><code>LOCALBIN=\"/home/ubuntu/.local/bin\"\nmkdir -p $LOCALBIN\nexport PATH=$PATH:$LOCALBIN\n</code></pre>"},{"location":"jumphost/#minio","title":"MinIO","text":"<ul> <li>MinIO: https://min.io/docs/minio/linux/reference/minio-mc.html</li> </ul> <pre><code>curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n--create-dirs \\\n-o $HOME/minio-binaries/mc\n\nchmod +x $HOME/minio-binaries/mc\nexport PATH=$PATH:$HOME/minio-binaries/\n\nmc --help\n</code></pre>"},{"location":"jumphost/#kubectl","title":"Kubectl","text":"<pre><code>curl -sSfLO \"https://dl.k8s.io/release/$(curl -sSfL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ninstall kubectl ${LOCALBIN}/kubectl\n</code></pre>"},{"location":"jumphost/#carvel","title":"Carvel","text":"<pre><code>sudo chmod -R 777 /usr/local/bin/\nwget -O- https://carvel.dev/install.sh | bash\n</code></pre>"},{"location":"jumphost/#vcc","title":"VCC","text":"<pre><code>export VCC_USER=\"\"\nexport VCC_PASS='\n</code></pre> <pre><code># list all the available products\nvcc get products\n\n# list all the available subproducts belonging to the vmware_tanzu_kubernetes_grid product\nvcc get subproducts -p vmware_tanzu_kubernetes_grid\n\n# list all the versions for the subproduct tkg\nvcc get versions -p vmware_tanzu_kubernetes_grid -s tkg\n\n# list all the files available for version 2.1.0\n# at this point vcc does the actual login\nvcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.1\n\n# download the tanzu cli\nvcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.1 -f 'tanzu-cli-bundle-linux-amd64.tar.gz' --accepteula\n</code></pre> <pre><code>tar xvzf vcc-downloads/tanzu-cli-bundle-linux-amd64.tar.gz\ninstall cli/core/v0.28.1/tanzu-core-linux_amd64 ${LOCALBIN}/tanzu\n</code></pre>"},{"location":"jumphost/#kind","title":"Kind","text":"<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n</code></pre>"},{"location":"jumphost/#tanzu-cli","title":"Tanzu CLI","text":"<ul> <li>Download Tanzu CLI</li> <li>login to Tanzu Network: https://network.tanzu.vmware.com/</li> <li>download CLI from TAP Download Page: https://network.tanzu.vmware.com/products/tanzu-application-platform</li> </ul> <pre><code>scp tanzu-framework-linux-amd64-v0.28.1.1.tar ubuntu@10.220.13.174:/home/ubuntu/\n</code></pre> <pre><code>mkdir -p $HOME/tanzu\ntar -xvf tanzu-framework-linux-amd64-v0.28.1.1.tar -C $HOME/tanzu\n</code></pre> <pre><code>cd $HOME/tanzu\nexport VERSION=v0.28.1.1.\n# sudo install cli/core/$VERSION/tanzu-core-linux_amd64 /usr/local/bin/tanzu\ntanzu plugin install --local cli all\n</code></pre> <pre><code>tanzu plugin list\n</code></pre> <p>This should now list both TKG and TAP plugins:</p> <pre><code>Standalone Plugins\n  NAME                DESCRIPTION                                                        TARGET      DISCOVERY  VERSION        STATUS\n  accelerator         Manage accelerators in a Kubernetes cluster                                               v1.5.0         installed\n  apps                Applications on Kubernetes                                                                v0.11.1        installed\n  external-secrets    interacts with external-secrets.io resources                                              v0.1.0-beta.4  installed\n  insight             post &amp; query image, package, source, and vulnerability data                               v1.5.0         installed\n  isolated-cluster    isolated-cluster operations                                                    default    v0.28.1        installed\n  login               Login to the platform                                                          default    v0.28.1        installed\n  pinniped-auth       Pinniped authentication operations (usually not directly invoked)              default    v0.28.1        installed\n  services            Commands for working with service instances, classes and claims                           v0.6.0         installed\n  management-cluster  Kubernetes management-cluster operations                           kubernetes  default    v0.28.1        installed\n  package             Tanzu package management                                           kubernetes  default    v0.28.1        installed\n  secret              Tanzu secret management                                            kubernetes  default    v0.28.1        installed\n  telemetry           Configure cluster-wide telemetry settings                          kubernetes  default    v0.28.1        installed\n\nPlugins from Context:  tkgm-m\n  NAME                DESCRIPTION                           TARGET      VERSION  STATUS\n  cluster             Kubernetes cluster operations         kubernetes  v0.28.1  installed\n  feature             Operate on features and featuregates  kubernetes  v0.28.1  installed\n  kubernetes-release  Kubernetes release op erations         kubernetes  v0.28.1  installed\n</code></pre>"},{"location":"jumphost/#cosign","title":"Cosign","text":"<ul> <li>https://docs.sigstore.dev/cosign/installation/</li> </ul> <pre><code>wget \"https://github.com/sigstore/cosign/releases/download/v2.0.0/cosign-linux-amd64\"\nmv cosign-linux-amd64 /usr/local/bin/cosign\nchmod +x /usr/local/bin/cosign\n</code></pre>"},{"location":"supply-chain/basic-to-test-scan/","title":"Test & Scan Supply Chain","text":"<p>TODO</p>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/custom-notes/","title":"Notes On Workshop","text":""},{"location":"supply-chain/custom-notes/#cartographer","title":"Cartographer","text":"<ul> <li>harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/tap-demo-03-dev@sha256:5e7615dd7e9e02dc32b96e089a87870c19d55d143ecb9eea3eb701ad8cba8013 git</li> <li>ghcr.io/joostvdg/go-demo:v2.1.16</li> <li>https://github.com/joostvdg/go-demo/blob/main/k8s/deployment.yaml</li> </ul> <p>To construct and use a Supply Chain, we need the following ingredients:</p> <ol> <li>One or more Resources, usually Cartographer templates</li> <li>A SupplyChain definition, using those Resources</li> <li>A Workload definition that selects the Supply Chain</li> </ol>"},{"location":"supply-chain/custom-notes/#my-first-template","title":"My First Template","text":"<p>Relies on Workload Definition</p> <p>Cartographer works from a Workload definition.</p> <p>So any template, like the ClusterTemplate below, works because the trigger is a Workload CR.</p> cluster-template-app-deploy.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy\nspec:\ntemplate:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\nlabels:\napp: $(workload.metadata.name)$\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: $(workload.metadata.name)$\ntemplate:\nmetadata:\nlabels:\napp: $(workload.metadata.name)$\nspec:\ncontainers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\n</code></pre> <p>For the Workload definition, it needs to supply the following fields:</p> <ul> <li><code>metadata.name</code></li> <li><code>spec.image</code></li> </ul>"},{"location":"supply-chain/custom-notes/#supply-chain-definition","title":"Supply Chain Definition","text":"<p>The supply chain has three top level fields in its spec, the resources, a service account reference and a selector for workloads.</p> <ol> <li>Resources: Which CRs are used (and how)</li> <li>ServiceAccount Reference: reference to a ServiceAccount that can use the Templates/Resources</li> <li>Selector: Set of Kubernetes Labels which you have to specify on a Workload to trigger this Supply Chain</li> </ol> my-first-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: my-first-supply-chain\nspec:\nresources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy\nserviceAccountRef:\nname: cartographer-pre-built-sa\nnamespace: default\nselector:\nworkload-type: pre-built\n</code></pre>"},{"location":"supply-chain/custom-notes/#workload-definition","title":"Workload Definition","text":"<p>Remember, we need to specify the following fields:</p> <ul> <li><code>metadata.name</code></li> <li><code>spec.image</code></li> </ul> workload-pre-built-hello.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello\nlabels:\nworkload-type: pre-built\nspec:\nimage: ghcr.io/joostvdg/go-demo:v2.1.16\n</code></pre>"},{"location":"supply-chain/custom-notes/#rbac","title":"RBAC","text":"rbac.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cartographer-pre-built-sa\nnamespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: deploy-image-role\nrules:\n- apiGroups:\n- apps\nresources:\n- deployments\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-prebuilt-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: deploy-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-pre-built-sa\n</code></pre>"},{"location":"supply-chain/custom-notes/#putting-it-together","title":"Putting it together","text":"<pre><code>kubectl apply -f rbac.yaml\nkubectl apply -f cluster-template-app-deploy.yaml\nkubectl apply -f my-first-supply-chain.yaml\nkuebctl apply -f workload-pre-built-hello.yaml\n</code></pre> <pre><code>kubectl tree workload hello\n</code></pre> <pre><code>NAMESPACE  NAME                                       READY  REASON  AGE\ndefault    Workload/hello                             True   Ready   98m\ndefault    \u2514\u2500Deployment/hello-deployment              -              98m\ndefault      \u2514\u2500ReplicaSet/hello-deployment-cfdf74d6   -              98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-kjjp5  True           98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-wgtvl  True           98m\ndefault        \u2514\u2500Pod/hello-deployment-cfdf74d6-x2pmc  True           98m\n</code></pre> <pre><code>kubectl port-forward deployment/hello-deployment 8080:8080\n</code></pre> <pre><code>curl http://localhost:8080/\n</code></pre>"},{"location":"supply-chain/custom-notes/#with-params","title":"With Params","text":"<ul> <li>https://cartographer.sh/docs/v0.7.0/tutorials/using-params/</li> </ul> <pre><code>kubectl port-forward deployment/hello-deployment 8080:8080\n</code></pre> <pre><code>curl http://localhost:8080/\n</code></pre>"},{"location":"supply-chain/custom-notes/#extending","title":"Extending","text":"<ul> <li>https://cartographer.sh/docs/v0.7.0/tutorials/extending-a-supply-chain/</li> <li>https://github.com/paketo-buildpacks/go-build</li> <li>https://github.com/joostvdg/go-demo/</li> <li>https://github.com/pivotal/kpack/blob/main/docs/image.md</li> </ul> <pre><code>tanzu apps workload tail source-code-01 --namespace dev --timestamp --since 1h\n</code></pre>"},{"location":"supply-chain/custom-notes/#tekton-lifecycle","title":"Tekton LifeCycle","text":""},{"location":"supply-chain/custom-notes/#ootb-supply-chain","title":"OOTB Supply Chain","text":""},{"location":"supply-chain/custom-notes/#build-cluster","title":"Build Cluster","text":"<pre><code>kubectl tree workload -n dev tap-demo-03   </code></pre> <pre><code>NAMESPACE  NAME                                                READY  REASON              AGE\ndev        Workload/tap-demo-03                                True   Ready               5d2h\ndev        \u251c\u2500ConfigMap/tap-demo-03                             -                          5d\ndev        \u251c\u2500ConfigMap/tap-demo-03-deliverable                 -                          5d2h\ndev        \u251c\u2500ConfigMap/tap-demo-03-with-api-descriptors        -                          5d\ndev        \u251c\u2500ConfigMap/tap-demo-03-with-claims                 -                          5d\ndev        \u251c\u2500GitRepository/tap-demo-03                         True   Succeeded           5d2h\ndev        \u251c\u2500Image/tap-demo-03                                 True                       5d1h\ndev        \u2502 \u251c\u2500Build/tap-demo-03-build-1                       -                          5d1h\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-build-1-build-pod             False  PodCompleted        5d1h\ndev        \u2502 \u251c\u2500Build/tap-demo-03-build-2                       -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-build-2-build-pod             False  PodCompleted        5d\ndev        \u2502 \u251c\u2500Build/tap-demo-03-build-3                       -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-build-3-build-pod             False  PodCompleted        5d\ndev        \u2502 \u251c\u2500PersistentVolumeClaim/tap-demo-03-cache         -                          5d1h\ndev        \u2502 \u2514\u2500SourceResolver/tap-demo-03-source               True                       5d1h\ndev        \u251c\u2500ImageScan/tap-demo-03                             -                          5d1h\ndev        \u2502 \u251c\u2500TaskRun/scan-tap-demo-03-29z7x                  -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/scan-tap-demo-03-29z7x-pod                False  PodCompleted        5d\ndev        \u2502 \u251c\u2500TaskRun/scan-tap-demo-03-2hwn7                  -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/scan-tap-demo-03-2hwn7-pod                False  PodCompleted        5d\ndev        \u2502 \u251c\u2500TaskRun/scan-tap-demo-03-bzrb9                  -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/scan-tap-demo-03-bzrb9-pod                False  PodCompleted        5d\ndev        \u2502 \u2514\u2500TaskRun/scan-tap-demo-03-phsmz                  -                          5d1h\ndev        \u2502   \u2514\u2500Pod/scan-tap-demo-03-phsmz-pod                False  PodCompleted        5d1h\ndev        \u251c\u2500PodIntent/tap-demo-03                             True   ConventionsApplied  5d\ndev        \u251c\u2500Runnable/tap-demo-03                              True   Ready               5d2h\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-46rnd                   -                          5d\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-16560cea6b          -                          5d\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-46rnd-fetch-repository    -                          5d\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-46rnd-fetch-repository-pod  False  PodCompleted        5d\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-46rnd-maven-run           -                          5d\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-46rnd-maven-run-pod         False  PodCompleted        5d\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-66fp8                   -                          5d\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-8bd0367a05          -                          5d\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-66fp8-fetch-repository    -                          5d\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-66fp8-fetch-repository-pod  False  PodCompleted        5d\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-66fp8-maven-run           -                          5d\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-66fp8-maven-run-pod         False  PodCompleted        5d\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-6xtlf                   -                          5d\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-8bd736ceef          -                          5d\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-6xtlf-fetch-repository    -                          5d\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-6xtlf-fetch-repository-pod  False  PodCompleted        5d\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-6xtlf-maven-run           -                          5d\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-6xtlf-maven-run-pod         False  PodFailed           5d\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-ff5dn                   -                          5d1h\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-b5da4a07af          -                          5d1h\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-ff5dn-fetch-repository    -                          5d1h\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-ff5dn-fetch-repository-pod  False  PodCompleted        5d1h\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-ff5dn-maven-run           -                          5d1h\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-ff5dn-maven-run-pod         False  PodFailed           5d1h\ndev        \u2502 \u2514\u2500PipelineRun/tap-demo-03-wfswt                   -                          5d2h\ndev        \u2502   \u251c\u2500PersistentVolumeClaim/pvc-29e80c6f81          -                          5d2h\ndev        \u2502   \u251c\u2500TaskRun/tap-demo-03-wfswt-fetch-repository    -                          5d2h\ndev        \u2502   \u2502 \u2514\u2500Pod/tap-demo-03-wfswt-fetch-repository-pod  False  PodCompleted        5d2h\ndev        \u2502   \u2514\u2500TaskRun/tap-demo-03-wfswt-maven-run           -                          5d2h\ndev        \u2502     \u2514\u2500Pod/tap-demo-03-wfswt-maven-run-pod         False  PodCompleted        5d2h\ndev        \u251c\u2500Runnable/tap-demo-03-config-writer                True   Ready               5d\ndev        \u2502 \u251c\u2500TaskRun/tap-demo-03-config-writer-gllk2         -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-config-writer-gllk2-pod       False  PodCompleted        5d\ndev        \u2502 \u2514\u2500TaskRun/tap-demo-03-config-writer-qgsk5         -                          5d\ndev        \u2502   \u2514\u2500Pod/tap-demo-03-config-writer-qgsk5-pod       False  PodCompleted        5d\ndev        \u2514\u2500SourceScan/tap-demo-03                            -                          5d1h\ndev          \u251c\u2500TaskRun/scan-tap-demo-03-7dnjg                  -                          5d\ndev          \u2502 \u2514\u2500Pod/scan-tap-demo-03-7dnjg-pod                False  PodCompleted        5d\ndev          \u251c\u2500TaskRun/scan-tap-demo-03-9vlxv                  -                          5d\ndev          \u2502 \u2514\u2500Pod/scan-tap-demo-03-9vlxv-pod                False  PodCompleted        5d\ndev          \u251c\u2500TaskRun/scan-tap-demo-03-kv2sv                  -                          5d\ndev          \u2502 \u2514\u2500Pod/scan-tap-demo-03-kv2sv-pod                False  PodCompleted        5d\ndev          \u2514\u2500TaskRun/scan-tap-demo-03-nf6j6                  -                          5d1h\ndev            \u2514\u2500Pod/scan-tap-demo-03-nf6j6-pod                False  PodCompleted        5d1h\n</code></pre>"},{"location":"supply-chain/custom-notes/#run-cluster","title":"Run Cluster","text":"<pre><code>kubectl tree deliverable tap-demo-03 -n apps\n</code></pre> <pre><code>NAMESPACE  NAME                                    READY  REASON  AGE\napps       Deliverable/tap-demo-03                 True   Ready   4d23h\napps       \u251c\u2500App/tap-demo-03                       -              4d23h\napps       \u2514\u2500ImageRepository/tap-demo-03-delivery  True   Ready   4d23h\n</code></pre>"},{"location":"supply-chain/custom-notes/#iterate-cluster","title":"Iterate Cluster","text":"<pre><code>kubectl tree workload -n dev smoke-app\n</code></pre> <pre><code>NAMESPACE  NAME                                         READY  REASON              AGE\ndev        Workload/smoke-app                           True   Ready               2d23h\ndev        \u251c\u2500ConfigMap/smoke-app                        -                          2d23h\ndev        \u251c\u2500ConfigMap/smoke-app-with-api-descriptors   -                          2d23h\ndev        \u251c\u2500ConfigMap/smoke-app-with-claims            -                          2d23h\ndev        \u251c\u2500Deliverable/smoke-app                      True   Ready               2d23h\ndev        \u2502 \u251c\u2500App/smoke-app                            -                          2d23h\ndev        \u2502 \u2514\u2500ImageRepository/smoke-app-delivery       True   Ready               2d23h\ndev        \u251c\u2500GitRepository/smoke-app                    True   Succeeded           2d23h\ndev        \u251c\u2500Image/smoke-app                            True                       2d23h\ndev        \u2502 \u251c\u2500Build/smoke-app-build-1                  -                          2d23h\ndev        \u2502 \u2502 \u2514\u2500Pod/smoke-app-build-1-build-pod        False  PodCompleted        2d23h\ndev        \u2502 \u251c\u2500PersistentVolumeClaim/smoke-app-cache    -                          2d23h\ndev        \u2502 \u2514\u2500SourceResolver/smoke-app-source          True                       2d23h\ndev        \u251c\u2500PodIntent/smoke-app                        True   ConventionsApplied  2d23h\ndev        \u2514\u2500Runnable/smoke-app-config-writer           True   Ready               2d23h\ndev          \u2514\u2500TaskRun/smoke-app-config-writer-wdlzc    -                          2d23h\ndev            \u2514\u2500Pod/smoke-app-config-writer-wdlzc-pod  False  PodCompleted        2d23h\n</code></pre> <pre><code>kubectl tree deliverable -n dev smoke-app\n</code></pre> <pre><code>NAMESPACE  NAME                                  READY  REASON  AGE\ndev        Deliverable/smoke-app                 True   Ready   2d23h\ndev        \u251c\u2500App/smoke-app                       -              2d23h\ndev        \u2514\u2500ImageRepository/smoke-app-delivery  True   Ready   2d23h\n</code></pre>"},{"location":"supply-chain/custom-notes/#modify-ootb-supply-chains","title":"Modify OOTB Supply Chains","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/scc-authoring-supply-chains.html#modifying-an-out-of-the-box-supply-chain-2</li> </ul> <p>To change the shape of a supply chain or the template that it points to, do the following:</p> <ol> <li>Copy one of the reference supply chains.</li> <li>Remove the old supply chain. See preventing Tanzu Application Platform supply chains from being installed.</li> <li>Edit the supply chain object.</li> <li>Submit the modified supply chain to the cluster</li> </ol>"},{"location":"supply-chain/custom-notes/#tekton-tutorial","title":"Tekton Tutorial","text":"<ul> <li>https://tekton.dev/docs/getting-started/</li> <li>https://tekton.dev/docs/getting-started/tasks/</li> <li>https://cartographer.sh/docs/v0.7.0/tutorials/lifecycle/</li> <li>https://tekton.dev/docs/results/</li> <li>https://github.com/tektoncd/catalog/tree/main/task</li> <li>Adding custom behavior to Supply Chains: https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/scc-authoring-supply-chains.html#adding-custom-behavior-to-supply-chains-7</li> </ul>"},{"location":"supply-chain/custom-notes/#tekton-hello-world","title":"Tekton Hello World","text":"<pre><code>export TAP_DEVELOPMENT_NAMESPACE=dev\n</code></pre> <pre><code>kubectl apply -f 01-task-hello.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl apply -f 02-task-run.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl get taskrun hello-task-run -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl -n ${TAP_DEVELOPMENT_NAMESPACE} logs hello-task-run-pod -c step-echo\n</code></pre> <pre><code>kubectl delete -f 01-task-hello.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\nkubectl delete -f 02-task-run.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre>"},{"location":"supply-chain/custom-notes/#tekton-pipeline","title":"Tekton Pipeline","text":"<pre><code>kubectl -n dev logs hello-goodbye-run-goodbye-pod -c step-goodbye\n</code></pre> <pre><code>kubectl create -f 04-pipeline-run-dynamic.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl get taskrun,pipelinerun -n dev\n</code></pre>"},{"location":"supply-chain/custom-notes/#tekton-pipeline-with-workspace","title":"Tekton Pipeline With Workspace","text":"<pre><code>kubectl get taskrun,pipelinerun -n dev\n</code></pre> <pre><code>kubectl get pod  -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl -n ${TAP_DEVELOPMENT_NAMESPACE} logs  clone-git-next-tag-run-z5vrr-git-next-tag-pod\n</code></pre> <p>``sh kubectl get taskrun -n dev -l tekton.dev/task=git-next-tag <pre><code>```sh\nkubectl get taskrun -n dev -l tekton.dev/task=git-next-tag -ojson | jq '.items | map(.status.taskResults)'\n</code></pre></p>"},{"location":"supply-chain/custom-notes/#app-deploy-04","title":"App Deploy 04","text":"<pre><code>kubectl tree workload -n dev hello-again\n</code></pre> <pre><code>tanzu apps workload tail hello-04 --namespace dev --timestamp --since 1h\n</code></pre> <pre><code>kubectl port-forward deployment/hello-deployment 8080:8080\n</code></pre> <pre><code>curl http://localhost:8080/\n</code></pre>"},{"location":"supply-chain/custom-notes/#examples","title":"Examples","text":"<ul> <li>ClusterSupplyChain</li> <li>ClusterSourceTemplate<ul> <li>ClusterRunTemplate</li> <li>PipelineRun<ul> <li>Task</li> <li>TaskRun (Generated)<ul> <li>Pod (Generated)</li> <li>InitContainer -&gt; Shell (Generated)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nspec:\nresources:\n- name: source-tester\nsources:\n- name: source\nresource: source-provider\ntemplateRef:\nkind: ClusterSourceTemplate\nname: testing-pipeline-workspace\n</code></pre> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSourceTemplate\nspec:\nytt: ...\n</code></pre> ClusterRunTemplate-tektonsource-pipelinerun.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterRunTemplate\nmetadata:\nannotations:\nkapp.k14s.io/identity: v1;/carto.run/ClusterRunTemplate/tekton-source-pipelinerun;carto.run/v1alpha1\nkapp.k14s.io/original: '{\"apiVersion\":\"carto.run/v1alpha1\",\"kind\":\"ClusterRunTemplate\",\"metadata\":{\"labels\":{\"kapp.k14s.io/app\":\"1683272492779471078\",\"kapp.k14s.io/association\":\"v1.72b4cf08dac7ed1e3cac533f6a62ff76\"},\"name\":\"tekton-source-pipelinerun\"},\"spec\":{\"outputs\":{\"revision\":\"spec.params[?(@.name==\\\"source-revision\\\")].value\",\"url\":\"spec.params[?(@.name==\\\"source-url\\\")].value\"},\"template\":{\"apiVersion\":\"tekton.dev/v1beta1\",\"kind\":\"PipelineRun\",\"metadata\":{\"generateName\":\"$(runnable.metadata.name)$-\",\"labels\":\"$(runnable.metadata.labels)$\"},\"spec\":{\"params\":\"$(runnable.spec.inputs.tekton-params)$\",\"pipelineRef\":{\"name\":\"$(selected.metadata.name)$\"}}}}}'\nkapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3\ncreationTimestamp: \"2023-05-05T07:41:34Z\"\ngeneration: 1\nlabels:\nkapp.k14s.io/app: \"1683272492779471078\"\nkapp.k14s.io/association: v1.72b4cf08dac7ed1e3cac533f6a62ff76\nname: tekton-source-pipelinerun\nresourceVersion: \"10224684\"\nuid: 34c133ce-228d-4f0d-85df-fe60bae905f7\nspec:\noutputs:\nrevision: spec.params[?(@.name==\"source-revision\")].value\nurl: spec.params[?(@.name==\"source-url\")].value\ntemplate:\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: $(runnable.metadata.name)$-\nlabels: $(runnable.metadata.labels)$\nspec:\nparams: $(runnable.spec.inputs.tekton-params)$\npipelineRef:\nname: $(selected.metadata.name)$\n</code></pre>"},{"location":"supply-chain/custom/","title":"Create Supply Chain","text":"","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#questions","title":"Questions","text":"<ul> <li>FluxCD Webhook instead of Polling</li> <li>Polling must die</li> <li>Git metadata (e.g., Git Webhook data)?</li> <li>Other branches, tags, PRs?</li> <li>Handle multiple commits to same revision in sequence?</li> <li>Build caching?</li> <li>Tekton GUI?</li> <li>Carto Live Editor?</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#goals-outcomes","title":"Goals &amp; Outcomes","text":"<ul> <li>I've written and implemented my own Custom Supply Chain on a Cluster</li> <li>?</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#steps","title":"Steps","text":"<ul> <li>Cartographer Introduction</li> <li>Tekton Introduction</li> <li>Integrating Tekton into Cartographer</li> <li>Compare with OOTB Supply Chains</li> </ul> <p>We start with exploring the core components of Cartographer. First with hardcoded values, then with some templating involved.</p> <p>Eventually, for a Supply Chain, we need something to run commands in a container. For this, we rely on Tekton.</p> <p>Before we add the Tekton bits to the Cartographer Supply Chain, we take a look a Tekton's core components. We then join the two in a more elaborate Supply Chain.</p> <p>And lastly, you should now be able to read and comprehend the OOTB Supply Chains.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#prerequisites","title":"Prerequisites","text":"<ul> <li>TAP Profile installed which includes Tekton and Cartographer</li> <li>Full, Iterate, or Build</li> <li>TAP Developer Namespace setup</li> <li>for RBAC and Repository Credentials</li> <li>Checkout this repository</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#cartographer-introduction","title":"Cartographer Introduction","text":"<p>The Cartographer docs have a good tutorial1, which we'll follow in condensed form.</p> <p>If you'd rather follow the tutorial from there, the files included in this repository have minor changes, it is recommend to compare them. Just make sure to apply the files into a namespace that is configured as a TAP Developer Namespace.</p> <p>We'll start with a the minimal configuration and then build it out:</p> <ol> <li>My First Supply Chain</li> <li>Include Parameters</li> <li>Git Checkout and Image Build</li> </ol> <p>To construct and use a Supply Chain, we need the following ingredients:</p> <ol> <li>One or more Resources, usually Cartographer templates</li> <li>A SupplyChain definition, using those Resources</li> <li>A Workload definition that selects the Supply Chain</li> </ol>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#my-first-template","title":"My First Template","text":"<p>Relies on Workload Definition</p> <p>Cartographer works from a Workload definition.</p> <p>So any template, like the ClusterTemplate below, works because the trigger is a Workload CR.</p> <p>As the name implies, a ClusterTemplate is a resource that generates another resource. You define that other resource via the <code>spec.template</code> field.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy\nspec:\ntemplate:\n</code></pre> <p>In the case of our example, we will templatize a Deployment.</p> <p>Parameters for the template come from upstream resources, such as the Workload that triggers a Supply Chain. We define the parameter as <code>$(source.propery)$</code>, note the double <code>$</code>.</p> <pre><code>template:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\n</code></pre> <p>A Workload is a namespaced Kubernetes CR, which we can leverage to fill in all required (and unique) fields of a Deployment.</p> ClusterTemplate resources/cartographer/app-deploy-01/01-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy\nspec:\ntemplate:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\nlabels:\napp: $(workload.metadata.name)$\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: $(workload.metadata.name)$\ntemplate:\nmetadata:\nlabels:\napp: $(workload.metadata.name)$\nspec:\ncontainers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\n</code></pre> <p>Note the property <code>spec.image</code>. This is something we must define within the Workload to make this Template work.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#supply-chain-definition","title":"Supply Chain Definition","text":"<p>The supply chain has three top level fields in its spec, the resources, a service account reference and a selector for workloads.</p> <ol> <li>Resources: Which CRs are used (and how)</li> <li>ServiceAccount Reference: reference to a ServiceAccount that can use the Templates/Resources</li> <li>Selector: Set of Kubernetes Labels which you have to specify on a Workload to trigger this Supply Chain</li> </ol> <p>The most minimal Supply Chain contains a single Resource. The Cluster Template we just created is one of those resources.</p> <p>There are five types of Resources2:</p> <ul> <li>ClusterTemplate: instructs the supply chain to instantiate a Kubernetes object that has no outputs to be supplied to other objects in the chain</li> <li>ClusterSourceTemplates: indicates how the supply chain could instantiate an object responsible for providing source code</li> <li>ClusterImageTemplates: instructs how the supply chain should instantiate an object responsible for supplying container images</li> <li>ClusterDeploymentTemplate: indicates how the delivery should configure the environment (namespace/cluster)</li> <li>ClusterConfigTemplates: Instructs the supply chain how to instantiate a Kubernetes object that knows how to make Kubernetes configurations available to further resources in the chain</li> </ul> <p>You can find the details of each resource in the docs3</p> <p>One of the goals of Cartographer is to be the abstraction layer above CI/CD Pipelines. As such, the majority of the resources make the most sense with a Cluster scope.</p> <p>To make the disctintion clear, all Cluster scoped CRs start with <code>Cluster</code>. This includes the Supply Chain, so our empty Supply Chain looks like this:</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: my-first-supply-chain\nspec:\nresources: []\nserviceAccountRef: {}\nselector: {}\n</code></pre> <p>Each resource has a <code>name</code> and <code>templateRef</code> field. And the <code>templateRef</code> field maps to a Kubernetes CR with <code>kind</code> and <code>name</code>.</p> <p>For example, here we map our Cluster Template from earlier:</p> <pre><code>resources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy\n</code></pre> <p>The <code>serviceAccountRef</code> requires the <code>name</code> and <code>namespace</code> of the ServiceAccount Cartographer uses.</p> <p>The <code>selector</code> contains a Kubernetes Label:</p> <pre><code>selector:\nworkload-type: pre-built\n</code></pre> <p>See the complete example below.</p> Supply Chain resources/cartographer/app-deploy-01/02-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: my-first-supply-chain\nspec:\nresources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy\nserviceAccountRef:\nname: cartographer-pre-built-sa\nnamespace: default\nselector:\nworkload-type: pre-built\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#workload-definition","title":"Workload Definition","text":"<p>Remember, we need to specify the <code>spec.image</code> field.</p> <p>And if we want to trigger our Supply Chain, our Workload needs to contain the Label <code>workload-type=pre-built</code>.</p> workload-pre-built-hello.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello\nlabels:\nworkload-type: pre-built\nspec:\nimage: ghcr.io/joostvdg/go-demo:v2.1.16\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#rbac","title":"RBAC","text":"<p>Before we apply the resources to the cluster, we need to make sure our ServiceAccount exists and has the required permissions.</p> ServiceAccount, Roles, and RoleBindings resources/cartographer/rbac.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cartographer-pre-built-sa\nnamespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: deploy-image-role\nrules:\n- apiGroups:\n- apps\nresources:\n- deployments\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-prebuilt-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: deploy-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-pre-built-sa\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-1","title":"Excercise 1","text":"<p>We should now have four files, which you can apply to the Cluster and the appropriate Namespace.</p> <pre><code>export DEV_NAMESPACE=dev\n</code></pre> <pre><code>kubectl apply -f resources/cartographer/rbac.yaml -n ${DEV_NAMESPACE}\nkubectl apply -f resources/cartographer/app-deploy-01/01-cluster-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-01/02-supply-chain.yaml\nkuebctl apply -f resources/cartographer/app-deploy-01/03-workload.yaml -n ${DEV_NAMESPACE}\n</code></pre> <p>Once you have applied the resources, the workload should be valid:</p> <pre><code>kubectl get workload -n $DEV_NAMESPACE\n</code></pre> <p>And if you have the <code>kubectl tree</code> plugin, you can see the related resources:</p> <pre><code>kubectl tree workload hello -n ${DEV_NAMESPACE}\n</code></pre> <p>Which should show something like this:</p> <pre><code>NAMESPACE  NAME                                       READY  REASON  AGE\ndefault    Workload/hello                             True   Ready   98m\ndefault    \u2514\u2500Deployment/hello-deployment              -              98m\ndefault      \u2514\u2500ReplicaSet/hello-deployment-cfdf74d6   -              98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-kjjp5  True           98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-wgtvl  True           98m\ndefault        \u2514\u2500Pod/hello-deployment-cfdf74d6-x2pmc  True           98m\n</code></pre> <p>To test the application, you can use <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward deployment/hello-deployment -n $DEV_NAMESPACE 8080:8080\n</code></pre> <p>And then Curl:</p> <pre><code>curl http://localhost:8080/\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#adding-additional-parameters","title":"Adding Additional Parameters","text":"<p>A next step is to add more templating to our Supply Chain4.</p> <p>One of the ways we can do this is via additional parameters.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#add-paramters-to-clustertemplate","title":"Add Paramters To ClusterTemplate","text":"<p>We extend the ClusterTemplate from the previous example.</p> <p>I tend the rename updated examples:</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-02\n</code></pre> <p>Parameters are a top level spec resource (<code>params</code>), which you can verify with <code>kubectl explain</code>.</p> <pre><code>kubectl explain ClusterTemplate.spec\n</code></pre> <p>Which lists among other things:</p> <pre><code>params  &lt;[]Object&gt;\n  Additional parameters. See:\n  https://cartographer.sh/docs/latest/architecture/#parameter-hierarchy\n</code></pre> <p>We'll add a single environment variable to our Deployment. For this, we create two <code>params</code> entries:</p> <pre><code>params:\n- name: env_key\ndefault: \"FOO\"\n- name: env_value\ndefault: \"BAR\"\n</code></pre> <p>Which we can use in our <code>spec.template</code> via <code>$(params.&lt;nameOfParam&gt;)$</code>:</p> <pre><code>containers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\nenv:\n- name: $(params.env_key)$\nvalue: $(params.env_value)$\n</code></pre> Updated ClusterTemplate resources/cartographer/app-deploy-02/01-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-02\nspec:\ntemplate:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\nlabels:\napp: $(workload.metadata.name)$\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: $(workload.metadata.name)$\ntemplate:\nmetadata:\nlabels:\napp: $(workload.metadata.name)$\nspec:\ncontainers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\nenv:\n- name: $(params.env_key)$\nvalue: $(params.env_value)$\nparams:\n- name: env_key\ndefault: \"FOO\"\n- name: env_value\ndefault: \"BAR\"\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-supply-chain-to-use-updated-template","title":"Update Supply Chain To Use Updated Template","text":"<p>The parameters we added to the ClusterTemplate need to be supplied by the Workload.</p> <p>So technically we do not have to change our ClusterSupplyChain. I do so anyway, so we can have both Supply Chains in the cluster at the same time.</p> <p>We change the Template Ref:</p> <pre><code>- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-02\n</code></pre> <p>If we want Workloads to select the new Supply Chain, we should also update the Selector:</p> <pre><code>selector:\nworkload-type: pre-built-02\n</code></pre> Updated Supply Chain resources/cartographer/app-deploy-02/02-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: supply-chain-02\nspec:\nresources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-02\nserviceAccountRef:\nname: cartographer-pre-built-sa\nnamespace: default\nselector:\nworkload-type: pre-built-02\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#add-parameters-to-workload","title":"Add Parameters To Workload","text":"<p>In the Workload we change a few more things.</p> <p>We rename it, to <code>hello-02</code>, to differentiate from our previous Workload.</p> <p>We update the Label to <code>workload-type: pre-built-02</code> to reflect the updated Supply Chain.</p> <p>And, last but not least, we add the parameters!</p> <p>We do so similarly as we did for the Cluster Template:</p> <pre><code>params:\n- name: env_key\nvalue: \"K_SERVICE\"\n- name: env_value\nvalue: \"carto-hello\"\n</code></pre> <p>Tip</p> <p>For those wondering what this parameter does.</p> <p>When run via Knative Serving (as is done in TAP) the <code>k_SERVICE</code> env variable is printed in the http get on the <code>/</code> endpoint.</p> <p>So setting this, let's us directly see our value in the output.</p> <pre><code>Chart Version: ; Image Version: ; Release: unknown, SemVer: , GitCommit: ,Host: hello-04-deployment-7645c7c549-dpn8v, Revision: , Service: carto-hello-source\n</code></pre> Updated Workload resources/cartographer/app-deploy-02/03-workload.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello-02\nlabels:\nworkload-type: pre-built-02\nspec:\nimage: ghcr.io/joostvdg/go-demo:v2.1.16\nparams:\n- name: env_key\nvalue: \"K_SERVICE\"\n- name: env_value\nvalue: \"carto-hello\"\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-2","title":"Excercise 2","text":"<pre><code>export DEV_NAMESPACE=dev\n</code></pre> <p>Technically, we do not have to update the RBAC configuration, but its included for completeness.</p> <pre><code>kubectl apply -f resources/cartographer/rbac.yaml -n ${DEV_NAMESPACE}\nkubectl apply -f resources/cartographer/app-deploy-02/01-cluster-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-02/02-supply-chain.yaml\nkubectl apply -f resources/cartographer/app-deploy-02/03-workload.yaml  -n ${DEV_NAMESPACE}\n</code></pre> <p>Once you have applied the resources, the workload should be valid:</p> <pre><code>kubectl get workload -n $DEV_NAMESPACE\n</code></pre> <p>To test the application, you can use <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward deployment/hello-02-deployment -n $DEV_NAMESPACE 8080:8080\n</code></pre> <p>And then Curl:</p> <pre><code>curl http://localhost:8080/\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#extending-the-supply-chain","title":"Extending The Supply Chain","text":"<p>So far our Supply Chain deploys our application by generating a Deployment.</p> <p>Now it is time to add more steps to our Supply Chain5, so we, you know, a Chain of steps instead of just one.</p> <p>To stay in that theme, the Deployment needs to be supplied with a (container) Image. So our next goal is to build that Image and supply it to the Deployment.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#add-image-template","title":"Add Image Template","text":"<p>As doing CI/CD in Kubernetes often involves creating Container Images, Cartographer has a first-class support for this.</p> <p>This capability is supplied by the CR ClusterImageTemplate6.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterImageTemplate\nmetadata:\nname: image-builder-01\nspec:\ntemplate: {}\n</code></pre> <p>Cartographer doesn't actually build the image for us, it relies on third party tools. One such tool, which matches really well with Cartographer, is KPack7.</p> KPack Image CR example <pre><code>apiVersion: kpack.io/v1alpha1\nkind: Image\nmetadata:\nname: example-image\nnamespace: default\nspec:\ntag: &lt;DOCKER-IMAGE-TAG&gt;\nserviceAccount: &lt;SERVICE-ACCOUNT&gt;\nbuilder:\nname: &lt;BUILDER&gt;\nkind: Builder\nsource:\ngit:\nurl: &lt;APPLICATION-SOURCE-REPOSITORY&gt;\nrevision: &lt;APPLICATION-SOURCE-REVISION&gt;\n</code></pre> <p>KPack has an Image CR, that requires us the provide the following values:</p> <ul> <li>Tag: the name of the Image to build</li> <li>Builder: a KPack Builder, which is a Kubernetes CR that configures Cloud Native Buildpacks8</li> <li>Source: the source code input for the image building process</li> </ul> <p>The Tag speaks for itself, let's look at the Builder.</p> <p>When installing TAP, it also includes Tanzu Build Service (TBS). TBS installs and configures KPack, and thus we already have a set of KPack Builders available.</p> <p>We can retrieve them as follows:</p> <pre><code>kubectl get clusterbuilder\n</code></pre> <p>Which in my environment returns the following (some values are abbreviated):</p> <pre><code>NAME         LATESTIMAGE                                                    READY\nbase         h.s.h.v.com/b../tbs-full-deps:clusterbuilder-base@s.......85   True\nbase-jammy   h.s.h.v.com/b../tbs-full-deps:clusterbuilder-base-jammy@..70   True\ndefault      h.s.h.v.com/b../tbs-full-deps:clusterbuilder-default@.....57   True\nfull         h.s.h.v.com/b../tbs-full-deps:clusterbuilder-full@........2c   True\nfull-jammy   h.s.h.v.com/b../tbs-full-deps:clusterbuilder-full-jammy@..d0   True\ntiny         h.s.h.v.com/b../tbs-full-deps:clusterbuilder-tiny@........96   True\ntiny-jammy   h.s.h.v.com/b../tbs-full-deps:clusterbuilder-tiny-jammy@..9f   True\n</code></pre> <p>I like small Images (and I cannot lie), so I choose <code>tiny</code>:</p> <pre><code>builder:\nkind: ClusterBuilder\nname: tiny\n</code></pre> <p>Missing API Schema</p> <p>Unfortunately, the KPack CRs do not contain the OpenAPI schema.</p> <p>So we cannot use <code>kubectl explain</code> to explore the schema.</p> <p>For the source, we assume it is a Git repository, and require the Workload to specify it.</p> <pre><code>source:\ngit:\nurl: $(workload.spec.source.git.url)$\nrevision: $(workload.spec.source.git.ref.branch)$\n</code></pre> <p>For the ClusterImageTemplate itself, we need to specify two more things:</p> <ul> <li>params*: an input parameter for the <code>image_prefix</code> for the **Tag</li> <li><code>tag: $(params.image_prefix)$$(workload.metadata.name)$</code></li> <li>imagePath: we need to specify where the template can retrieve the URI of the built Image, to supply it to the next steps</li> </ul> <pre><code>params:\n- name: image_prefix\ndefault: harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/test\nimagePath: .status.latestImage\n</code></pre> Full ClusterImageTemplate <p>```yaml title=\"resources/cartographer/app-deploy-03/01-cluster-template.yaml\" apiVersion: carto.run/v1alpha1 kind: ClusterImageTemplate metadata:   name: image-builder-01 spec:   template:     apiVersion: kpack.io/v1alpha2     kind: Image     metadata:       name: (workload.metadata.name)     spec:       tag: (params.image_prefix)(params.image_prefix)(workload.metadata.name)(workload.metadata.name)       builder:         kind: ClusterBuilder         name: tiny       source:         git:           url: (workload.spec.source.git.url)(workload.spec.source.git.url)           revision: (workload.spec.source.git.ref.branch)(workload.spec.source.git.ref.branch)   params:     - name: image_prefix       default: harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/test   imagePath: .status.latestImage</p> <p>```</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-cluster-template","title":"Update Cluster Template","text":"<p>As with the other examples, we rename the ClusterTemplate so it does not replace our previous efforts.</p> resources/cartographer/app-deploy-03/01-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-from-sc-image-01\n</code></pre> <p>We want our Image to come from within the Supply Chain, and no longer supplied by our Workload.</p> <p>Later we'll declare in the Supply Chain how and where the Image comes from. For now, let's assume we'll end up with the value supplied in the struct <code>images.built-image.image</code>.</p> <pre><code>containers:\n- name: $(workload.metadata.name)$\nimage: $(images.built-image.image)$\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-rbac","title":"Update RBAC","text":"<p>The SA we use needs permissions to use the KPack resources.</p> <p>So we need to update our RBAC configuration:</p> Updated RBAC Config resources/cartographer/app-deploy-03/00-rbac.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cartographer-from-source-sa\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: deploy-image-role\nrules:\n- apiGroups:\n- apps\nresources:\n- deployments\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-deploy-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: deploy-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-from-source-sa\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: build-image-role\nrules:\n- apiGroups:\n- kpack.io\nresources:\n- images\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-build-image-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: build-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-from-source-sa\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-supply-chain","title":"Update Supply Chain","text":"<p>Oke. so we created a second Cartographer Resource, the ClusterImageTemplate.</p> <p>Let's add it to our Supply Chain.</p> <p>We add it to the list of Resources:</p> <pre><code>resources:\n- name: build-image\ntemplateRef:\nkind: ClusterImageTemplate\nname: image-builder-01\n</code></pre> <p>Our Deployment Template now needs to receive the Image URI from the Supply Chain. So we need to add that information to the Template's Resource definition.</p> <p>In case you don't remember, we assumed it would be defined within <code>images.built-image.image</code>.</p> <p>You can see below how we can express that. We add the <code>images</code> list, and link it to the output <code>built-image</code> from the Resource <code>build-image</code>.</p> <p>The Resource <code>build-image</code> is the Resource name of our ClusterImageTemplate. </p> <pre><code>- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-from-sc-image-01\nimages:\n- resource: build-image\nname: built-image\n</code></pre> <p>Below is the complete updated Supply Chain.</p> Updated Supply Chain resources/cartographer/app-deploy-03/02-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: source-code-supply-chain-01\nspec:\nresources:\n- name: build-image\ntemplateRef:\nkind: ClusterImageTemplate\nname: image-builder-01\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-from-sc-image-01\nimages:\n- resource: build-image\nname: built-image\nserviceAccountRef:\nname: cartographer-from-source-sa\nnamespace: dev\nselector:\nworkload-type: source-code-01\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-workload","title":"Update Workload","text":"<p>As the Image for our Deployment is now generated by the Supply Chain, we remove it from our Workload manifest.</p> <p>Instead, we now need to supply the (Git) Source of our Workload.</p> <pre><code>source:\ngit:\nref:\nbranch: main\nurl: https://github.com/joostvdg/go-demo\n</code></pre> <p>Below is the complete updated Workload manifest:</p> Updated Workload resources/cartographer/app-deploy-03/03-workload.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: source-code-01\nlabels:\nworkload-type: source-code-01\nspec:\nparams:\n- name: env_key\nvalue: \"K_SERVICE\"\n- name: env_value\nvalue: \"carto-hello-source\"\nsource:\ngit:\nref:\nbranch: main\nurl: https://github.com/joostvdg/go-demo\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-3","title":"Excercise 3","text":"<pre><code>export DEV_NAMESPACE=dev\n</code></pre> <p>We can now apply all the Resources to the Cluster:</p> <pre><code>kubectl apply -f resources/cartographer/app-deploy-03/00-rbac.yaml -n ${DEV_NAMESPACE}\nkubectl apply -f resources/cartographer/app-deploy-03/01-cluster-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-03/02-supply-chain.yaml\nkubectl apply -f resources/cartographer/app-deploy-03/03-workload.yaml  -n ${DEV_NAMESPACE}\n</code></pre> <p>Info</p> <p>The <code>01-cluster-template.yaml</code> file contains both the ClusterTemplate and the ClusterImageTemplate.</p> <p>Once you have applied the resources, the workload should be valid:</p> <pre><code>kubectl get workload -n $DEV_NAMESPACE\n</code></pre> <p>Because we're now doing a build, you might want to follow allong with the logs:</p> <pre><code>tanzu apps workload tail source-code-01 --namespace dev --timestamp --since 1h\n</code></pre> <p>To test the application, you can use <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward deployment/source-code-01-deployment -n $DEV_NAMESPACE 8080:8080\n</code></pre> <p>And then Curl:</p> <pre><code>curl http://localhost:8080/\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-introduction","title":"Tekton Introduction","text":"<p>At some point a Supply Chain needs to perform actions that are specific to your application or tech stack.</p> <p>Usually by means of running a particular Container with some shell commands.</p> <p>While you could use the ClusterTemplate to run a Pod with specific commands, there is a better tool for the job: Tekton9</p> <p>Tekton is an open-source cloud native CICD (Continuous Integration and Continuous Delivery/Deployment) solution</p> <p>We'll examine the core resources of Tekton and how to use them. So that later we can include them in our Cartographer Supply Chain.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#core-resources","title":"Core Resources","text":"<p>For those new to Tekton, here is a brief explanation of the core resources:</p> <ul> <li>Step: definition of a command to execute in a container image, part of the Task definition</li> <li>Task: template for a series of Steps with Workspaces, Outputs and (input) Parameters, more on those below</li> <li>TaskRun: a one-time runtime instantiation of a Task with its Workspaces and Parameters defined</li> <li>Pipeline: a template for a series of Tasks, with Workspaces and Parameters</li> <li>PipelineRun: a one-time runtime instantiation of a Pipeline with its Workspaces and Parameters defined</li> <li>Workspace: storage definition, essentially Kubernetes Volume definitions, conceptually in Task and Pipeline, specified in TaskRun and PipelineRun</li> <li>Results: Tasks can have outputs, named Results, which is a way of exporting information from a Task into the Pipeline, so it can be used by other Tasks</li> <li>Params: input parameters for a Task</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#task-taskrun","title":"Task &amp; TaskRun","text":"<p>A Tekton Task is a Kubernetes CR that contains one or more Steps, and lot of other optional configuration10.</p> <p>We will ignore most the optional configuration for now, and focus on the Steps.</p> <p>A Step has a <code>name</code>, <code>image</code>, and then either <code>args</code> and/or a <code>command</code> or a <code>script</code>, depending on the Image used.</p> <p>Below is an example of a Task using an Image to execute a shell command:</p> resources/tekton/task/01-task-hello.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: hello\nspec:\nsteps:\n- name: echo\nimage: harbor.services.h2o-2-9349.h2o.vmware.com/library/alpine@sha256:c0669ef34cdc14332c0f1ab0c2c01acb91d96014b172f1a76f3a39e63d1f0bda\nscript: |\n#!/bin/sh\necho \"Hello World\"\n</code></pre> <p>You can add this to your cluster as follows:</p> <pre><code>kubectl apply -f resources/tekton/task/01-task-hello.yaml -n $DEV_NAMESPACE\n</code></pre> <p>You can verify it exists, by running:</p> <pre><code>kubectl get task -n $DEV_NAMESPACE\n</code></pre> <p>Which returns something like this:</p> <pre><code>NAME            AGE\nhello           24h\n</code></pre> <p>You might wonder, now what?</p> <p>Nothing, a Task is a template, it doesn't do anything by itself.</p> <p>For that we need either a PipelineRun (when the Task is part of a Pipeline) or a TaskRun.</p> <p>A TaskRun refers to a Task, satisfies its requirements and then instantiates a version of that Task.</p> <p>In our case, a TaskRun looks like this:</p> resources/tekton/task/02-task-run.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\nname: hello-task-run\nspec:\ntaskRef:\nname: hello\n</code></pre> <p>We can add this to the cluster as follows:</p> <pre><code>kubectl apply -f resources/tekton/task/02-task-run.yaml -n $DEV_NAMESPACE\n</code></pre> <p>And then verify its status:</p> <pre><code>kubectl get taskrun -n $DEV_NAMESPACE\n</code></pre> <p>Which initially returns this:</p> <pre><code>NAME                                         SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-task-run                               Unknown     Pending     5s\n</code></pre> <p>And then this:</p> <pre><code>NAME                                         SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-task-run                               True        Succeeded   53s         47s\n</code></pre> <p>Because our TaskRun has a fixed name, we can loot at the Pod and request the logs of the container. The container is named after the step, so our step <code>echo</code> becomes <code>step-echo</code>:</p> <pre><code>kubectl -n ${DEV_NAMESPACE} logs hello-task-run-pod -c step-echo\n</code></pre> <p>Which should return the following:</p> <pre><code>Hello World\n</code></pre> <p>Feel free to clean them up:</p> <pre><code>kubectl delete -f resources/tekton/task/02-task-run.yaml -n $DEV_NAMESPACE\nkubectl delete -f resources/tekton/task/01-task-hello.yaml -n $DEV_NAMESPACE\n</code></pre> <p>Let us look at Pipeline and PipelineRun next.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#pipeline-pipelinerun","title":"Pipeline &amp; PipelineRun","text":"<p>Having a single Task with a fixed named TaskRun is a bit limiting.</p> <p>If you need to support more flows, workspaces, some logic, or want to re-use existing Tasks you need a Pipeline.</p> <p>A Tekton Pipeline11 is a collection of Tasks with inputs, outputs, workspaces and built-in logic for CI/CD workflows.</p> <p>We won't go into too much detail, but feel free to explore the options a Pipeline offers11.</p> <p>For now, we'll stick the ability to combine a series of Tasks and supply them with their requirements (e.g., input parameters).</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: hello-goodbye\nspec:\nparams: []\ntasks: []\n</code></pre> <p>That's our Pipeline skeleton, let us add some tasks to it:</p> <pre><code>spec:\ntasks:\n- name: hello\ntaskRef:\nname: hello\n- name: goodbye\nrunAfter:\n- hello\ntaskRef:\nname: goodbye\nparams:\n- name: username\nvalue: $(params.username)\n</code></pre> <p>As you can see, we can re-use our <code>hello</code> Task. And there's is a new Task, called <code>goodbye</code>.</p> <p>There are two new things here:</p> <ul> <li><code>runAfter</code>: Tasks might rely on outputs (Results) from other tasks, or have a logical sequential order, so we can specify Task B runs after Task A completed (successfully)</li> <li><code>params</code>: our second Task, <code>goodbye</code>, requires an input parameter named <code>username</code></li> </ul> <p>As you can see in the <code>params</code> section of the <code>goodbye</code> Task, we supply it a value from the Pipeline <code>params</code> object. Let's add that to our Pipeline:</p> <pre><code>spec:\nparams:\n- name: username\ntype: string\n</code></pre> <p>Our Pipeline now looks like this:</p> resources/tekton/pipeline/03-pipeline.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: hello-goodbye\nspec:\nparams:\n- name: username\ntype: string\ntasks:\n- name: hello\ntaskRef:\nname: hello\n- name: goodbye\nrunAfter:\n- hello\ntaskRef:\nname: goodbye\nparams:\n- name: username\nvalue: $(params.username)\n</code></pre> <p>We can add our two Tasks and the Pipeline to the cluster, and Tekton verifies all Tasks are accounted for.</p> <pre><code>kubectl apply -f resources/tekton/pipeline/01-task-hello.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline/02-task-goodbye.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline/03-pipeline.yaml -n $DEV_NAMESPACE\n</code></pre> <p>And verify the Pipeline is healthy:</p> <pre><code>kubectl get pipeline -n $DEV_NAMESPACE\n</code></pre> <p>Which should result in:</p> <pre><code>NAME                 AGE\nhello-goodbye        24h\n</code></pre> <p>Now that we have a Pipeline, we have to Run it. It is probably not a surpise we do this via a PipelineRun.</p> <p>Ignoring all the other bells and whistles Tekton offers, instantiating a Pipeline is straightforward. We create a PipelineRun manifest, reference the Pipeline and supply it its requirements (e.g., <code>params</code>).</p> resources/tekton/pipeline/04-pipeline-run-static.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\nname: hello-goodbye-run\nspec:\npipelineRef:\nname: hello-goodbye\nparams:\n- name: username\nvalue: \"Tekton\"\n</code></pre> <p>As you can see, we refer to our Pipeline via <code>pipelineRef.name</code>. And we supply the input parameters via <code>params</code>.</p> <p>Let's apply this to the cluster, and initiate our first Pipeline Run.</p> <pre><code>kubectl apply -f resources/tekton/pipeline/04-pipeline-run-static.yaml -n $DEV_NAMESPACE\n</code></pre> <p>And verify the state:</p> <pre><code>kubectl get pipelinerun -n $DEV_NAMESPACE\n</code></pre> <p>Which should yield something like this at first:</p> <pre><code>NAME                           SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-goodbye-run              Unknown     Running     6s\n</code></pre> <p>And then:</p> <pre><code>NAME                           SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-goodbye-run              True        Succeeded   64s         45s\n</code></pre> <p>And if we look for the TaskRuns:</p> <pre><code>kubectl get taskrun -n $DEV_NAMESPACE\n</code></pre> <p>We should see the following</p> <pre><code>NAME                                         SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-goodbye-run-goodbye                    True        Succeeded   76s         63s\nhello-goodbye-run-hello                      True        Succeeded   82s         76s\n</code></pre> <p>There is a downside to running Pipelines like this, we can not have more than one Run.</p> <p>The simple solution to this, is the replace the <code>metadata.name</code> property by <code>metadata.generateName</code>. Where the convention is to end in a <code>-</code>, so a generated hash is included.</p> resources/tekton/pipeline/05-pipeline-run-dynamic.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: hello-goodbye-run-\nspec:\npipelineRef:\nname: hello-goodbye\nparams:\n- name: username\nvalue: \"Tekton\"\n</code></pre> <p>We cannot apply this resource to the cluster, we have to use <code>kubectl create</code> instead (because of the <code>generatedName</code>):</p> <pre><code>kubectl create -f  resources/tekton/pipeline/05-pipeline-run-dynamic.yaml -n $DEV_NAMESPACE\n</code></pre> <p>If we now look at the PipelineRun and TaskRuns:</p> <pre><code>kubectl get taskrun,pipelinerun -n $DEV_NAMESPACE\n</code></pre> <p>We get the following:</p> <pre><code>NAME                                                            SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\ntaskrun.tekton.dev/hello-goodbye-run-goodbye                    True        Succeeded   6m54s       6m41s\ntaskrun.tekton.dev/hello-goodbye-run-hello                      True        Succeeded   7m          6m54s\ntaskrun.tekton.dev/hello-goodbye-run-vhswg-goodbye              True        Succeeded   29s         21s\ntaskrun.tekton.dev/hello-goodbye-run-vhswg-hello                True        Succeeded   36s         29s\n\nNAME                                                  SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\npipelinerun.tekton.dev/hello-goodbye-run              True        Succeeded   7m          6m41s\npipelinerun.tekton.dev/hello-goodbye-run-vhswg        True        Succeeded   36s         21s\n</code></pre> <p>The Run objects now include a hash in their name, allowing more than one to exists at once.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-pipeline-with-workspace","title":"Tekton Pipeline With Workspace","text":"<p>One common thing CI/CD Pipelines share, is the need to share data between Tasks. For example, you might have a Git Clone task to collect your source code, and then re-use that in the rest of the pipeline.</p> <p>Before going into how we work with Workspaces, let's highlight another concept. Tasks are re-usable templates, as such, there is a large collection of them maintained by the community. This is called the Tekton Catalog12, in which you'll find common tasks such a GitClone, building an Image with Kaniko and so on.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#setting-up-the-tasks","title":"Setting Up The Tasks","text":"<p>We will re-use the existing GitClone Task13, although with one minor modification.</p> <p>Which is included in the resources:</p> <pre><code>kubectl apply -f resources/tekton/pipeline-w-workspace/02-task-git-clone-0.10.yaml -n $DEV_NAMESPACE\n</code></pre> <p>This Task let's us Clone a Git repository, with practically all common Git clone configuration options.</p> <p>It defines a list of Workspaces:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: git-clone\nspec:\nworkspaces:\n- name: output\ndescription: The git repo will be cloned onto the volume backing this Workspace.\n- name: ssh-directory\noptional: true\ndescription: |\nA .ssh directory with private key, known_hosts, config, etc. Copied to\nthe user's home before git commands are executed. Used to authenticate\nwith the git remote when performing the clone. Binding a Secret to this\nWorkspace is strongly recommended over other volume types.\n- name: basic-auth\noptional: true\ndescription: |\nA Workspace containing a .gitconfig and .git-credentials file. These\nwill be copied to the user's home before any git commands are run. Any\nother files in this Workspace are ignored. It is strongly recommended\nto use ssh-directory over basic-auth whenever possible and to bind a\nSecret to this Workspace over other volume types.\n- name: ssl-ca-directory\noptional: true\ndescription: |\nA workspace containing CA certificates, this will be used by Git to\nverify the peer with when fetching or pushing over HTTPS.\n</code></pre> <p>Most of which have <code>optional: true</code>, except for <code>output</code>. Which means we'll need to supply that in our Pipeline later.</p> <p>It also has many paramaters, of which <code>url</code> and <code>revision</code> are required. We'll need to supply those as well.</p> <p>Next up is a Task that uses this Git checkout, to determine the next Git tag (e.g., application's release version).</p> <pre><code>kubectl apply -f resources/tekton/pipeline-w-workspace/01-task-git-next-tag.yaml -n $DEV_NAMESPACE\n</code></pre> <p>This task also required a Workspace:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: git-next-tag\nspec:\nworkspaces:\n- name: source\n</code></pre> <p>It requires one parameter, <code>base</code>, which is the SemVer base (e.g., <code>1.0.*</code>).</p> <p>And it provided an Output:</p> <pre><code>spec:\nresults:\n- name: NEXT_TAG\ndescription: Next version for Git Tag.\n</code></pre> <p>We'll look at this result later.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#creating-the-pipeline-with-workspace","title":"Creating the Pipeline with Workspace","text":"<p>So far, our two Tasks have a \"shopping list\" of required items:</p> <ul> <li>Workspace (<code>source</code> for git-next-tag, and <code>output</code> for git-clone)</li> <li>Parameters: (Git) <code>url</code>, (Git) <code>revision</code>, and (SemVer Tag) <code>base</code></li> </ul> <p>This is how our Pipeline looks like, satisfying those requirements:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: clone-git-next-tag\nspec:\nparams:\n- name: repo-url\ntype: string\n- name: base\ntype: string\n- name: gitrevision\nworkspaces:\n- name: shared-data\n</code></pre> <p>Next, we add the Tasks and configure their requirements via these properties:</p> <pre><code>spec:\ntasks:\n- name: fetch-source\ntaskRef:\nname: git-clone\nworkspaces:\n- name: output\nworkspace: shared-data\nparams:\n- name: url\nvalue: $(params.repo-url)\n- name: revision\nvalue: $(params.gitrevision)\n- name: git-next-tag\nrunAfter: [\"fetch-source\"]\ntaskRef:\nname: git-next-tag\nworkspaces:\n- name: source\nworkspace: shared-data\nparams:\n- name: base\nvalue: $(params.base)\n</code></pre> <p>As you might have spotted, we re-use the Workspace. Supplying the same single Workspace we defined in the Pipeline to both Tasks!</p> <p>This is intended, else we can't re-use our Git clone.</p> <p>Apply the Pipeline to the cluster.</p> <pre><code>kubectl apply -f resources/tekton/pipeline-w-workspace/03-pipeline.yaml -n $DEV_NAMESPACE\n</code></pre> Complete Pipeline Example resources/tekton/pipeline-w-workspace/03-pipeline.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: clone-git-next-tag\nspec:\ndescription: |\nThis pipeline clones a git repo, then echoes the README file to the stdout.\nparams:\n- name: repo-url\ntype: string\ndescription: The git repo URL to clone from.\n- name: base\ndescription: version Base to query Git tags for (e.g., v2.1.*)\ntype: string\n- name: gitrevision\ndescription: git revision to checkout\nworkspaces:\n- name: shared-data\ndescription: |\nThis workspace contains the cloned repo files, so they can be read by the\nnext task.\ntasks:\n- name: fetch-source\ntaskRef:\nname: git-clone\nworkspaces:\n- name: output\nworkspace: shared-data\nparams:\n- name: url\nvalue: $(params.repo-url)\n- name: revision\nvalue: $(params.gitrevision)\n- name: git-next-tag\nrunAfter: [\"fetch-source\"]\ntaskRef:\nname: git-next-tag\nworkspaces:\n- name: source\nworkspace: shared-data\nparams:\n- name: base\nvalue: $(params.base)\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#create-the-pipelinerun","title":"Create the PipelineRun","text":"<p>To instantiate our Pipeline, we'll create a PipelineRun.</p> <p>In this PipelineRun, we need to do the following:</p> <ul> <li>reference the Pipeline</li> <li>supply a Workspace</li> <li>supply the Parameters</li> </ul> <p>To make it re-usable, we'll start with a <code>generateName</code> style:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: clone-git-next-tag-run-\n</code></pre> <p>To ensure all containers can read the filesystem of the volume, we set a specific <code>fsGroup</code>:</p> <pre><code>spec:\n  podTemplate:\n    securityContext:\n      fsGroup: 65532\n</code></pre> <p>And then we create a Workspace via a <code>volumeClaimTemplate</code>:</p> <pre><code>spec:\nworkspaces:\n- name: shared-data\nvolumeClaimTemplate:\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 50Mi\nvolumeMode: Filesystem\n</code></pre> <p>Note, the name <code>shared-data</code> is the name specified in the Pipeline. The Pipeline definition ensures each Task gets it supplied as however it named it.</p> <p>And the parameters:</p> <pre><code>spec:\nparams:\n- name: repo-url\nvalue: https://github.com/joostvdg/go-demo.git\n- name: base\nvalue: \"v2.1\"\n- name: gitrevision\nvalue: main\n</code></pre> Full PipelineRun Example <pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: clone-git-next-tag-run-\nspec:\npipelineRef:\nname: clone-git-next-tag\npodTemplate:\nsecurityContext:\nfsGroup: 65532\nworkspaces:\n- name: shared-data\nvolumeClaimTemplate:\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 50Mi\nvolumeMode: Filesystem\nparams:\n- name: repo-url\nvalue: https://github.com/joostvdg/go-demo.git\n- name: base\nvalue: \"v2.1\"\n- name: gitrevision\nvalue: main\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-4","title":"Excercise 4","text":"<pre><code>export DEV_NAMESPACE=dev\n</code></pre> <p>Just in case you haven't applied all files yet, here's the whole list again:</p> <pre><code>kubectl apply -f resources/tekton/pipeline-w-workspace/02-task-git-clone-0.10.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline-w-workspace/01-task-git-next-tag.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline-w-workspace/03-pipeline.yaml -n $DEV_NAMESPACE\n</code></pre> <p>Verify the Pipeline is valid, and then create the PipelineRun:</p> <pre><code>kubectl create -f resources/tekton/pipeline-w-workspace/04-pipeline-run.yaml -n $DEV_NAMESPACE\n</code></pre> <p>Once created, you can verify the status:</p> <pre><code>kubectl get taskrun,pipelinerun -n $DEV_NAMESPACE\n</code></pre> <p>If you want the logs, you'll now have to find the appropriate Pod name, as its name is generated.</p> <pre><code>kubectl get pod  -n ${DEV_NAMESPACE}\n</code></pre> <pre><code>POD_NAME=\n</code></pre> <pre><code>kubectl -n ${DEV_NAMESPACE} logs ${POD_NAME}\n</code></pre> <p>You can also use the (automatic) Labels to query them:</p> <pre><code>kubectl get taskrun -n dev -l tekton.dev/task=git-next-tag\n</code></pre> <p>And then you can find the output of the <code>Result</code> from the <code>git-next-tag</code> Task in its <code>status.taskResults</code> field:</p> <pre><code>kubectl get taskrun -n dev -l tekton.dev/task=git-next-tag -ojson | jq '.items | map(.status.taskResults)'\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-in-supply-chain","title":"Tekton In Supply Chain","text":"<p>Info</p> <p>In case you are wondering how this hierarchy now looks:</p> <pre><code>* ClusterSupplyChain\n  * ClusterSourceTemplate\n    * ClusterRunTemplate\n      * PipelineRun\n        * Task\n          * TaskRun (Generated)\n* Pod (Generated)\n* InitContainer -&gt; Shell (Generated)\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#ootb-pipeline-appendix","title":"OOTB Pipeline Appendix","text":"<p>Source in TAP</p> <p>In TAP, we don't have to clone our sources from Git, we can download them from FluxCD.</p> <p>The way TAP works, it that the trigger for a SupplyChain goes through FluxCD's GitRepository management. So below is a way of codifying that process into a Tekton Task:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: fluxcd-repo-download\nspec:\nparams:\n- name: source-url\ntype: string\ndescription: |\nthe source url to download the code from, \nin the form of a FluxCD repository checkout .tar.gz\nworkspaces:\n- name: output\ndescription: The git repo will be cloned onto the volume backing this Workspace.\nsteps:\n- name: download-source\nimage: public.ecr.aws/docker/library/gradle:jdk17-focal\nscript: |\n#!/usr/bin/env sh\ncd $(workspaces.output.path)\nwget -qO- $(params.source-url) | tar xvz -m\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#references","title":"References","text":"<ol> <li> <p>Cartographer - First Supply Chain Tutorial \u21a9</p> </li> <li> <p>Cartographer - Types of Resources \u21a9</p> </li> <li> <p>Cartographer - Resource Definitions \u21a9</p> </li> <li> <p>Cartographer - Parameters Tutorial \u21a9</p> </li> <li> <p>Cartographer - Extend A Supply Chain Tutorial \u21a9</p> </li> <li> <p>Cartographer - ClusterImageTemplate resource spec \u21a9</p> </li> <li> <p>KPack - Kubernetes solution for running Cloud Native Buildpacks \u21a9</p> </li> <li> <p>Cloud Native Build Packs \u21a9</p> </li> <li> <p>Tekton - Open-source cloud native CICD \u21a9</p> </li> <li> <p>Tekton - Task details \u21a9</p> </li> <li> <p>Tekton - Pipeline details \u21a9\u21a9</p> </li> <li> <p>Tekton - Task Catalog \u21a9</p> </li> <li> <p>Tekton Catalog - GitClone Task \u21a9</p> </li> <li> <p> \u21a9</p> </li> <li> <p> \u21a9</p> </li> <li> <p> \u21a9</p> </li> </ol>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/debug/","title":"TAP Debug Supply Chains","text":"","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#debug-workload","title":"Debug Workload","text":"","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#source","title":"Source","text":"<pre><code>kubectl get gitrepo -A\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#workload","title":"Workload","text":"<pre><code>kubectl get workload -A\n</code></pre> <pre><code>tanzu apps workload get smoke-app --namespace dev\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#image-failed","title":"Image Failed","text":"<pre><code>\ud83d\udce1 Overview\n   name:        smoke-app\n   type:        web\n   namespace:   dev\n\n\ud83d\udcbe Source\n   type:     git\n   url:      https://github.com/sample-accelerators/tanzu-java-web-app.git\n   branch:   main\n\n\ud83d\udce6 Supply Chain\n   name:   source-to-url\n\nNAME               READY   HEALTHY   UPDATED   RESOURCE\n   source-provider    True    True      9m49s     gitrepositories.source.toolkit.fluxcd.io/smoke-app\n   image-provider     False   False     9m40s     images.kpack.io/smoke-app\n   config-provider    False   Unknown   9m57s     not found\n   app-config         False   Unknown   9m56s     not found\n   service-bindings   False   Unknown   9m56s     not found\n   api-descriptors    False   Unknown   9m56s     not found\n   config-writer      False   Unknown   9m56s     not found\n\n\ud83d\ude9a Delivery\n   name:   delivery-basic\n\nNAME              READY   HEALTHY   UPDATED   RESOURCE\n   source-provider   False   False     9m45s     imagerepositories.source.apps.tanzu.vmware.com/smoke-app-delivery\n   deployer          False   Unknown   9m49s     not found\n\n\ud83d\udcac Messages\n   Workload [HealthyConditionRule]:   condition status: False, message: Unable to find builder default.\n   Deliverable [HealthyConditionRule]:   Unable to resolve image with tag \"harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/smoke-app-dev-bundle:ed5561a7-5bfc-45fb-be33-90b2aeb0a9a1\" to a digest: HEAD https://harbor.services.h2o-2-9349.h2o.vmware.com/v2/tap-apps/smoke-app-dev-bundle/manifests/ed5561a7-5bfc-45fb-be33-90b2aeb0a9a1: unexpected status code 404 Not Found (HEAD responses have no body, use GET for details)\n</code></pre> <pre><code>kubectl get ClusterImageTemplate kpack-template -o yaml | yq\n</code></pre> <pre><code>kubectl describe img -n dev smoke-app\n</code></pre> <pre><code>Status:\n  Conditions:\n    Last Transition Time:  2023-05-05T12:07:52Z\n    Message:               Unable to find builder default.\n    Reason:                BuilderNotFound\n    Status:                False\n    Type:                  Ready\n  Observed Generation:     1\nEvents:                    &lt;none&gt;\n</code></pre> <p>This means we're missing Tanzu Buildservice components!</p>","tags":["tap","kubernetes","cartographer","tekton","debug"]}]}