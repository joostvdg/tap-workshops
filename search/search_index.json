{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TAP Workshop","text":"<p>Welcome.</p>"},{"location":"#lab-exercises","title":"Lab Exercises","text":"<ol> <li>TAP Install - Full Profile &amp; OOTB Basic Supply Chain</li> <li>Hello World++ Workload</li> <li>Create Custom Supply Chain</li> <li>Upgrade OOTB Basic Supply Chain to OOTB Testing &amp; Scanning</li> <li>Leverage External Services for your Workload</li> </ol> <p>There are more guided workshop materials out there, for example the Tanzu Solution Engineer workshops, visit the E2E Demo Portal.</p>"},{"location":"#dockerhub-proxy","title":"DockerHub Proxy","text":"<p>We have a DockerHub Proxy repository setup in the local Harbor instance.</p> <p>This Proxy repository is logged in, and thus helps you prevent DockerHub's Rate Limits.</p> <p>The project in Harbor is <code>dockerhub</code>, or <code>harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/</code> in full.</p> <p>So instead of <code>alpine</code>, you would use <code>harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/library/alpine</code></p> <pre><code>docker pull harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/library/alpine\n</code></pre>"},{"location":"#links","title":"Links","text":""},{"location":"#examples","title":"Examples","text":"<ul> <li>Tanzu Labs US - Custom Cartographer Supply Chains</li> <li>TAP Application Accelerator samples</li> <li>VRabbi TAP GitOps</li> </ul>"},{"location":"#example-applications","title":"Example Applications","text":"<ul> <li>Where For Dinner - Main TAP demo application</li> <li>Spring Cloud Stream</li> <li>TAP - Open Telemetry For Applications demo</li> <li>Spring Cloud Demo</li> </ul>"},{"location":"#community-links","title":"Community Links","text":"<ul> <li>VRabbi - Whats New In 1.5</li> <li>VRabbi - TAP 1.5 GitOps</li> <li>VRabbi - Service Toolkit Dynamic Provisioning</li> </ul>"},{"location":"#other-links","title":"Other Links","text":"<ul> <li>Testcontainers with Tekton</li> <li>Tanzu Developer - Getting Started With Testcontainers</li> <li>CNCF Security TAG - Software Supply Chain Best Practices</li> <li>Miro Board with US based TAP Engagements</li> <li>Cartographer Lifecyle docs (useful for leveraging Tekton TaskRun)</li> <li>What Is Supply Chain Choreography, and Why Should You Care?</li> </ul>"},{"location":"labs/","title":"Labs","text":"<p>See the index for the Labs to do in recommended order.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"apps/external-service/","title":"TAP App with External Service","text":"<p>In this workshop, we onboard an Application into TAP that depends on an external database.</p> <p>For this to work well, we need to support the following: * Ability to have a database during testing * Ability to inject connection details to external database</p> <p>For the first, we need to customize our Supply Chain. The latter is supported in TAP via the Services Toolkit component, leveraging Crossplane and Service Binding Spec to do so.</p>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#checks","title":"Checks","text":"<ul> <li> I am able to register API Documentation for a Workload in TAP GUI</li> <li> I am able to stand up shared services and claim resources on a TAP Run Cluster</li> <li> I can confgure API Auto Registration within a Supply Chain for a Workload</li> <li> I can configure API Scoring and Validation within TAP GUI</li> <li> I can configure API Portal with TAP</li> <li> I can install Shared Services on a TAP Cluster (Services Toolkit)</li> <li> I can claim a shared service on a TAP Cluster (Service Bindings)</li> <li> I can use a postgres resource claim with sample Spring Boot Application Pet Clinic Accelerator</li> <li> Expose Accerators Endpoint for use in Tanzu Accelerator Plugins</li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#steps","title":"Steps","text":"<ul> <li>update supply chain<ul> <li>create new Tekton Tasks</li> <li>create new Tekton Pipeline that uses these Tasks</li> <li>create copy from OOTB Supply Chain that uses Tekton Pipeline</li> </ul> </li> <li>fork existing application</li> <li>create Workload and see tests succeed</li> <li>deployment fails -&gt; we need a database</li> <li>create database via Bitnami Services<ul> <li>static vs. dynamic</li> <li>we do static for now</li> <li>verify database exists</li> </ul> </li> <li>use services toolkit to bind<ul> <li>create claim</li> <li>update workload to use claim</li> </ul> </li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#update-supply-chain","title":"Update Supply Chain","text":"<p>As stated in the preamble, we supply our application with a database during testing.</p> <p>To be precise, we will provide it with a means to do so itself, via Testcontainers.</p> <p>To do so, we need to give it a Docker host to talk to, to spin up arbitrary containers.</p> <p>Not Production Ready</p> <p>There are better ways of giving the application a Docker host to talk to, so that it can leverage Test Containers.</p> <p>You can run the build on specific VMs or Micro VMs with Docker, or expose the Docker daemon on those VMs.</p> <p>To solution below uses Docker In Docker, or DinD. While supported by Docker, it is not a best practice.</p> <p>To make this Supply Chain leverage Tekton in a good way, we will also introduce a (Tekton) Workspace to share code between Tasks.</p> <p>Create the files below</p> <p>The Files you see below need to be applied to the cluster.</p> <p>So please create them on your machine, and follow the instructions after.</p>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#checkout-task","title":"Checkout Task","text":"<p>A proper Pipeline starts with checking out the code. Below is a Task that uses the information provided by the OOTB Supply Chain to copy the code from FluxCD.</p> <p>It stores this on a Workspace that is shared with any next Task in the (Tekton) Pipeline.</p> task-fluxcd-repo-download.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: fluxcd-repo-download\nspec:\nparams:\n- description: \"the source url to download the code from, \\nin the form of a FluxCD\nrepository checkout .tar.gz\\n\"\nname: source-url\ntype: string\nsteps:\n- image: harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/library/gradle:jdk17-focal\nname: download-source\nresources: {}\nscript: |\n#!/usr/bin/env sh\ncd $(workspaces.output.path)\nwget -qO- $(params.source-url) | tar xvz -m\nworkspaces:\n- description: The git repo will be cloned onto the volume backing this Workspace.\nname: output\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#maven-testcontainers-task","title":"Maven TestContainers Task","text":"<p>Below is a Task that supports a Maven build that requires a Docker daemon.</p> <p>One such type of build that requires a Docker daemon, is the use of TestContainers.</p> <p>TestContainers provides a build with ad-hoc creation of a Docker container required for tests, and then handles the cleanup as well.</p> task-maven-test-containers.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: maven-test-containers\nspec:\nsidecars:\n- image: harbor.services.h2o-2-9349.h2o.vmware.com/library/docker:20.10-dind\nname: docker\nresources: {}\nsecurityContext:\nprivileged: true\nvolumeMounts:\n- mountPath: /var/lib/docker\nname: dind-storage\n- mountPath: /var/run/\nname: dind-socket\nsteps:\n- image: harbor.services.h2o-2-9349.h2o.vmware.com/library/eclipse-temurin:17.0.3_7-jdk-alpine\nname: read\nresources: {}\nscript: ./mvnw test\nvolumeMounts:\n- mountPath: /var/run/\nname: dind-socket\nworkingDir: $(workspaces.output.path)\nvolumes:\n- emptyDir: {}\nname: dind-storage\n- emptyDir: {}\nname: dind-socket\nworkspaces:\n- description: The workspace consisting of maven project.\nname: output\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#tekton-pipeline-fluxcd-maven-test","title":"Tekton Pipeline FluxCD Maven Test","text":"<p>Here is the Pipeline that combines the two Tasks.</p> pipeline-fluxcd-maven-test.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nlabels:\napps.tanzu.vmware.com/pipeline: test\nname: fluxcd-maven-test\nspec:\nparams:\n- name: source-url\ntype: string\n- name: source-revision\ntype: string\ntasks:\n- name: fetch-repository\nparams:\n- name: source-url\nvalue: $(params.source-url)\ntaskRef:\nkind: Task\nname: fluxcd-repo-download\nworkspaces:\n- name: output\nworkspace: shared-workspace\n- name: maven-run\nparams:\n- name: GOALS\nvalue:\n- clean\n- verify\nrunAfter:\n- fetch-repository\ntaskRef:\nkind: Task\nname: maven-test-containers\nworkspaces:\n- name: output\nworkspace: shared-workspace\nworkspaces:\n- name: shared-workspace\n- name: maven-settings\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#clustertemplate-for-new-pipeline","title":"ClusterTemplate For New Pipeline","text":"<p>A Tekton Pipeline needs to be triggered via a PipelineRun.</p> <p>In our Supply Chain, we use a ClusterRunTemplate to generate a PipelineRun with the appropriate parameters.</p> tekton-source-pipelinerun-workspace.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterRunTemplate\nmetadata:\nname: tekton-source-pipelinerun-workspace\nspec:\noutputs:\nrevision: spec.params[?(@.name==\"source-revision\")].value\nurl: spec.params[?(@.name==\"source-url\")].value\ntemplate:\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: $(runnable.metadata.name)$-\nlabels: $(runnable.metadata.labels)$\nspec:\nparams: $(runnable.spec.inputs.tekton-params)$\npipelineRef:\nname: $(selected.metadata.name)$\npodTemplate:\nsecurityContext:\nfsGroup: 65532\nworkspaces:\n- name: maven-settings\nemptyDir: {}\n- name: shared-workspace\nvolumeClaimTemplate:\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 500Mi\nvolumeMode: Filesystem\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#add-new-components-to-the-cluster","title":"Add New Components to the Cluster","text":"<p>We can now add the resources to the cluster.</p> <p>The Tekton resources are namespaced, the Cartographer ClusterRunTemplate, as the name implies, is not.</p> <pre><code>kubectl apply -f task-fluxcd-repo-download.yaml \\\n-n ${TAP_DEVELOPER_NAMESPACE}\nkubectl apply -f task-maven-test-containers.yaml \\\n-n ${TAP_DEVELOPER_NAMESPACE}\nkubectl apply -f pipeline-fluxcd-maven-test.yaml \\\n-n ${TAP_DEVELOPER_NAMESPACE}\nkubectl apply -f tekton-source-pipelinerun-workspace.yaml </code></pre> <p>There Can Be Only One</p> <p>Make sure there is one Pipeline with the label <code>apps.tanzu.vmware.com/pipeline=test</code>.</p> <p>Verify this with this command:</p> <pre><code>get pipeline -l apps.tanzu.vmware.com/pipeline=test \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>If there is more than one, e.g., you still have <code>developer-defined-tekton-pipeline</code>, as below:</p> <pre><code>NAME                                AGE\ndeveloper-defined-tekton-pipeline   14h\nfluxcd-maven-test                   87m\n</code></pre> <p>Please remove that one:</p> <pre><code>kubectl delete pipeline developer-defined-tekton-pipeline \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#copy-existing-supply-chain-components","title":"Copy Existing Supply Chain Components","text":"<p>And then customize the ClusterSourceTemplate name <code>testing-pipeline</code> to use our new ClusterRunTemplate instead.</p> <pre><code>kubectl get ClusterSourceTemplate testing-pipeline \\\n-o yaml &gt; testing-pipeline-workspace.yaml\n</code></pre> <p>We first strip away all the things from the <code>metadata</code> that we don't need, and rename it in a single <code>yq</code> command.</p> <pre><code>yq e -i '.metadata = {\"name\": \"testing-pipeline-workspace\"}' testing-pipeline-workspace.yaml\n</code></pre> <p>The next step is a bit more difficult, so we need some <code>sed</code> magic. There is an inlined YTT template, in which we need to rename the Tekton Pipeline we refer to the one we just created.</p> <pre><code>sed -i -e \"s/tekton-source-pipelinerun/tekton-source-pipelinerun-workspace/g\" testing-pipeline-workspace.yaml\n</code></pre> Example <pre><code>cat testing-pipeline-workspace.yaml\n</code></pre> testing-pipeline-workspace.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSourceTemplate\nmetadata:\nname: testing-pipeline-workspace\nspec:\nhealthRule:\nsingleConditionType: Ready\nlifecycle: mutable\nparams:\n- default:\napps.tanzu.vmware.com/pipeline: test\nname: testing_pipeline_matching_labels\nrevisionPath: .status.outputs.revision\nurlPath: .status.outputs.url\nytt: \"#@ load(\\\"@ytt:data\\\", \\\"data\\\")\\n\\n#@ def merge_labels(fixed_values):\\n#@   labels = {}\\n#@   if hasattr(data.values.workload.metadata, \\\"labels\\\"):\\n#@     labels.update(data.values.workload.metadata.labels)\\n#@   end\\n#@   labels.update(fixed_values)\\n#@   return labels\\n#@ end\\n\\n#@ def merged_tekton_params():\\n#@   params = []\\n#@   if hasattr(data.values, \\\"params\\\") and hasattr(data.values.params, \\\"testing_pipeline_params\\\"):\\n#@     for param in data.values.params[\\\"testing_pipeline_params\\\"]:\\n#@       params.append({ \\\"name\\\": param, \\\"value\\\": data.values.params[\\\"testing_pipeline_params\\\"][param] })\\n#@     end\\n#@   end\\n#@   params.append({ \\\"name\\\": \\\"source-url\\\", \\\"value\\\": data.values.source.url })\\n#@   params.append({ \\\"name\\\": \\\"source-revision\\\", \\\"value\\\": data.values.source.revision })\\n#@   return params\\n#@ end\\n---\\napiVersion: carto.run/v1alpha1\\nkind: Runnable\\nmetadata:\\n  name: #@ data.values.workload.metadata.name\\n  labels: #@ merge_labels({ \\\"app.kubernetes.io/component\\\": \\\"test\\\" })\\nspec:\\n  #@ if/end hasattr(data.values.workload.spec, \\\"serviceAccountName\\\"):\\n  serviceAccountName: #@ data.values.workload.spec.serviceAccountName\\n\\n  runTemplateRef:\\n    name: tekton-source-pipelinerun-workspace\\n    kind: ClusterRunTemplate\\n\\n  selector:\\n    resource:\\n      apiVersion: tekton.dev/v1beta1\\n      kind: Pipeline\\n\\n    #@ not hasattr(data.values, \\\"testing_pipeline_matching_labels\\\") or fail(\\\"testing_pipeline_matching_labels param is required\\\")\\n    matchingLabels: #@ data.values.params[\\\"testing_pipeline_matching_labels\\\"] or fail(\\\"testing_pipeline_matching_labels param cannot be empty\\\")\\n\\n  inputs: \\n    tekton-params: #@ merged_tekton_params()\\n\"\n</code></pre> <p>Again, this is a Cluster resource, so no need to supply a namespace:</p> <pre><code>kubectl apply -f testing-pipeline-workspace.yaml\n</code></pre> <p>Next, let's create a new ClusterSupplyChain, wich uses the resources we just created. This way, our existing Supply Chain and the Applications depending on it, continue to function.</p> <p>First, retrieve the existing the ClusterSupplyChain as a start:</p> <pre><code>kubectl get ClusterSupplyChain source-test-scan-to-url \\\n-o yaml &gt; source-test-scan-to-url.yaml\n</code></pre> <p>We first strip away all the things from the <code>metadata</code> that we don't need, and rename it in a single <code>yq</code> command.</p> <pre><code>yq e -i '.metadata = {\"name\": \"source-test-scan-to-url-workspace\"}' source-test-scan-to-url.yaml\n</code></pre> <p>Clear the Status, just in case:</p> <pre><code>yq e -i '.status = {}' source-test-scan-to-url.yaml\n</code></pre> <p>Change to Source Template to the one we just created:</p> <pre><code>sed -i -e \"s/testing-pipeline/testing-pipeline-workspace/g\" source-test-scan-to-url.yaml\n</code></pre> <p>Set Selector to:</p> <pre><code>selector:\napps.tanzu.vmware.com/has-tests-needs-workspace: \"true\"\n</code></pre> <pre><code>sed -i -e \"s/has-tests/has-tests-needs-workspace/g\" source-test-scan-to-url.yaml\n</code></pre> <p>Verify the SupplyChain looks valid:</p> <pre><code>cat source-test-scan-to-url.yaml\n</code></pre> Example <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: source-test-scan-to-url-workspace\nspec:\nparams:\n- name: maven_repository_url\nvalue: https://repo.maven.apache.org/maven2\n- default: main\nname: gitops_branch\n- default: supplychain\nname: gitops_user_name\n- default: supplychain\nname: gitops_user_email\n- default: supplychain@cluster.local\nname: gitops_commit_message\n- default: \"\"\nname: gitops_ssh_secret\nresources:\n- name: source-provider\nparams:\n- default: default\nname: serviceAccount\n- default: go-git\nname: gitImplementation\ntemplateRef:\nkind: ClusterSourceTemplate\nname: source-template\n- name: source-tester\nsources:\n- name: source\nresource: source-provider\ntemplateRef:\nkind: ClusterSourceTemplate\nname: testing-pipeline-workspace\n- name: source-scanner\nparams:\n- default: scan-policy\nname: scanning_source_policy\n- default: blob-source-scan-template\nname: scanning_source_template\nsources:\n- name: source\nresource: source-tester\ntemplateRef:\nkind: ClusterSourceTemplate\nname: source-scanner-template\n- name: image-provider\nparams:\n- default: default\nname: serviceAccount\n- name: registry\nvalue:\nca_cert_data: |-\n-----BEGIN CERTIFICATE-----\nMIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\nBQAwgY4xCzAJBgNVBAYTAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAO\nBgNVBAcTB1V0cmVjaHQxFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMU\nS2Vhcm9zIFRhbnp1IFJvb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290\nIENBMB4XDTIyMDMyMzE1MzUwMFoXDTI3MDMyMjE1MzUwMFowgY4xCzAJBgNVBAYT\nAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAOBgNVBAcTB1V0cmVjaHQx\nFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMUS2Vhcm9zIFRhbnp1IFJv\nb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290IENBMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyZXDL9W2vu365m//E/w8n1M189a5mI9HcTYa\n0xZhnup58Zp72PsgzujI/fQe43JEeC+aIOcmsoDaQ/uqRi8p8phU5/poxKCbe9SM\nf1OflLD9k2dwte6OV5kcSUbVOgScKL1wGEo5mdOiTFrEp5aLBUcbUeJMYz2IqLVa\nv52H0vTzGfmrfSm/PQb+5qnCE5D88DREqKtWdWW2bCW0HhxVHk6XX/FKD2Z0FHWI\nChejeaiarXqWBI94BANbOAOmlhjjyJekT5hL1gh7BuCLbiE+A53kWnXO6Xb/eyuJ\nobr+uHLJldoJq7SFyvxrDd/8LAJD4XMCEz+3gWjYDXMH7GfPWwIDAQABo0IwQDAO\nBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUfGU50Pe9\nYTv5SFvGVOz6R7ddPcUwDQYJKoZIhvcNAQELBQADggEBAHMoNDxy9/kL4nW0Bhc5\nGn0mD8xqt+qpLGgChlsMPNR0xPW04YDotm+GmZHZg1t6vE8WPKsktcuv76d+hX4A\nuhXXGS9D0FeC6I6j6dOIW7Sbd3iAQQopwICYFL9EFA+QAINeY/Y99Lf3B11JfLU8\njN9uGHKFI0FVwHX428ObVrDi3+OCNewQ3fLmrRQe6F6q2OU899huCg+eYECWvxZR\na3SlVZmYnefbA87jI2FRHUPqxp4P2mDwj/RZxhgIobhw0zz08sqC6DW0Aj1OIJe5\nsDAm0uiUdqs7FZN2uKkLKekdTgW0QkTFEJTk5Yk9t/hOrjnHoWQfB+mLhO3vPhip\nvhs=\n-----END CERTIFICATE-----\nrepository: tap-apps\nserver: harbor.services.h2o-2-9349.h2o.vmware.com\n- default: default\nname: clusterBuilder\n- default: ./Dockerfile\nname: dockerfile\n- default: ./\nname: docker_build_context\n- default: []\nname: docker_build_extra_args\nsources:\n- name: source\nresource: source-scanner\ntemplateRef:\nkind: ClusterImageTemplate\noptions:\n- name: kpack-template\nselector:\nmatchFields:\n- key: spec.params[?(@.name==\"dockerfile\")]\noperator: DoesNotExist\n- name: kaniko-template\nselector:\nmatchFields:\n- key: spec.params[?(@.name==\"dockerfile\")]\noperator: Exists\n- images:\n- name: image\nresource: image-provider\nname: image-scanner\nparams:\n- default: scan-policy\nname: scanning_image_policy\n- default: private-image-scan-template\nname: scanning_image_template\ntemplateRef:\nkind: ClusterImageTemplate\nname: image-scanner-template\n- images:\n- name: image\nresource: image-scanner\nname: config-provider\nparams:\n- default: default\nname: serviceAccount\ntemplateRef:\nkind: ClusterConfigTemplate\nname: convention-template\n- configs:\n- name: config\nresource: config-provider\nname: app-config\ntemplateRef:\nkind: ClusterConfigTemplate\noptions:\n- name: config-template\nselector:\nmatchLabels:\napps.tanzu.vmware.com/workload-type: web\n- name: server-template\nselector:\nmatchLabels:\napps.tanzu.vmware.com/workload-type: server\n- name: worker-template\nselector:\nmatchLabels:\napps.tanzu.vmware.com/workload-type: worker\n- configs:\n- name: app_def\nresource: app-config\nname: service-bindings\ntemplateRef:\nkind: ClusterConfigTemplate\nname: service-bindings\n- configs:\n- name: app_def\nresource: service-bindings\nname: api-descriptors\ntemplateRef:\nkind: ClusterConfigTemplate\nname: api-descriptors\n- configs:\n- name: config\nresource: api-descriptors\nname: config-writer\nparams:\n- default: default\nname: serviceAccount\n- name: registry\nvalue:\nca_cert_data: |-\n-----BEGIN CERTIFICATE-----\nMIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\nBQAwgY4xCzAJBgNVBAYTAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAO\nBgNVBAcTB1V0cmVjaHQxFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMU\nS2Vhcm9zIFRhbnp1IFJvb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290\nIENBMB4XDTIyMDMyMzE1MzUwMFoXDTI3MDMyMjE1MzUwMFowgY4xCzAJBgNVBAYT\nAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAOBgNVBAcTB1V0cmVjaHQx\nFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMUS2Vhcm9zIFRhbnp1IFJv\nb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290IENBMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyZXDL9W2vu365m//E/w8n1M189a5mI9HcTYa\n0xZhnup58Zp72PsgzujI/fQe43JEeC+aIOcmsoDaQ/uqRi8p8phU5/poxKCbe9SM\nf1OflLD9k2dwte6OV5kcSUbVOgScKL1wGEo5mdOiTFrEp5aLBUcbUeJMYz2IqLVa\nv52H0vTzGfmrfSm/PQb+5qnCE5D88DREqKtWdWW2bCW0HhxVHk6XX/FKD2Z0FHWI\nChejeaiarXqWBI94BANbOAOmlhjjyJekT5hL1gh7BuCLbiE+A53kWnXO6Xb/eyuJ\nobr+uHLJldoJq7SFyvxrDd/8LAJD4XMCEz+3gWjYDXMH7GfPWwIDAQABo0IwQDAO\nBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUfGU50Pe9\nYTv5SFvGVOz6R7ddPcUwDQYJKoZIhvcNAQELBQADggEBAHMoNDxy9/kL4nW0Bhc5\nGn0mD8xqt+qpLGgChlsMPNR0xPW04YDotm+GmZHZg1t6vE8WPKsktcuv76d+hX4A\nuhXXGS9D0FeC6I6j6dOIW7Sbd3iAQQopwICYFL9EFA+QAINeY/Y99Lf3B11JfLU8\njN9uGHKFI0FVwHX428ObVrDi3+OCNewQ3fLmrRQe6F6q2OU899huCg+eYECWvxZR\na3SlVZmYnefbA87jI2FRHUPqxp4P2mDwj/RZxhgIobhw0zz08sqC6DW0Aj1OIJe5\nsDAm0uiUdqs7FZN2uKkLKekdTgW0QkTFEJTk5Yk9t/hOrjnHoWQfB+mLhO3vPhip\nvhs=\n-----END CERTIFICATE-----\nrepository: tap-apps\nserver: harbor.services.h2o-2-9349.h2o.vmware.com\ntemplateRef:\nkind: ClusterTemplate\nname: config-writer-template\n- name: deliverable\nparams:\n- name: registry\nvalue:\nca_cert_data: |-\n-----BEGIN CERTIFICATE-----\nMIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\nBQAwgY4xCzAJBgNVBAYTAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAO\nBgNVBAcTB1V0cmVjaHQxFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMU\nS2Vhcm9zIFRhbnp1IFJvb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290\nIENBMB4XDTIyMDMyMzE1MzUwMFoXDTI3MDMyMjE1MzUwMFowgY4xCzAJBgNVBAYT\nAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAOBgNVBAcTB1V0cmVjaHQx\nFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMUS2Vhcm9zIFRhbnp1IFJv\nb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290IENBMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyZXDL9W2vu365m//E/w8n1M189a5mI9HcTYa\n0xZhnup58Zp72PsgzujI/fQe43JEeC+aIOcmsoDaQ/uqRi8p8phU5/poxKCbe9SM\nf1OflLD9k2dwte6OV5kcSUbVOgScKL1wGEo5mdOiTFrEp5aLBUcbUeJMYz2IqLVa\nv52H0vTzGfmrfSm/PQb+5qnCE5D88DREqKtWdWW2bCW0HhxVHk6XX/FKD2Z0FHWI\nChejeaiarXqWBI94BANbOAOmlhjjyJekT5hL1gh7BuCLbiE+A53kWnXO6Xb/eyuJ\nobr+uHLJldoJq7SFyvxrDd/8LAJD4XMCEz+3gWjYDXMH7GfPWwIDAQABo0IwQDAO\nBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUfGU50Pe9\nYTv5SFvGVOz6R7ddPcUwDQYJKoZIhvcNAQELBQADggEBAHMoNDxy9/kL4nW0Bhc5\nGn0mD8xqt+qpLGgChlsMPNR0xPW04YDotm+GmZHZg1t6vE8WPKsktcuv76d+hX4A\nuhXXGS9D0FeC6I6j6dOIW7Sbd3iAQQopwICYFL9EFA+QAINeY/Y99Lf3B11JfLU8\njN9uGHKFI0FVwHX428ObVrDi3+OCNewQ3fLmrRQe6F6q2OU899huCg+eYECWvxZR\na3SlVZmYnefbA87jI2FRHUPqxp4P2mDwj/RZxhgIobhw0zz08sqC6DW0Aj1OIJe5\nsDAm0uiUdqs7FZN2uKkLKekdTgW0QkTFEJTk5Yk9t/hOrjnHoWQfB+mLhO3vPhip\nvhs=\n-----END CERTIFICATE-----\nrepository: tap-apps\nserver: harbor.services.h2o-2-9349.h2o.vmware.com\n- default: go-git\nname: gitImplementation\ntemplateRef:\nkind: ClusterTemplate\nname: deliverable-template\nselector:\napps.tanzu.vmware.com/has-tests-needs-workspace: \"true\"\nselectorMatchExpressions:\n- key: apps.tanzu.vmware.com/workload-type\noperator: In\nvalues:\n- web\n- server\n- worker\nstatus: {}\n</code></pre> <p>And then apply it to the cluster.</p> <pre><code>kubectl apply -f source-test-scan-to-url.yaml\n</code></pre> <p>Then we can verify if our new Supply Chain is valid:</p> <pre><code>kubectl get ClusterSupplyChain\n</code></pre> <p>Which should yield:</p> <pre><code>NAME                                READY   REASON   AGE\nscanning-image-scan-to-url          True    Ready    13h\nsource-test-scan-to-url             True    Ready    13h\nsource-test-scan-to-url-workspace   True    Ready    8s\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#create-external-service-with-bitnami-services","title":"Create External Service with Bitnami Services","text":"<p>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/bitnami-services-tutorials-working-with-bitnami-services.html</p> <p>Starting from TAP 1.5, TAP includes Crossplane to help with managing External Services, such as Databases.</p> <p>By default, it includes prepared Crossplane packages (called Configurations) for several Bitnami Helm Charts. These are managed via the App <code>bitnami-services</code>, which is part of the Iterate, Run, and Full profile.</p> <p>We will use these services, to create a \"production\" database for our application.</p>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#view-existing-services","title":"View Existing Services","text":"<p>Let's look at the services that we have available.</p> <pre><code>tanzu service class list\n</code></pre> <p>Which should yield the following:</p> <pre><code>  NAME                  DESCRIPTION\n  mysql-unmanaged       MySQL by Bitnami\n  postgresql-unmanaged  PostgreSQL by Bitnami\n  rabbitmq-unmanaged    RabbitMQ by Bitnami\n  redis-unmanaged       Redis by Bitnami\n</code></pre> <p>We can explore the services with the Tanzu CLI, for example, the Postgresql:</p> <pre><code>tanzu service class get postgresql-unmanaged\n</code></pre> <p>Which gives a few details, such as the parameters:</p> <pre><code>NAME:           postgresql-unmanaged\nDESCRIPTION:    PostgreSQL by Bitnami\nREADY:          true\nPARAMETERS:\n  KEY        DESCRIPTION                                                  TYPE     DEFAULT  REQUIRED\n  storageGB  The desired storage capacity of the database, in Gigabytes.  integer  1        false\n</code></pre> <p>In this case, there is only one parameter, <code>storageGB</code>, which is optional.</p>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#claim-external-service","title":"Claim External Service","text":"<p>Let's create an external service to be consumed by a Workload.</p> <pre><code>export SQL_CLASS_CLAIM_NAME=psql-1\nexport TAP_DEVELOPER_NAMESPACE=\n</code></pre> <p>Important</p> <p>If binding to more than one application workload then all application workloads must exist in the same namespace. This is a known limitation. For more information, see Cannot claim and bind to the same service instance from across multiple namespaces2.</p> <p>We then create a Class Claim, which means we create a Service of \"Class\". This is a static generation, but when defining them dynamically, a Class works like Storage Classes.</p> <p>For now, we can ignore those details, and we crate our Service by creating the Class Claim.</p> <pre><code>tanzu service class-claim create ${SQL_CLASS_CLAIM_NAME} \\\n--class postgresql-unmanaged \\\n--parameter storageGB=\"3\" \\\n--namespace ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>We can view our Class Claim:</p> <pre><code>tanzu services class-claims list -n  ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>Which shows it is ready:</p> <pre><code>  NAME    CLASS                 READY  REASON\n  psql-1  postgresql-unmanaged  True   Ready\n</code></pre> <p>For more details, use the <code>get</code> command:</p> <pre><code>tanzu services class-claims get ${SQL_CLASS_CLAIM_NAME} \\\n--namespace ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>Which gives this details repsonse:</p> <pre><code>Name: psql-1\nNamespace: dev\nClaim Reference: services.apps.tanzu.vmware.com/v1alpha1:ClassClaim:psql-1\nClass Reference:\n  Name: postgresql-unmanaged\nParameters:\n  storageGB: 3\nStatus:\n  Ready: True\n  Claimed Resource:\n    Name: 7af21ebe-bf15-4e88-9607-72bfcf9d3cb7\n    Namespace: dev\n    Group:\n    Version: v1\n    Kind: Secret\n</code></pre> <p>If you're curious, you can verify there isn't currently running any database or any pod for that matter, in that Namespace.</p> <pre><code>kubectl get po -n $DEV_TEAM_NAMESPACE\n</code></pre> <p>Which should result into this:</p> <pre><code>No resources found in dev-team-1 namespace.\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#fork-test-application","title":"Fork Test Application","text":"<p>Go to the Gitea instance, and find the <code>shared/spring-boot-postgres</code> instance.</p> <p>Or go directly to it here.</p> <p>Then click Fork, to add it to your User.</p>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#create-workload","title":"Create Workload","text":"<p>Now we can create a Workload that consumes this service.</p> <p>Assuming the Pipeline and SupplyChain we defined is valid, this will use a Testcontainer database for testing, and a \"real\" database when deployed.</p> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\nexport SSH_SECRET=ssh-credentials\nexport LAB=\n</code></pre> <pre><code>tanzu apps workload create spring-boot-postgres-01 \\\n--namespace ${TAP_DEVELOPER_NAMESPACE} \\\n--git-repo ssh://git@gitssh.h2o-2-9349.h2o.vmware.com/${LAB}/spring-boot-postgres.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=spring-boot-spring-01 \\\n--label apps.tanzu.vmware.com/has-tests-needs-workspace=true \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--build-env BP_JVM_VERSION=17 \\\n--param gitops_ssh_secret=${SSH_SECRET} \\\n--service-ref db=services.apps.tanzu.vmware.com/v1alpha1:ClassClaim:${SQL_CLASS_CLAIM_NAME} \\\n--yes\n</code></pre> <p>SourceScan Error with no Data</p> <p>Unfortunately, the ScanTemplate setup doesn't always handle to secrets correctly.</p> <p>The Scan Templates are generated by TAP when you install the profile.</p> <p>They need to contact the Metadata Store, and need to trust its Certificate.</p> <p>It does so by importing the <code>app-tls-cert</code> from the <code>metadata-store</code> Namespace.</p> <p>Sometimes this fails, and then the Source Scan returns this data:</p> <pre><code>kubectl get sourcescan -n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <pre><code>NAME          PHASE   SCANNEDREVISION   SCANNEDREPOSITORY   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN   CVETOTAL\ntap-demo-04   Error                                         19m\n</code></pre> <p>Read this paragraph for a more permanent solution.</p> <p>To remedy this, we can do the secret export and import ourselves with the SecretGen Controller:</p> metadata-tls-secret-import-and-export.yaml<pre><code>---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretExport\nmetadata:\nname: app-tls-cert\nnamespace: metadata-store\nspec:\ntoNamespace: dev\n---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretImport\nmetadata:\nname: app-tls-cert\nnamespace: dev\nspec:\nfromNamespace: metadata-store\n</code></pre> <p>And then apply it to the cluster.</p> <pre><code>kubectl apply -f metadata-tls-secret-import-and-export.yaml \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#security-scan-failed","title":"Security Scan Failed","text":"<p>If you look at the Supply Chain view in TAP GUI, you will notice the Source Scan failed.</p> <p>That is because this application has a few more violations.</p> <p>Failed because of 4 violations: CVE postgresql CVE-2015-0244 {\"Critical\"}. CVE postgresql CVE-2015-3166 {\"Critical\"}. CVE postgresql CVE-2018-1115 {\"Critical\"}. CVE postgresql CVE-2019-10211 {\"Critical\"}</p> <p>We can add them to the list of ingored CVE's in our current ScanPolicy.</p> <pre><code>ignoreCves := [\"GHSA-36p3-wjmg-h94x\", \"CVE-2015-0244\", \"CVE-2015-3166\", \"CVE-2018-1115\", \"CVE-2019-10211\", \"CVE-2016-1000027\"]\n</code></pre> <p>Or, we can create a new policy, just for this application.</p> spring-boot-postgres-policy.yaml<pre><code>apiVersion: scanning.apps.tanzu.vmware.com/v1beta1\nkind: ScanPolicy\nmetadata:\nlabels:\napp.kubernetes.io/part-of: enable-in-gui\nname: scan-policy-spring-boot-postgres\nspec:\nregoFile: |\npackage main\n# Accepted Values: \"Critical\", \"High\", \"Medium\", \"Low\", \"Negligible\", \"UnknownSeverity\"\nnotAllowedSeverities := [\"Critical\"]\nignoreCves := [\"GHSA-36p3-wjmg-h94x\", \"CVE-2015-0244\", \"CVE-2015-3166\", \"CVE-2018-1115\", \"CVE-2019-10211\", \"CVE-2016-1000027\"]\ncontains(array, elem) = true {\narray[_] = elem\n} else = false { true }\nisSafe(match) {\nseverities := { e | e := match.ratings.rating.severity } | { e | e := match.ratings.rating[_].severity }\nsome i\nfails := contains(notAllowedSeverities, severities[i])\nnot fails\n}\nisSafe(match) {\nignore := contains(ignoreCves, match.id)\nignore\n}\ndeny[msg] {\ncomps := { e | e := input.bom.components.component } | { e | e := input.bom.components.component[_] }\nsome i\ncomp := comps[i]\nvulns := { e | e := comp.vulnerabilities.vulnerability } | { e | e := comp.vulnerabilities.vulnerability[_] }\nsome j\nvuln := vulns[j]\nratings := { e | e := vuln.ratings.rating.severity } | { e | e := vuln.ratings.rating[_].severity }\nnot isSafe(vuln)\nmsg = sprintf(\"CVE %s %s %s\", [comp.name, vuln.id, ratings])\n}\n</code></pre> <pre><code>kubectl apply -f spring-boot-postgres-policy.yaml\\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>We will update our Workload1 to use this specific Policy for its Source and Image scan:</p> <pre><code>--param scanning_source_policy=\"scan-policy-spring-boot-postgres\" \\\n--param scanning_image_policy=\"scan-policy-spring-boot-postgres\" \\\n</code></pre> <p>To update an existing Workload, we can use <code>tanzu apps workload apply</code>:</p> <pre><code>tanzu apps workload apply spring-boot-postgres-01 \\\n--namespace ${TAP_DEVELOPER_NAMESPACE} \\\n--git-repo ssh://git@gitssh.h2o-2-9349.h2o.vmware.com/${LAB}/spring-boot-postgres.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=spring-boot-spring-01 \\\n--label apps.tanzu.vmware.com/has-tests-needs-workspace=true \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--build-env BP_JVM_VERSION=17 \\\n--param gitops_ssh_secret=${SSH_SECRET} \\\n--param scanning_source_policy=\"scan-policy-spring-boot-postgres\" \\\n--param scanning_image_policy=\"scan-policy-spring-boot-postgres\" \\\n--service-ref db=services.apps.tanzu.vmware.com/v1alpha1:ClassClaim:${SQL_CLASS_CLAIM_NAME} \\\n--yes\n</code></pre> <p>Push a change the repository to trigger a new build.</p>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#test-the-application","title":"Test The Application","text":"<pre><code>kubectl get httpproxy -n dev\n</code></pre> <p>Collect the URL, something like <code>spring-boot-postgres-01.dev.lab02.h2o-2-9349.h2o.vmware.com</code></p> <pre><code>export APP_URL=\n</code></pre> <pre><code>curl -lk \"https://${APP_URL}\"\n</code></pre> <p>Which returns an empty list: </p> <pre><code>[]\n</code></pre> <p>We can verify the database works by adding an entry then retrieving the results again.</p> <pre><code>curl -X POST -lk \"https://${APP_URL}/create\" -d '{\"name\": \"piet\"}' \\\n-H \"content-type: application/json\"\n</code></pre> <p>Querying the server again:</p> <pre><code>curl -lk \"https://${APP_URL}\"\n</code></pre> <p>Now results in a value:</p> <pre><code>[{\"id\":1,\"userId\":null,\"name\":\"piet\",\"creationDate\":\"2023-05-17T11:01:02.136+00:00\"}]\n</code></pre>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/external-service/#references","title":"References","text":"<ol> <li> <p>TAP 1.5 - More Tekton and Scan Policy resources per Namespace \u21a9</p> </li> <li> <p>TAP 1.5 - Know Limitations With Class Claims \u21a9</p> </li> </ol>","tags":["tap","kubernetes","spring","java","spring-boot","mysql","crossplane"]},{"location":"apps/hello-world/","title":"Workload from Self-hosted Git","text":"<p>In this workshop, we explore the creation, updating, and onboarding of a TAP Workload from a self-hosted Git server.</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#requirements","title":"Requirements","text":"<p>Before you can proceed with this workshop, ensure you have met the following requirements:</p> <ul> <li>Kubernetes cluster with TAP installed</li> <li>With one of the following TAP Profiles</li> <li>Full or Iterate (recommended), or a combination of View, Build &amp; Run (and then manually copy the Deliverable)</li> <li>Self-hosted Git server with TLS</li> <li>Ideally with the CA certificate on hand</li> <li>SSH Key usable for the self-hosted Git server</li> </ul>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#checks","title":"Checks","text":"<ul> <li> I'm able to use an accelerator to generate a Web TAP Workload</li> <li> I applied a TAP Web Workload and register it within TAP GUI</li> <li> I'm able to do basic troubleshooting of a TAP Workload</li> <li> I've configured App Live View in TAP GUI for a TAP Web Workload</li> <li> I've configured Tanzu Build Service and it works for a Spring Boot Application to produce an Image</li> <li> I am able to view and verify a TAP Workload displays in properly in Supply Chain view of TAP GUI</li> <li> I know how to configure an integration to a private Git provider in TAP GUI</li> <li> I know how to configure access to private repositories (SCM and Container) for TAP Workloads</li> <li> I have configured custom CA's for TAP</li> </ul> <p>To add:</p> <ul> <li> I am able to view and verify results for the Security Analysis within TAP GUI</li> </ul>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#steps","title":"Steps","text":"<ul> <li>go to TAP GUI -&gt; Create | use VSCode Extention<ul> <li>https://tap-gui.view.h2o-2-9349.h2o.vmware.com/create</li> <li>Tanzu Java Web UI</li> </ul> </li> <li>create project in Gitea<ul> <li>make the project public (should be the default)</li> </ul> </li> <li>Push project to Gitea</li> <li> <p>create secret for Gitea with Credentials &amp; CA Cert</p> </li> <li> <p>create workload</p> </li> <li>verify application runs</li> <li>update TAP GUI config<ul> <li>to trust Gitea</li> </ul> </li> <li>register application in TAP GUI</li> <li>view app resources in App Live View</li> </ul> <p>The workshop consists of the following steps:</p> <ul> <li>Generate Application: to create the application source code to use for our Workload</li> <li>Import the Application's source code into Gitea</li> <li>Onboard the Application into TAP</li> <li>View the Application's Supply Chain in TAP GUI</li> <li>Register the Appliction in TAP GUI</li> <li>View Application's live resources in App Live View (part of TAP)</li> </ul>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#generate-project","title":"Generate Project","text":"<p>Go to the TAP GUI, either your own or the shared environment TAP GUI.</p> <p>Then go the to Create page, or the <code>+</code> icon (with a circle) in the left hand menu.</p> <p>Select <code>Tanzu Java Web App</code>, by clicking on the <code>CHOOSE</code> button next it, or go directly to the wizard screen.</p> <ul> <li>Name: give it a unique name, at least for our Gitea instance</li> <li>Prefix: you can leave this to <code>dev.local</code></li> <li>Use Spring Boot 3.0: as the name says, if you use this, you have to use <code>Java 17</code>, which you set in the Workload definition<ul> <li>either in the Workload manifest, or add <code>--build-env BP_JVM_VERSION=17</code> to the <code>tanzu apps workload create</code> command</li> </ul> </li> <li>Java Version: either leave it on Java 11 or set it to Java 17 (recommended)</li> </ul> <p>Click <code>NEXT</code>.</p> <p>Review the settings, for example:</p> <pre><code>Project Name: tap-demo-04\nRepository Prefix: dev.local\nUpdate Boot 3: Yes\nJava Version: 17\nInclude Build Tool Wrapper: Yes\n</code></pre> <p>And then click <code>GENERATE ACCELERATOR</code>.</p> <p>Wait a few seconds, and the click <code>DOWNLOAD ZIP FILE</code>.</p> <pre><code>export APP_NAME=\nexport LAB=\n</code></pre> <p>Then SCP the Zip to your Lab:</p> <pre><code>scp ~/Downloads/${APP_NAME}.zip ubuntu@${LAB}.h2o-2-9349.h2o.vmware.com:/home/ubuntu\n</code></pre> <p>Test the Zip:</p> <pre><code>unzip -t ${APP_NAME}.zip\n</code></pre> <p>And then unpack it:</p> <pre><code>unzip ${APP_NAME}.zip\n</code></pre> <p>We should now have the application's source ready to import into Gitea.</p> <pre><code>tree $APP_NAME\n</code></pre> <p>Which should yield:</p> <pre><code>tap-demo-04\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 Tiltfile\n\u251c\u2500\u2500 accelerator-info.yaml\n\u251c\u2500\u2500 accelerator-log.md\n\u251c\u2500\u2500 catalog\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 catalog-info.yaml\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workload.yaml\n\u251c\u2500\u2500 mvnw\n\u251c\u2500\u2500 mvnw.cmd\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 java\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 com\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 example\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 springboot\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u251c\u2500\u2500 Application.java\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2514\u2500\u2500 HelloController.java\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 resources\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 application.yml\n    \u2514\u2500\u2500 test\n\u2514\u2500\u2500 java\n            \u2514\u2500\u2500 com\n                \u2514\u2500\u2500 example\n                    \u2514\u2500\u2500 springboot\n                        \u2514\u2500\u2500 HelloControllerTest.java\n</code></pre> <p>Use IDE Plugins</p> <p>TAP also has an IDE plugin for VSCode and Jetbrain's IDE's. When this plugin is connected to a TAP GUI, you can create a new Accelerator based Application directly from your IDE.</p> <p>TODO: add link</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#import-into-gitea","title":"Import into Gitea","text":"<p>Before we can push our newly created Project to Gitea, we need a repository in Gitea.</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#create-gitea-repository","title":"Create Gitea Repository","text":"<p>You can login into Gitea and create a new Repository that way, or use the REST API:</p> <pre><code>export APP_NAME=\nexport LAB=\n</code></pre> <p>Replace App Name bin the command</p> <p>Make sure you replace <code>REPLACE_WITH_APP_NAME</code> with your App Name.</p> <pre><code>curl -k -X POST \"https://gitea.services.h2o-2-9349.h2o.vmware.com/api/v1/user/repos\" \\\n-u ${LAB}:'12345678' \\\n-H \"content-type: application/json\" \\\n--data '{\"auto_init\": false,\"default_branch\": \"main\",\"name\": \"REPLACE_WITH_APP_NAME\",\"private\": true}'\n</code></pre>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#push-to-gitea","title":"Push to Gitea","text":"<p>Enter the directory of the application:</p> <pre><code>cd $APP_NAME\n</code></pre> <p>And then run the following Git commands, to push the branch to Gitea.</p> <pre><code>git init\ngit add .\ngit commit -m \"first commit\"\ngit remote add origin \"git@gitssh.h2o-2-9349.h2o.vmware.com:${LAB}/${APP_NAME}.git\"\ngit branch -m main\ngit push -u origin main\n</code></pre> <p>The SSH should be configured for you, so it should push directly.</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#onboard-application-into-tap","title":"Onboard Application Into TAP","text":"<p>Your application is now in Gitea, great!</p> <p>As you might have noticed, we made the repository private. While not required in this case, it is expected in customer environments that Repositories require credendentials.</p> <p>So before we can onboard our application via a Workload definition, we need to create a credential so [FluxCD]https://fluxcd.io/flux/components/source/gitrepositories/)1 can checkout the code.</p> <p>We have two things to look at:</p> <ol> <li>do we want a Per Repository credential (e.g., per Workload) or per TAP Profile (not recommended)</li> <li>do we want to use a HTTPS credential (username, password, cert) or SSH Key credential</li> </ol>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#create-credential","title":"Create Credential","text":"<pre><code>export TAP_DEVELOPER_NAMESPACE=dev\nexport GITEA_SSH_URL=gitssh.h2o-2-9349.h2o.vmware.com\nexport LAB=\nexport LAB_GITEA_PASSWORD=\n</code></pre> <p>We will use a Credential per Repository, so we will include it in our Workload definition later.</p> <p>First, we must decide on using a HTTPS credential or SSH. Both will work, with the HTTPS type we need to include the CA, with the SSK Key we need to include the Known Hosts.</p> HTTPS CredentialSSH Credential <pre><code>kubectl create secret generic https-credentials \\\n--namespace $TAP_DEVELOPER_NAMESPACE \\\n--from-file caFile=ca.crt \\\n--from-literal username=\"${LAB}\" \\\n--from-literal password=\"${LAB_GITEA_PASSWORD}\"\n</code></pre> <pre><code>ssh-keyscan $GITEA_SSH_URL &gt; gitea-known-hosts.txt\n</code></pre> ssh-secret.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: #@ data.values.secretName\nannotations:\ntekton.dev/git-0: #@ data.values.server\ntype: kubernetes.io/ssh-auth\nstringData:\nssh-privatekey: #@ data.values.sshPushKey\nidentity: #@ data.values.sshPullKey\nidentity.pub: #@ data.values.sshPullId\nknown_hosts: #@ data.values.knownHosts\n</code></pre> <pre><code>export GIT_SSH_SECRET_KEY=\"ssh-credentials\"\nexport GIT_SSH_PUSH_KEY=$(cat ~/.ssh/id_ed25519)\nexport GIT_SSH_PULL_KEY=$(cat ~/.ssh/id_ed25519)\nexport GIT_SSH_PULL_ID=$(cat ~/.ssh/id_ed25519.pub)\nexport GIT_SSH_KNOWN_HOSTS=$(cat gitea-known-hosts.txt)\n</code></pre> <pre><code>ytt -f ssh-secret.ytt.yaml \\\n-v secretName=\"$GIT_SSH_SECRET_KEY\" \\\n-v server=\"$GITEA_SSH_URL\" \\\n-v sshPushKey=\"$GIT_SSH_PUSH_KEY\" \\\n-v sshPullKey=\"$GIT_SSH_PULL_KEY\" \\\n-v sshPullId=\"$GIT_SSH_PULL_ID\" \\\n-v knownHosts=\"$GIT_SSH_KNOWN_HOSTS\" \\\n&gt; \"ssh-secret.yaml\"\n</code></pre> <pre><code>kubectl apply -f ssh-secret.yaml \\\n--namespace ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>You should now have a usable credential, and can create the Workload.</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#create-workload-via-tanzu-cli","title":"Create Workload Via Tanzu CLI","text":"<pre><code>export TAP_DEVELOPER_NAMESPACE=dev\nexport JAVA_VERSION=17\nexport APP_NAME=\n</code></pre> <p>Warning</p> <p>Make sure you set the <code>JAVA_VERSION</code> to the version you selected when you created the application!</p> <p>As the repository URL and the Credential we used depends on the type we created earlier, there are different commands to set them.</p> HTTPS CredentialSSH Credential <pre><code>export APP_REPO=\"https://gitea.services.h2o-2-9349.h2o.vmware.com/${LAB}/${APP_NAME}.git\"\nexport APP_REPO_SECRET=https-credentials\n</code></pre> <p>Also note, that FluxCD doesn't support the SSH URL that Gitea generates, so we use a <code>/</code> after the SSH URL (instead of the traditional <code>:</code>). <pre><code>export APP_REPO=\"ssh://git@${GITEA_SSH_URL}/${LAB}/${APP_NAME}.git\"\nexport APP_REPO_SECRET=\"${GIT_SSH_SECRET_KEY}\"\n</code></pre></p> <p>We can now create the Workload:</p> <p>Note</p> <p>Note that the parameter is called <code>gitops_ssh_secret</code>.</p> <p>The name implies its for SSH Keys only, but that is not true. The secret will be used by FluxCD1 to checkout the repository.</p> <pre><code>tanzu apps workload create ${APP_NAME} \\\n--namespace ${TAP_DEVELOPER_NAMESPACE} \\\n--git-repo ${APP_REPO} \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=${APP_NAME} \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--param gitops_ssh_secret=${APP_REPO_SECRET} \\\n--build-env BP_JVM_VERSION=${JAVA_VERSION} \\\n--yes\n</code></pre>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#create-workload-via-manifest","title":"Create Workload Via Manifest","text":"<p>The application we generated contains a Workload definition!</p> <p>Alas, we need to add the <code>gitops_ssh_secret</code> parameter before we can apply it.</p> <p>Open up the Workload definition:</p> <pre><code>vim config/workload.yaml\n</code></pre> <p>Replace URL Placeholder</p> <p>Replace the URL placeholder values, with your HTTPS or SSH URLs.</p> <p>Add a parameter for the Secret: <code>gitops_ssh_secret</code>:</p> <pre><code>  - name: gitops_ssh_secret\n    value: ssh-credentials\n</code></pre> Full Example <p>This a full example of a <code>workload.yaml</code> after the changes.</p> config/workload.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: YOUR_APP\nlabels:\napps.tanzu.vmware.com/workload-type: web\napps.tanzu.vmware.com/has-tests: \"true\"\napps.tanzu.vmware.com/auto-configure-actuators: \"true\"\napp.kubernetes.io/part-of: YOUR_APP\nspec:\nbuild:\nenv:\n- name: BP_JVM_VERSION\nvalue: \"17\"\nparams:\n- name: gitops_ssh_secret\nvalue: ssh-credentials\n- name: annotations\nvalue:\nautoscaling.knative.dev/minScale: \"1\"\nsource:\ngit:\nurl: ssh://git@gitssh.h2o-2-9349.h2o.vmware.com/YOUR_LAB/YOUR_APP.git\nref:\nbranch: main\n</code></pre> <pre><code>kubectl apply -f config/workload.yaml \\\n-n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#verify-workload","title":"Verify Workload","text":"<p>Verify the Workload exists and is valid.</p> <pre><code>kubectl get workload -n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>Verify if FluxCD can checkout your application's source from Gitea.</p> <pre><code>kubectl get GitRepository -n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>Verify if your Workload is targeting a valid Supply Chain.</p> <pre><code>tanzu apps workload get ${APP_NAME} -n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>Tail the logs:</p> <pre><code>tanzu apps workload tail ${APP_NAME} -n ${TAP_DEVELOPER_NAMESPACE} --timestamp --since 1h\n</code></pre> <p>And last but not least, once the Supply Chain completes, retrieve its HTTPProxy:</p> <pre><code>kubectl get httpproxy \\\n-n ${TAP_DEVELOPER_NAMESPACE} \\\n-l contour.networking.knative.dev/parent=${APP_NAME}\n</code></pre> <p>It will have four proxy entries, select the one with the external DNS name and save it:</p> <pre><code>export URL=\n</code></pre> <p>Now we can <code>curl</code> the application:</p> <pre><code>curl -lk \"https://${URL}\"\n</code></pre> <p>Which should respond with:</p> <pre><code>Greetings from Spring Boot + Tanzu!\n</code></pre>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#update-workload","title":"Update Workload","text":"<p>Let's test the update of the Workload.</p> <p>If all went well, the curl command returned <code>Greetings from Spring Boot + Tanzu!</code>.</p> <p>Let's change this.</p> <p>The place this line is written, is in the <code>HelloController</code>.</p> <p>The file resides here: <code>src/main/java/com/example/springboot/HelloController.java</code>.</p> <p>If we change it here, we must also change it in the test: <code>src/test/java/com/example/springboot/HelloControllerTest.java</code>.</p> <p>Make sure all three occurances are the same. Either manually edit the files with vim, or use the sed command below:</p> <pre><code>export ORIGINAL=\"Greetings from Spring Boot + Tanzu!\"\nexport NEW=\"Greetings from Barcelona!\"\n</code></pre> <p>Feel free to replace <code>NEW</code> with your own text.</p> <pre><code>sed -i -e \\\n\"s/$ORIGINAL/$NEW/g\" \\\nsrc/main/java/com/example/springboot/HelloController.java\n\nsed -i -e \\\n\"s/$ORIGINAL/$NEW/g\" \\\nsrc/test/java/com/example/springboot/HelloControllerTest.java\n</code></pre> <p>Then we add, commit, and push the changes:</p> <pre><code>git add src/\ngit commit -m \"change test\"\n</code></pre> <p>Verify the change happend:</p> <pre><code>curl -lk \"https://${URL}\"\n</code></pre> <p>Which should now return (or your message):</p> <pre><code>Greetings from Barcelona!\n</code></pre>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#view-app-supply-chain","title":"View App Supply Chain","text":"<p>Open the TAP GUI and visit the Supply Chain screen.</p> <p>You should be able to see your application listed there.</p> <p>If we want to see more of the application's resources, such as its (runtime) Deployment, we need to register the application.</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#register-application-in-tap-gui","title":"Register Application in TAP GUI","text":"<p>Before we can register our application in the TAP GUI, we need to make TAP GUI trust our Gitea server.</p> <p>We do this by updating the TAP Profile intallation values.</p> <p>We will do the following steps:</p> <ul> <li>Update TAP GUI Config / TAP install</li> <li>Register Application in TAP GUI</li> <li>View Workload in App Live View (part of TAP GUI)</li> </ul>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#update-profile","title":"Update Profile","text":"<p>We do this by updating the TAP GUI configuration in our profile. Either change the <code>ytt</code> template, or edit the <code>tap-values-full.yml</code> file directly.</p> DirectlyVia YTT Template <p>Add the <code>backend</code> and <code>integrations</code> segment to the <code>tap_gui.app_config</code> property, so that it looks like below. <pre><code>tap_gui:\napp_config:\nbackend:\nreading:\nallow:\n- host: \"gitea.services.h2o-2-9349.h2o.vmware.com\"\nintegrations:\ngitea:\n- host: gitea.services.h2o-2-9349.h2o.vmware.com\nusername: gitea\npassword: 'VMware123!'\n</code></pre></p> full-profile.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: full\nshared:\ningress_domain: #@ dv.domainName\nca_cert_data: #@ dv.caCert\nimage_registry:\nsecret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nkp_default_repository: #@ kpRegistry\nkp_default_repository_secret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nsupply_chain: basic\nootb_supply_chain_basic:\nregistry:\nserver: #@ dv.buildRegistry\nrepository: #@ dv.buildRepo\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: #@ \"appliveview.\"+dv.domainName\nappliveview:\ningressEnabled: true\nserver:\ntls:\nenabled: false\ntap_gui:\nservice_type: ClusterIP\napp_config:\nbackend:\nreading:\nallow:\n- host: #@ dv.gitServer\nintegrations:\ngitea:\n- host: #@ dv.gitServer\nusername: #@ dv.gitUser\npassword: #@ dv.gitPassword\nauth:\nallowGuestAccess: true\ncustomize:\ncustom_name: 'Portal McPortalFace'\norganization:\nname: 'Org McOrg Face'\ncatalog:\nlocations:\n- type: url\ntarget: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n- type: url\ntarget: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\ncrossplane:\nregistryCaBundleConfig:\nname: ca-bundle-config\nkey: ca-bundle\n#! reduces memory and CPU requirements, not recommended for production\n#! but our Lab environments have resource restrictions\ncnrs:\nlite:\nenable: true contour:\nenvoy:\nservice:\ntype: LoadBalancer\nceip_policy_disclosed: true\nexcluded_packages:\n- scanning.apps.tanzu.vmware.com #! disabled for now, enabled when we upgrade OOTB Basic to Test &amp; Scanning\n- grype.scanning.apps.tanzu.vmware.com #! disabled for now, enabled when we upgrade OOTB Basic to Test &amp; Scanning\n- policy.apps.tanzu.vmware.com #! disabled for now, enabled when we upgrade OOTB Basic to Test &amp; Scanning\n- eventing.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n- tap-telemetry.tanzu.vmware.com.0.5.0-build #! not used, so removing to reduce memory/cpu footprint\n</code></pre> <pre><code>export TAP_BUILD_REGISTRY_SECRET=registry-credentials\nexport BUILD_REGISTRY_REPO=tap-apps\nexport TBS_REPO=buildservice/tbs-full-deps\nexport CA_CERT=$(cat ca.crt)\nexport BUILD_REGISTRY=\nexport DOMAIN_NAME=\n</code></pre> <pre><code>export GIT_SERVER=gitea.services.h2o-2-9349.h2o.vmware.com\nexport GIT_USER=gitea\nexport GIT_PASSWORD='VMware123!'\n</code></pre> <p>And then we run YTT to generate our Profile configuration file.</p> <pre><code>ytt -f full-profile.ytt.yaml \\\n-v buildRegistry=\"$BUILD_REGISTRY\" \\\n-v buildRegistrySecret=\"$BUILD_REGISTRY_SECRET\" \\\n-v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n-v tbsRepo=\"$TBS_REPO\" \\\n-v domainName=\"$DOMAIN_NAME\" \\\n-v caCert=\"${CA_CERT}\" \\\n-v gitUser=\"${GIT_USER}\" \\\n-v gitPassword=\"${GIT_PASSWORD}\" \\\n-v gitServer=\"${GIT_SERVER}\" \\\n&gt; \"tap-values-full.yml\"\n</code></pre> <p>We recommend you inspect the generated file:</p> <pre><code>cat tap-values-full.yml\n</code></pre> <p>We then update the TAP installation via the same command.</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_VERSION=1.5.0\n</code></pre> <pre><code>tanzu package install tap \\\n-p tap.tanzu.vmware.com \\\n-v $TAP_VERSION \\\n--values-file tap-values-full.yml \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#register-app-in-tap-gui","title":"Register App in TAP GUI","text":"<p>To get a live view2 of our application, we have to register it in the Software Catalog3.</p> <p>We do so by going to the <code>Home</code> screen, via the house icon in the left hand menu.</p> <p>We can then add the application, by clicking the <code>REGISTER ENTITY</code> button on the right.</p> <p>Here we add a link to the <code>catalog-info.yaml</code> of the application.</p> <p>Our generated application has such a file generated, in the <code>catalog</code> folder.</p> <p>The URL to use, is the raw URL, which should look like this: <code>https://gitea.services.h2o-2-9349.h2o.vmware.com/lab02/tap-demo-04/raw/branch/main/catalog/catalog-info.yaml</code></p> <p>If all went well, you should now see your application on the TAP GUI home screen.</p>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/hello-world/#references","title":"References","text":"<ol> <li> <p>FluxCD Git Repository authentication \u21a9\u21a9</p> </li> <li> <p>Tanzu Application Platform - App Live View \u21a9</p> </li> <li> <p>Backstage - Software Catalog \u21a9</p> </li> </ol>","tags":["tap","kubernetes","spring","java"]},{"location":"apps/where-for-dinner/","title":"TAP Where For Dinner App","text":"","tags":["tap","kubernetes","spring","java","spring-boot","micoservice"]},{"location":"apps/where-for-dinner/#source","title":"Source","text":"<ul> <li>https://github.com/vmware-tanzu/application-accelerator-samples/blob/main/where-for-dinner/doc/TAPDeployment.md</li> </ul>","tags":["tap","kubernetes","spring","java","spring-boot","micoservice"]},{"location":"custom/accelerator/","title":"TAP Create Accelerator","text":"<p>TODO</p>","tags":["tap","kubernetes","install"]},{"location":"custom/tap-gui-auth/","title":"TAP GUI Auth","text":"<ul> <li>https://backstage.io/docs/auth/gitlab/provider</li> <li> <p>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/tap-gui-auth.html</p> </li> <li> <p>setup auth in GitLab</p> </li> <li>update TAP View's values</li> <li>update TAP View install</li> </ul> <p>In GitLab</p> <ul> <li>new application</li> <li>name: <code>tap-gui</code></li> <li>redirect URI: <code>https://tap-gui.view.h2o-2-9349.h2o.vmware.com/api/auth/gitlab/handler/frame</code></li> </ul> <pre><code>tap_gui:\napp_config:\nauth:\nenvironment: development\nproviders:\ngitlab:\ndevelopment:\nclientId: 272e7d7008d5ed4d43124b25d11ff288b5dcea638e065d93167430b67d2712fe\nclientSecret: 0fa2a065896b7214a07217eb29ed8ce21f402f3aed8a11eac1e6fedb2d3479d3\naudience: https://gitlab.services.h2o-2-9349.h2o.vmware.com/\n</code></pre>","tags":["tap","kubernetes","GitLab","TAP","LDAP"]},{"location":"install/basic/","title":"TAP Basic Install","text":"<p>Important</p> <p>The Goals and Outcomes is the work of Rick Farmer.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#goals-outcomes-operator","title":"Goals &amp; Outcomes Operator","text":"<ol> <li>I can do a basic install of TAP within a customer environment</li> <li>I have a basic understanding of the core features of TAP such that I can explain these to a customer</li> <li>I\u2019m able to demo TAP deployment via an OOTB accelerator and can explain the benefits of accelerator</li> </ol>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#checks","title":"Checks","text":"<ul> <li> Know the relevant parts of the docs</li> <li> Understand the prerequisites: TAP install</li> <li> Understand the prerequisites: Jump Host</li> <li> Can configure a TAP Full Profile config file</li> <li> Can discover how to configure TAP Packages</li> <li> Understand how to configure custom CA</li> <li> Can install TAP Full Profile</li> <li> I manually installed TAP</li> <li> I'm able to use an accelerator to generate a Web TAP Workload</li> <li> I'm able to manually setup a single developer namespace</li> <li> I applied a TAP Web Workload and register it within TAP GUI</li> <li> I'm able to do basic troubleshooting of a TAP Workload</li> <li> I'm able to do basic troubleshooting of a TAP Installation</li> <li> I've configured App Live View in TAP GUI for a TAP Web Workload</li> <li> I can update TAP values and reconcile changes on the cluster</li> <li> I understand TAP GUI Catalog System</li> <li> I know how to configure an integration to a private Git provider in TAP GUI</li> <li> I know how to configure access to private container registries in TAP Values</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#optional","title":"Optional","text":"<ul> <li> I'm able to install Learning Center and use the sample workshop.</li> <li> I understand the different Workload Types available in TAP</li> <li> I can configure a public Git provider for TAP GUI in TAP Values</li> <li> I can configure DNS for TAP Endpoints</li> <li> I have relocated TAP Images to my own container registry and understand the imgpkg utility.</li> <li> I understand Kapp Controller and Secret Gen Controller in the context of TAP</li> <li> I've able to configure and install the OOTB Testing and Scanning Supply Chain and verify a Workload</li> <li> I am able to configure a Scan Policy for a Source and Image Scan</li> <li> I am able to create a Tekton Task to execute Unit Tests within a Supply Chain for a Developer Namespace</li> <li> I know how to configure an Auth Provider for TAP GUI access</li> <li> I'm able to configure TAP GUI to read from Metadata Store.</li> <li> I know how to configure access to private repositories (SCM and Container) for TAP Workloads</li> <li> I am able to register API Documentation for a Workload in TAP GUI</li> <li> I am able to view and verify results for the Security Analysis within TAP GUI</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#steps","title":"Steps","text":"<ul> <li>Verify Prerequisites are met</li> <li>Install Cluster Essentials</li> <li>Choose TAP setup (profile, installation type)</li> <li>Review TAP packages configuration options<ul> <li>e.g., TAP values schema -&gt; map value to other package -&gt; other package values schema</li> </ul> </li> <li>Install TAP Profile</li> <li>Install Test Workload</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#verify-prerequisites-are-met","title":"Verify Prerequisites are met","text":"<p>The prerequisites1 we need:</p> <ul> <li>Credentials for Tanzu Net</li> <li>Credentials for local Container Image Registry</li> <li>Accept Tanzu Application Platform EULAs</li> <li>DNS Records for the clusters</li> <li>Suitable Kubernetes clusters</li> <li>Jump Host or other machine with the required tools in place</li> <li>Relocate TAP Images to local Container Image Registry</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#suitable-kubernetes-clusters","title":"Suitable Kubernetes clusters","text":"<p>TAP 1.5 supports Kubernetes 1.24, 1.25 and 1.262.</p> <p>Indicative cluster resource requirements of TAP per profile: (TODO: verify these numbers)</p> Profile Memory/Node Storage/Node vCPU Total Memory Total Iterate 8GB 150GB 12 16GB Build 16GB 150GB 12 12GB View 8GB 50GB 8 8GB Run 8GB 100GB 12 8GB Full 16GB 150BG 16 20GB <p>Warning</p> <p>These are requirements to run the TAP components. This does not include the applications or builds run in the cluster.</p> <p>For example, let's look at an application in a Run cluster. If your application requires 10vCPU and 40GB memory, you add that on top of the TAP requirements. Your Run cluser now needs a minimum of 22vCPU and 48GB of memory.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#tap-images-relocated","title":"TAP Images Relocated","text":"<p>TODO: provide information on what has been relocated and to where for the LAB environment 3</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#optional-install-cluster-essentials","title":"Optional: Install Cluster Essentials","text":"<p>Optional</p> <p>The intention is that your Lab environments has them installed already.</p> <p>You can verify this:</p> <pre><code>kubectl get po -n secretgen-controller\n</code></pre> <pre><code>NAME                                   READY   STATUS    RESTARTS   AGE\nsecretgen-controller-b9d795c44-ml5pk   1/1     Running   0          42m\n</code></pre> <p>And, to be sure:</p> <pre><code>kubectl get pod -n tkg-system\n</code></pre> <pre><code>NAME                                                     READY   STATUS    RESTARTS   AGE\nkapp-controller-55d5dd6486-b47mn                         2/2     Running   0          43m\ntanzu-capabilities-controller-manager-7cdd959657-sb9x6   1/1     Running   0          42m\n</code></pre> <p>If these are not there and both commands return empty, verify you are talking to the correct cluster. And if required, below are the instructions for installing the Cluster Essentials.</p> <p>If they are installed, proceed to Choose TAP setup</p> <p>Cluster Essentials4 essentially (pun intended) boils down to two components:</p> <ul> <li>KAPP Controller5</li> <li>SecretGen Controller6</li> </ul> <p>Please install the Cluster Essentials4 according to the docs.</p> <p>Important</p> <p>While not explicitly mentioned, for each TAP minor version (e.g., 1.4, 1.5) there is an associated Cluster Essentials4.</p> <p>A minimum version of either controller is required, although there is no explicit version of either known at this point in time (April 2023).</p> <p>So unless there is a strong reason not too, use the Cluster Essentials referenced in the TAP docs.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#choose-tap-setup-profile-installation-type","title":"Choose TAP setup (profile, installation type)","text":"<p>For almost every customer, the TAP installation covers multiple environments, multiple clusters, and various profiles.</p> <p>Some customers want a Run cluster per application, others assume multi-tenancy is oke.</p> <p>It is generally recommended to assume different environments with their own purpose. For example:</p> <ul> <li>Test: contains one or more clusters, regularly re-created, used for experimentation and learning. Only used by the Platform Team.</li> <li>Staging: multiple clusters and multiple profiles, used for testing specific features and upgrades. Used by the Platform Team, and a handful of \"beta testers\".</li> <li>Production: multiple clusters and multiple profiles, including an iterate cluster for learning for end-users.</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#full-profile-for-workshop","title":"Full Profile For Workshop","text":"<p>For the sake of brevity for this workshop, we'll stick to a single cluster with the Full profile.</p> <p>While this does not represent a typical installation, it let's you go through the steps of installing and using TAP. It also shows you all the components and let's you customize and interact with them, without having to go multiple times the same process.</p> <p>The Full profile contains all the components of TAP, which is what the name implies (not always true, but this time it is).</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-type","title":"Install Type","text":"<p>Next we choose how we want to install TAP.</p> <p>There are currently three options:</p> <ol> <li>Traditional manual KAPP package install, online</li> <li>Traditional manual KAPP package install, offline</li> <li>GitOps install, beta</li> </ol> <p>It is likely that the GitOps installation type will be the default in the future.</p> <p>For now the most common (and supported) installation is the Traditional Offline install.</p> <p>It is good to understand what steps need to be taken before automating them. So this is the type we'll use for this workshop.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#version","title":"Version","text":"<p>Ideally we want to use the latest (supported) version.</p> <p>Which at this time of writing is TAP 1.5.</p> <p>TAP 1.5 requires Kubernetes 1.24, so this is currently (April 2023) not supported on TGKs based customers, as they can only go to Kubernetes 1.23.</p> <p>Assuming that in due time vSphere 8 with TGKs does support 1.24 (the Supervisor cluster already does), TAP 1.5 is a relatively safe bet.</p> <p>TAP 1.6, not yet released, will require Kubernetes 1.25, which won't be supported anytime soon with TGKs or TGKm.</p> <p>So let's stick to TAP 1.5.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#conclusion","title":"Conclusion","text":"<p>Our TAP environment will be as follows:</p> <ul> <li>TAP 1.5</li> <li>single Kubernetes cluster of 1.24</li> <li>using the Full profile</li> <li>installed via the traditional KAPP package, assuming a internet restricted environment</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-package-repository","title":"Install Package Repository","text":"<p>Now that we know what version of TAP we want to install, and how we want to install it, we install the Package Repository.</p> <p>Assumptions: * All relevant TAP packages are relocated * We have read credentials to Image Registry containing the TAP apps * We have Kubernetes cluster   * Which runs a version that TAP supports (e.g., 1.24)   * The nodes trust the CA of Image Registry</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#create-required-secrets","title":"Create Required Secrets","text":"<p>The TAP Package Repository comes from either Tanzu Network or the Registry you relocated the TAP images to.</p> <p>In either case, you need a Registry read credential in the Namespace we install the Package Repository in.</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_INSTALL_REGISTRY_SECRET=tap-registry\nexport TAP_INSTALL_REGISTRY_HOSTNAME=\nexport TAP_INSTALL_REGISTRY_USERNAME=\nexport TAP_INSTALL_REGISTRY_PASSWORD=\n</code></pre> <p>First, create the Namespace:</p> <pre><code>kubectl create namespace ${TAP_INSTALL_NAMESPACE} || true\n</code></pre> <p>And then use the Tanzu CLI to create a credential via the SecretGen Controller:</p> <pre><code>tanzu secret registry add ${TAP_INSTALL_REGISTRY_SECRET} \\\n--server    $TAP_INSTALL_REGISTRY_HOSTNAME \\\n--username  $TAP_INSTALL_REGISTRY_USERNAME \\\n--password  $TAP_INSTALL_REGISTRY_PASSWORD \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--export-to-all-namespaces \\\n--yes </code></pre> <p>Registry Write Secret</p> <p>TAP profiles <code>Iterate</code>, <code>Full</code>, and <code>Build</code>, also need a Registry write secret.</p> <p>This secret is used to write built images of the applications going throuhg the Supply Chains. Define the appropriate environment variables:</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_BUILD_REGISTRY_SECRET=registry-credentials\nexport TAP_BUILD_REGISTRY_HOSTNAME=\nexport TAP_BUILD_REGISTRY_USERNAME=\nexport TAP_BUILD_REGISTRY_PASSWORD=\n</code></pre> <p>And then we use the Tanzu CLI to create the secret:</p> <pre><code>tanzu secret registry add ${TAP_BUILD_REGISTRY_SECRET} \\\n--username ${TAP_BUILD_REGISTRY_USERNAME} \\\n--password ${TAP_BUILD_REGISTRY_PASSWORD} \\\n--server   ${TAP_BUILD_REGISTRY_HOSTNAME} \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--export-to-all-namespaces \\\n--yes\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#create-package-repository","title":"Create Package Repository","text":"<p>To make the values less abstract, let's look at an example.</p> <p>Assume we have a internal Harbor registry, with the hostname <code>harbor.example.com</code>. We create a project in this Harbor instance called <code>tap</code>, and relocated the TAP packages to this project as <code>tap-packages</code>.</p> <p>The complete URL will now be: <code>harbor.example.com/tap/tap-packages:1.5.0</code>.</p> <p>And our environment variables will be:</p> <ul> <li><code>INSTALL_REGISTRY_REPO=tap</code></li> <li><code>INSTALL_REGISTRY_HOSTNAME=harbor.example.com</code></li> <li><code>TAP_VERSION=1.5.0</code></li> </ul> <p>Set the environment variables to values appropriate for your environment.</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_VERSION=1.5.0\nexport INSTALL_REGISTRY_REPO=tap\nexport INSTALL_REGISTRY_HOSTNAME=${TAP_INSTALL_REGISTRY_HOSTNAME}\n</code></pre> <p>And use the Tanzu CLI to create the Package Repository for TAP in the TAP install Namespace (usually <code>tap-install</code>).</p> <pre><code>tanzu package repository add tanzu-tap-repository \\\n--url ${INSTALL_REGISTRY_HOSTNAME}/${INSTALL_REGISTRY_REPO}/tap-packages:${TAP_VERSION} \\\n--namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Run the command below to verify the Package Repository is reconciled successfully:</p> <pre><code>tanzu package repository list -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>This should return something like this:</p> <pre><code>  NAME                      SOURCE                                                                            STATUS\n  tanzu-tap-repository      (imgpkg) harbor.services.h2o-2-9349.h2o.vmware.com/tap/tap-packages:1.5.0         Reconcile succeeded\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#configure-certificate-authority-bundle-for-crossplane","title":"Configure Certificate Authority Bundle For Crossplane","text":"<p>The use of external services via the Bitnami Services14 feature relies on Crossplane15.</p> <p>Unfortunately, in the <code>1.5.0</code> release of TAP, the Crossplane package does not pickup the <code>shared.ca_cert_data</code> property.</p> <p>This means we must configure Crossplane ourselves. Crossplane expects a ConfigMap with a ca-bundle property13, which we later configure when installing TAP Profile.</p> <p>First, verify the <code>crossplane-system</code> namespace exists:</p> <pre><code>kubectl create namespace crossplane-system || true\n</code></pre> <p>And then create the ConfigMap with the expected name and value.</p> <pre><code>kubectl -n crossplane-system create cm ca-bundle-config \\\n--from-file=ca-bundle=ca.crt\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#review-tap-packages-configuration-options","title":"Review TAP packages configuration options","text":"<p>Before we can install our TAP Profile as desired, we need to understand how to configure it.</p> <p>We can take a look at the available packages and specifically the TAP package itself to discover what we can configure.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#explore-packages","title":"Explore Packages","text":"<p>The first step, is to discover the packages available to us.</p> <pre><code>tanzu package available list --namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>This is a large list, so we'll limit the expected output:</p> <pre><code>  NAME                                                 DISPLAY-NAME\n  accelerator.apps.tanzu.vmware.com                    Application Accelerator for VMware Tanzu\n  api-portal.tanzu.vmware.com                          API portal\n  apis.apps.tanzu.vmware.com                           API Auto Registration for VMware Tanzu\n  ...\n</code></pre> <p>To see what we can configure with TAP, we have to take a few steps:</p> <pre><code>tanzu package available get tap.tanzu.vmware.com --namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>This returns the versions of the TAP main package available to us.</p> <p>You can also get a list of available packages and their version via <code>kubectl</code>:</p> <pre><code>kubectl get package -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Which results in something like this:</p> <pre><code>NAME                                                            PACKAGEMETADATA NAME                                  VERSION                         AGE\nantrea.tanzu.vmware.com.1.7.2+vmware.1-tkg.1-advanced           antrea.tanzu.vmware.com                               1.7.2+vmware.1-tkg.1-advanced   646h52m30s\ncapabilities.tanzu.vmware.com.0.28.1+vmware.1                   capabilities.tanzu.vmware.com                         0.28.1+vmware.1                 646h52m34s\nkube-vip-cloud-provider.tanzu.vmware.com.0.0.4+vmware.2-tkg.1   kube-vip-cloud-provider.tanzu.vmware.com              0.0.4+vmware.2-tkg.1            646h52m33s\n...\n</code></pre> <p>Eitherway, we now know the version of the TAP package available to us, <code>1.5.0</code>. We need this information if we want to see the possible values for the TAP package.</p> <p>We add the version to the package name (<code>&lt;packageName&gt;/&lt;packageVersion&gt;</code>), and then add the <code>--values-schema</code> flag.</p> <pre><code>tanzu package available get tap.tanzu.vmware.com/1.5.0 \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--values-schema\n</code></pre> <p>This results in a long list of possible values:</p> <pre><code>  KEY                                                  DEFAULT                 TYPE    DESCRIPTION\n  appliveview                                                                  object  App Live View configuration\n  contour.envoy.service.type                           LoadBalancer            string  Set to LoadBalancer by default; valid values are LoadBalancer, NodePort and\n                                                                                       ClusterIP(except for contour.infrastructure_provider=vsphere)\ncrossplane                                                                   object  Crossplane configuration\n  image_policy_webhook                                                         object  Image Policy Webhook configuration\n  learningcenter                                                               object  Learning Center configuration\n  ...\n</code></pre> <p>As you can probably tell, many of these values are not explained at this level.</p> <p>For example, the <code>TYPE</code> of <code>crossplane</code> is <code>object</code>. We will have to explore the values schema of the Crossplane package to discover what we can configure here if we need to.</p> <p>There is a lot of overlap between the packages, especially with configuration options such as a Domain name, secrets, or a custom CA.</p> <p>These are conventient configurable via the <code>shared</code> configuration key:</p> <pre><code>tanzu package available get tap.tanzu.vmware.com/1.5.0 \\\n--namespace ${TAP_INSTALL_NAMESPACE} \\\n--values-schema | grep shared.\n</code></pre> <p>This gives the following list:</p> <pre><code>  shared.image_registry.password                       \"\"                      string  Optional: Password for the image registry. Mutually exclusive with\n                                                                                       shared.image_registry.secret.name/namespace.\n  shared.image_registry.project_path                   \"\"                      string  Optional: Project path in the image registry server used for builder and\n  shared.image_registry.secret.name                    \"\"                      string  Optional: Secret name for the image registry credentials of\n                                                                                       shared.image_registry.username/password.\n  shared.image_registry.secret.namespace               \"\"                      string  Optional: Secret namespace for the image registry credentials. Mutually\n                                                                                       exclusive with shared.image_registry.username/password.\n  shared.image_registry.username                       \"\"                      string  Optional: Username for the image registry. Mutually exclusive with\n                                                                                       shared.image_registry.secret.name/namespace.\n  shared.ingress_domain                                \"\"                      string  Optional: Domain name to be used in service routes and hostnames for instances\n  shared.ingress_issuer                                tap-ingress-selfsigned  string  Optional: A cert-manager.io/v1/ClusterIssuer for issuing TLS certificates to TAP\n  shared.kubernetes_distribution                       \"\"                      string  Optional: Type of K8s infrastructure being used. Can be used in coordination\n  shared.kubernetes_version                            \"\"                      string  Optional: K8s version. Can be used independently or in coordination with\n  shared.activateAppLiveViewSecureAccessControl                                bool    Optional: Enable Secure Access Connection between App Live View Components\n  shared.ca_cert_data                                  \"\"                      string  Optional: PEM Encoded certificate data to trust TLS connections with a private\n</code></pre> <p>Let's dive into the Full profile next.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#full-profile-example-from-docs","title":"Full Profile Example From Docs","text":"<p>The documentation has an annotated Full profile configuration file7 example, you can expand the example below if you want to see it.</p> <p>Let's look at how we use YTT to template the Profile configuration.</p> Full Profile Example From Docs full-profile.yaml<pre><code>shared:\ningress_domain: \"INGRESS-DOMAIN\"\ningress_issuer: # Optional, can denote a cert-manager.io/v1/ClusterIssuer of your choice. Defaults to \"tap-ingress-selfsigned\".\nimage_registry:\nproject_path: \"SERVER-NAME/REPO-NAME\"\nsecret:\nname: \"KP-DEFAULT-REPO-SECRET\"\nnamespace: \"KP-DEFAULT-REPO-SECRET-NAMESPACE\"\nkubernetes_distribution: \"K8S-DISTRO\" # Only required if the distribution is OpenShift and must be used with the following kubernetes_version key.\nkubernetes_version: \"K8S-VERSION\" # Required regardless of distribution when Kubernetes version is 1.25 or later.\nca_cert_data: | # To be passed if using custom certificates.\n-----BEGIN CERTIFICATE-----\nMIIFXzCCA0egAwIBAgIJAJYm37SFocjlMA0GCSqGSIb3DQEBDQUAMEY...\n-----END CERTIFICATE-----\nceip_policy_disclosed: FALSE-OR-TRUE-VALUE # Installation fails if this is not set to true. Not a string.\n#The above keys are minimum numbers of entries needed in tap-values.yaml to get a functioning TAP Full profile installation.\n#Below are the keys which may have default values set, but can be overridden.\nprofile: full # Can take iterate, build, run, view.\nsupply_chain: basic # Can take testing, testing_scanning.\nootb_supply_chain_basic: # Based on supply_chain set above, can be changed to ootb_supply_chain_testing, ootb_supply_chain_testing_scanning.\nregistry:\nserver: \"SERVER-NAME\" # Takes the value from the shared section by default, but can be overridden by setting a different value.\nrepository: \"REPO-NAME\" # Takes the value from the shared section by default, but can be overridden by setting a different value.\ngitops:\nssh_secret: \"SSH-SECRET-KEY\" # Takes \"\" as value by default; but can be overridden by setting a different value.\ncontour:\nenvoy:\nservice:\ntype: LoadBalancer # This is set by default, but can be overridden by setting a different value.\nbuildservice:\n# Takes the value from the shared section by default, but can be overridden by setting a different value.\nkp_default_repository: \"KP-DEFAULT-REPO\"\nkp_default_repository_secret: # Takes the value from the shared section above by default, but can be overridden by setting a different value.\nname: \"KP-DEFAULT-REPO-SECRET\"\nnamespace: \"KP-DEFAULT-REPO-SECRET-NAMESPACE\"\ntap_gui:\nservice_type: ClusterIP # If the shared.ingress_domain is set as earlier, this must be set to ClusterIP.\nmetadataStoreAutoconfiguration: true # Create a service account, the Kubernetes control plane token and the requisite app_config block to enable communications between Tanzu Application Platform GUI and SCST - Store.\napp_config:\ncatalog:\nlocations:\n- type: url\ntarget: https://GIT-CATALOG-URL/catalog-info.yaml\nmetadata_store:\nns_for_export_app_cert: \"MY-DEV-NAMESPACE\"\napp_service_type: ClusterIP # Defaults to LoadBalancer. If shared.ingress_domain is set earlier, this must be set to ClusterIP.\nscanning:\nmetadataStore:\nurl: \"\" # Configuration is moved, so set this string to empty.\ngrype:\nnamespace: \"MY-DEV-NAMESPACE\"\ntargetImagePullSecret: \"TARGET-REGISTRY-CREDENTIALS-SECRET\"\n# In a single cluster, the connection between the scanning pod and the metadata store happens inside the cluster and does not pass through ingress. This is automatically configured, you do not need to provide an ingress connection to the store.\npolicy:\ntuf_enabled: false # By default, TUF initialization and keyless verification are deactivated.\ntap_telemetry:\ncustomer_entitlement_account_number: \"CUSTOMER-ENTITLEMENT-ACCOUNT-NUMBER\" # (Optional) Identify data for creating the Tanzu Application Platform usage reports.\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#full-profile-template","title":"Full Profile Template","text":"<p>Usually you will have more than one environment and possibly more than one of the same profile in the same environment. So you end up installing the same Profile more than once.</p> <p>It is recommended to use some kind of templating.</p> <p>In the world of Tanzu (and TAP), it makes sense to use YTT.</p> full-profile.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: full\nshared:\ningress_domain: #@ dv.domainName\nca_cert_data: #@ dv.caCert\nimage_registry:\nsecret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nkp_default_repository: #@ kpRegistry\nkp_default_repository_secret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nsupply_chain: basic\nootb_supply_chain_basic:\nregistry:\nserver: #@ dv.buildRegistry\nrepository: #@ dv.buildRepo\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: #@ \"appliveview.\"+dv.domainName\nappliveview:\ningressEnabled: true\nserver:\ntls:\nenabled: false\ntap_gui:\nservice_type: ClusterIP\napp_config:\nauth:\nallowGuestAccess: true\ncustomize:\ncustom_name: 'Portal McPortalFace'\norganization:\nname: 'Org McOrg Face'\ncatalog:\nlocations:\n- type: url\ntarget: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n- type: url\ntarget: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\ncrossplane:\nregistryCaBundleConfig:\nname: ca-bundle-config\nkey: ca-bundle\n#! reduces memory and CPU requirements, not recommended for production\n#! but our Lab environments have resource restrictions\ncnrs:\nlite:\nenable: true contour:\nenvoy:\nservice:\ntype: LoadBalancer\nceip_policy_disclosed: true\nexcluded_packages:\n- scanning.apps.tanzu.vmware.com #! disabled for now, enabled when we upgrade OOTB Basic to Test &amp; Scanning\n- grype.scanning.apps.tanzu.vmware.com #! disabled for now, enabled when we upgrade OOTB Basic to Test &amp; Scanning\n- policy.apps.tanzu.vmware.com #! disabled for now, enabled when we upgrade OOTB Basic to Test &amp; Scanning\n- eventing.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n- tap-telemetry.tanzu.vmware.com.0.5.0-build #! not used, so removing to reduce memory/cpu footprint\n</code></pre> <p>Template Explained</p> <p><code>shared</code>: values that are to be shared across any package that has the same configuration option.    It is here we configure Install Registry credentials and custom CA.</p> <p><code>buildservice</code>: if your environment has no or restricted access to the internet, you need to manage the depencies yourself.   It can re-use the repository from the shared configuration. Here we do that, and tell it to exclude the depencies as we install them ourselves.</p> <p><code>supply_chain</code>: TAP can install one Cartographer Supply Chain fully*. You specify the Supply Chain by name,   and then configure the Supply Chain by its own name**, in our case <code>basic</code> and <code>ootb_supply_chain_basic</code>.    Where we override the default Registry and Repository; this is where our built images are pushed.</p> <p><code>appliveview</code>: the App Live View component let's use view registered application's resources existing across TAP cluster in the TAP GUI.   Note, this is a TAP GUI component and that is where you will find it.</p> <p><code>tap_gui</code>: the TAP GUI is the way to discover APIs, applications, accelerators and the like. It is Backstage with a few VMware Tanzu plugins included.   It has a lot of configuration options, most of which can be ignored for an initial installation.    To use TAP GUI, you have to specify an authorization provider. In the event you do not have one (ready), you set app_config.auth.allowGuestAccess to true.</p> <p><code>crossplane</code>: Crossplane is a new package for TAP 1.5, and unfortunately does not (yet) pick up the Shared CA configuration. So we have to configure this ourselves.   TAP provides external services for use with the Service Binding Specification. By default it includes Crossplane packages for several Bitnami Helm Charts (packed as Bitnami Services).</p> <p><code>ceip_policy_disclosed</code>: if you accept the policy or not. If you set this to false, you cannot install TAP</p> <p><code>excluded_packages</code>: the packages we exclude are from the Test &amp; Scanning Supply Chain. They require some configuration, especially if you are in a restricted internet access environment.   Because we are using the Basic Supply Chain, these packages are not required and we can exclude them.</p> <ul> <li>= the Test and Test &amp; Scanning Supply Chains do miss a Tekton task, but are complete beyond that</li> </ul> <p>** = each Supply Chain is its own Carvel Package part of the TAP Packages Repository</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#tanzu-build-service","title":"Tanzu Build Service","text":"<p>TAP includes a version of Tanzu Build Service (TBS). One of the things TBS needs, is a bunch of container images for the various Build Packs and the various versions of tech stacks they support.</p> <p>A full set of depencies is ~12 GB, and TBS starts synchronizing these to the nodes once it is up and running. There are two complications with this, one, thats a lot of data to synchronize from the outside, and two, you might not be able to reach the outside.</p> <p>So what we do instad, is to relocate the TBS depencies8, in the same way as we do with the TAP packages.</p> <p>We configured this in the TAP Profile already, by specifying the following:</p> <pre><code>buildservice:\nexclude_dependencies: true\n</code></pre> <p>We cannot install the TAP TBS Dependencies (a TAP specific bundle of TBS) until we install TAP. The TAP TBS Dependencies package depends on CRDs that are installed by TAP. So we will continue this later.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tap-profile","title":"Install TAP Profile","text":"<p>At this point, we have the following:</p> <ul> <li>TAP Packages available in a local Image Registry</li> <li>TAP TBS Dependencies available in a local Image Registry</li> <li>A Namespace to install TAP into (the concention is <code>tap-install</code>)</li> <li>Read access secret to the local Image Registry to retrieve the TAP packages</li> <li>Write access secret to the local Image Registry to write application build images to</li> <li>Profile Template for our Profile of choice (<code>Full</code>)</li> </ul> <p>We now take the following steps:</p> <ol> <li>Generate a Profile config file from the YTT Template</li> <li>Install the TAP package</li> <li>Install the TBS Dependencies Package Repository</li> <li>Install the TBS Dependencies Package</li> </ol>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#generate-profile-config-file","title":"Generate Profile Config FIle","text":"<p>First, we set the required variables:</p> <p>Warning</p> <p>You should have received the address of the Build Registry and the Domain specific to your cluster!</p> <pre><code>export TAP_BUILD_REGISTRY_SECRET=registry-credentials\nexport BUILD_REGISTRY_REPO=tap-apps\nexport TBS_REPO=buildservice/tbs-full-deps\nexport CA_CERT=$(cat ca.crt)\nexport BUILD_REGISTRY=\nexport DOMAIN_NAME=\n</code></pre> <p>And then we run YTT to generate our Profile configuration file.</p> <pre><code>ytt -f full-profile.ytt.yaml \\\n-v buildRegistry=\"$BUILD_REGISTRY\" \\\n-v buildRegistrySecret=\"$TAP_BUILD_REGISTRY_SECRET\" \\\n-v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n-v tbsRepo=\"$TBS_REPO\" \\\n-v domainName=\"$DOMAIN_NAME\" \\\n-v caCert=\"${CA_CERT}\" \\\n&gt; \"tap-values-full.yml\"\n</code></pre> <p>We recommend you inspect the generated file:</p> <pre><code>cat tap-values-full.yml\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tap-package","title":"Install TAP Package","text":"<p>We are now ready to install TAP.</p> <p>In case you don't remember or your environment variables are no longer set, set them to the appropriate values:</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_VERSION=1.5.0\n</code></pre> <p>We can then install TAP9 via the Tanzu CLI.</p> <pre><code>tanzu package install tap \\\n-p tap.tanzu.vmware.com \\\n-v $TAP_VERSION \\\n--values-file tap-values-full.yml \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>You can get an overview of the Packages that are installed via the Tanzu CLI:</p> <pre><code>tanzu package installed list -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Or via <code>kubectl</code>:</p> <pre><code>kubectl get app -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>If there is an issue, you can debug the package installs via <code>kubectl</code> commands:</p> <pre><code>kubectl describe app -n ${TAP_INSTALL_NAMESPACE} ${APP}\n</code></pre> <p>Carvel Tips</p> <p>Some Carvel tips worth mentioning:</p> <ol> <li>If you need to update an installation, you can run <code>tap package install</code> again. The Tanzu CLI handles the difference between Install and Update for you.</li> <li>KAPP works with asynchronous loops. This can cause the Client (Tanzu CLI) to break out of the watch loop when it detects the faillure of the previous reconcilliation. Don't be alarmed, and check with <code>kubectl get app</code> or Tanzu CLI (below)  to verify if the update succeeded.   <pre><code>tanzu package installed list\n</code></pre></li> <li>If you are sure there's nothing holding back a successful reconciliation, but it isn't updating (yet), you can force a reconcilliation with <code>kick</code>:    <pre><code>tanzu package installed kick ${APP} -n ${TAP_INSTALL_NAMESPACE}\n</code></pre></li> <li>If you delete the TAP install, and then re-install it, remember that an uninstall removes all associated Namespaces as well. So you will have to do the Crossplane prep again.</li> </ol> <p>If everything is still reconciling and you want to wait on the TAP app to succeed:</p> <pre><code>kubectl wait --for=condition=ReconcileSucceeded app \\\n-n ${TAP_INSTALL_NAMESPACE} tap \\\n--timeout=10m\n</code></pre> <p>Takes about 7-8 minutes</p> <pre><code>NAME  DESCRIPTION           SINCE-DEPLOY   AGE\ntap   Reconcile succeeded   4m10s          7m6s\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#determine-tbs-version","title":"Determine TBS Version","text":"<p>Now that we have the TAP package installed, we can install the TBS dependencies.</p> <p>First, we need to verify which version of TBS TAP shipped8.</p> <pre><code>tanzu package available list buildservice.tanzu.vmware.com \\\n--namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Which for TAP <code>1.5.0</code> gives the following result:</p> <pre><code>  NAME                           VERSION  RELEASED-AT\n  buildservice.tanzu.vmware.com  1.10.8   -\n</code></pre> <p>We then set the environment variable, so we can re-use it:</p> <pre><code>export TBS_VERSION=1.10.8\n</code></pre> <p>Note</p> <p>Your environment should already have the TBS Dependencies relocated for you.</p> <p>If you want to know how to do so, you can find it documented here8</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tbs-depencies-package-repository","title":"Install TBS Depencies Package Repository","text":"<p>In case you have not yet, set the environment variables.</p> <pre><code>export TBS_REPO=buildservice/tbs-full-deps\n</code></pre> <p>And we then we can install the TBS Dependencies Package Repository:</p> <pre><code>tanzu package repository add tbs-full-deps-repository \\\n--url ${INSTALL_REGISTRY_HOSTNAME}/${TBS_REPO}:${TBS_VERSION} \\\n--namespace ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>We can verify the status via the Tanzu CLI:</p> <pre><code>tanzu package repository list -n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>Which should now yield this:</p> <pre><code>  NAME                      SOURCE                                                                            STATUS\n  tanzu-tap-repository      (imgpkg) harbor.services.h2o-2-9349.h2o.vmware.com/tap/tap-packages:1.5.0         Reconcile succeeded\n  tbs-full-deps-repository  (imgpkg)                                                                          Reconcile succeeded\n                            harbor.services.h2o-2-9349.h2o.vmware.com/buildservice/tbs-full-deps:1.10.8\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-tbs-full-dependencies-package","title":"Install TBS Full Dependencies Package","text":"<p>We can now install the TBS Full Dependencies package8:</p> <pre><code>tanzu package install full-tbs-deps \\\n-p full-tbs-deps.tanzu.vmware.com \\\n-v ${TBS_VERSION} \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre> <p>And then verify the package is installed and reconciled correctly:</p> <pre><code>tanzu package installed get full-tbs-deps -n $TAP_INSTALL_NAMESPACE\n</code></pre> <p>Which should return something like this:</p> <pre><code>NAMESPACE:          tap-install\nNAME:               full-tbs-deps\nPACKAGE-NAME:       full-tbs-deps.tanzu.vmware.com\nPACKAGE-VERSION:    1.10.8\nSTATUS:             Reconcile succeeded\nCONDITIONS:         - type: ReconcileSucceeded\n  status: \"True\"\nreason: \"\"\nmessage: \"\"\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#install-test-application","title":"Install Test Application","text":"<p>One of the goals of TAP is to be flexible, to support the various ways people build and run applications.</p> <p>Cartographer's Supply Chains let you define any workflow you want for any kind of resource you can express in Kubernetes.</p> <p>While that is interesting, starting with the basics, we focus on the <code>batteries included</code> part. Which gives you three main workflows:</p> <ul> <li>build an application</li> <li>run an application</li> <li>both build &amp; run an application</li> </ul> <p>Each of the three Out-Of-The-Box (OOTB) Supply Chains comes with two <code>ClusterSupplyChain</code> CRs.</p> <ul> <li><code>x-image-to-url</code></li> <li><code>source-x-to-url</code></li> </ul> <p>The Source to Image journey is defined by the <code>Workload</code> CR. The Image to URL journey is defined by the <code>Deliverable</code> CR.</p> <p>When you start at the Source, the OOTB Supply Chains generate the <code>Deliverable</code> CR for you.</p> <p>To test if our TAP machinery works as intended, we'll stick to creating a <code>Workload</code> that uses the <code>source-to-url</code> Supply Chain.</p> <p>In order to the tools used by the Supply Chain to do their work, they need the appropriate Secrets and RBAC permissions. So we start with setting up a Developer Namespace.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#set-up-developer-namespace","title":"Set Up Developer Namespace","text":"<p>Starting with TAP 1.5, it includes a package called the Namespace Provisioner10.</p> <p>This let's us configure Developer Namespaces by adding Label.</p> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\n</code></pre> Via kubectlVia Manifest <pre><code>kubectl create namespace ${TAP_DEVELOPER_NAMESPACE}\nkubectl label namespace ${TAP_DEVELOPER_NAMESPACE} \\\napps.tanzu.vmware.com/tap-ns=\"\"\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt; tap-dev-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    apps.tanzu.vmware.com/tap-ns: \"\"\n    kubernetes.io/metadata.name: ${TAP_DEVELOPER_NAMESPACE}\n  name: ${TAP_DEVELOPER_NAMESPACE}\nEOF\n</code></pre> <pre><code>kubectl apply -f tap-dev-namespace.yaml\n</code></pre> <p>If we wait a few moments, we can then see the Namespace contains Secrets and RoleBindings:</p> <pre><code>kubectl get secret,rolebinding -n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>Which shows something like this:</p> <pre><code>NAME                            TYPE                             DATA   AGE\nsecret/registries-credentials   kubernetes.io/dockerconfigjson   1      25d\n\nNAME                                                            ROLE                   AGE\nrolebinding.rbac.authorization.k8s.io/default-permit-workload   ClusterRole/workload   25d\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#create-workload","title":"Create Workload","text":"<p>Now that we have a Namespace to work in, we can define a Workload.</p> <p>We can then either use the CLI or the <code>Workload</code> CR to create our test workload.</p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create tanzu-java-web-app \\\n--git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=tanzu-java-web-app \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--yes \\\n-n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: tanzu-java-web-app\n    apps.tanzu.vmware.com/workload-type: web\n  name: tanzu-java-web-app\n  namespace: ${TAP_DEVELOPER_NAMESPACE}\nspec:\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/sample-accelerators/tanzu-java-web-app.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>Use <code>kubectl wait</code> to wait for the app to be ready.</p> <pre><code>kubectl wait --for=condition=Ready Workload tanzu-java-web-app --timeout=10m -n ${$TAP_DEVELOPER_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#verify-workload","title":"Verify Workload","text":"<p>To see the logs:</p> Tanzu CLIStern <pre><code>tanzu apps workload tail tanzu-java-web-app --timestamp --since 1h \\\n-n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <pre><code>stern -n ${TAP_DEVELOPER_NAMESPACE} -l app.kubernetes.io/part-of=tanzu-java-web-app\n</code></pre> <p>To get the status:</p> Tanzu CLIKubectl <pre><code>tanzu apps workload get tanzu-java-web-app \\\n-n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <pre><code>kubectl get workload tanzu-java-web-app \\\n-n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>And to verify the Image build (kpack):</p> <pre><code>kubectl describe images.kpack.io tanzu-java-web-app\\\n-n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#call-app-endpoint","title":"Call App Endpoint","text":"<p>Collect the endpoint:</p> <pre><code>kubectl get httpproxy -n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre> <p>Which should return something like this:</p> <pre><code>NAME                                                              FQDN                                                     TLS SECRET                                       STATUS   STATUS DESCRIPTION\ntanzu-java-web-app-contour-109492d47681cc5e55d2b928e2e9ff95tanz   tanzu-java-web-app.dev.svc.cluster.local                                                                  valid    Valid HTTPProxy\ntanzu-java-web-app-contour-39f7f53b78facb4d2d2027398616ca4btanz   tanzu-java-web-app.dev.lab02.h2o-2-9349.h2o.vmware.com   dev/route-08a46009-801a-4819-9ddd-b07f4377367d   valid    Valid HTTPProxy\ntanzu-java-web-app-contour-tanzu-java-web-app.dev                 tanzu-java-web-app.dev                                                                                    valid    Valid HTTPProxy\ntanzu-java-web-app-contour-tanzu-java-web-app.dev.svc             tanzu-java-web-app.dev.svc                                                                                valid    Valid HTTPProxy\n</code></pre> <p>Copy the URL that has an FQDN with this structure: <code>&lt;app&gt;.&lt;namespace&gt;.&lt;labName&gt;.h2o-2-9349.h2o.vmware.com</code>:</p> <pre><code>export URL=\n</code></pre> <p>And then you can curl:</p> <pre><code>curl -lk \"http://${URL}\"\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#register-and-view-app-in-tap-gui","title":"Register and View App in TAP GUI","text":"<p>If we want to use the App Live View feature16, we need to register our Application in the Software Catalog17.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#access-tap-gui","title":"Access TAP GUI","text":"<p>First, let us access the TAP GUI.</p> <pre><code>kubectl get httpproxy -n tap-gui\n</code></pre> <p>Which should give you something like: <code>tap-gui.${labName}.h2o-2-9349.h2o.vmware.com</code>:</p> <pre><code>NAME      FQDN                                      TLS SECRET     STATUS   STATUS DESCRIPTION\ntap-gui   tap-gui.lab02.h2o-2-9349.h2o.vmware.com   tap-gui-cert   valid    Valid HTTPProxy\n</code></pre> <pre><code>open \"https://tap-gui.lab02.h2o-2-9349.h2o.vmware.com\"\n</code></pre> <p>Click the <code>ENTER</code> button to enter as guest. We'll look at adding authentication in another Lab.</p> <p>You should already see our demo application in the Supply Chains view.</p> <p>Tip</p> <p>The Supply Chain view is the \"Cross\" icon.</p> <p>You can also expand the left hand menu with the <code>&gt;&gt;</code> button on the top of the left hand menu.</p> <p>As we have a Full Profile, the Cluster and Target Cluster set to <code>host</code>.</p> <p>The Cluster is where the Workload resource is handled, to build and test the application, and build the container image.</p> <p>The Target Cluster, is the cluster where the Deliverable is handled, to run the application with kNative Serving.</p> <p>In a Multicluster setup, this helps you keep track of where applications are build and where they are deployed.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#register-application","title":"Register Application","text":"<p>To get a live view of our application, we have to register it in the Software Catalog17.</p> <p>We do so by going to the <code>Home</code> screen, via the house icon in the left hand menu.</p> <p>We can then add the application, by clicking the <code>REGISTER ENTITY</code> button on the right.</p> <p>Here we add a link to the <code>catalog-info.yaml</code> of the application.</p> <p>A catalog describes the application, its owner, and most importantly, it includes information for the TAP GUI on how to find its resources. This is done with an annotation: </p> <pre><code>  annotations:\n'backstage.io/kubernetes-label-selector': 'app.kubernetes.io/part-of=tanzu-java-web-app'\n</code></pre> <p>The Catalog file looks as follows:</p> <pre><code>apiVersion: backstage.io/v1alpha1\nkind: Component\nmetadata:\nname: tanzu-java-web-app\ndescription: Tanzu Java Web App\ntags:\n- app-accelerator\n- java\n- spring\n- web\n- tanzu\nannotations:\n'backstage.io/kubernetes-label-selector': 'app.kubernetes.io/part-of=tanzu-java-web-app'\nspec:\ntype: service\nlifecycle: experimental\nowner: default-team\n</code></pre> <p>The link to add:</p> <pre><code>https://github.com/sample-accelerators/tanzu-java-web-app/blob/main/catalog/catalog-info.yaml\n</code></pre> <p>GitHub Is Trusted, Gitea is not!</p> <p>The demo application we just used and its catalog live in GitHub.</p> <p>GitHub has a proper certificate and is already trusted as a source.</p> <p>For other Git servers, such as our Gitea server, we will need to configure the TAP GUI to trust it.</p> <p>We do so in another Lab.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#view-application-resources","title":"View Application Resources","text":"<p>Go back to the Home screen of the TAP GUI.</p> <p>Here you should now see an entry for our application <code>tanzu-java-web-app</code>.</p> <p>If you click on the name, you see the resources the TAP GUI can find in the clusters it has access to.</p> <p>In our case, that is the same cluster, but in a Multicluster setup, it depends on which clusters TAP GUI has read access to.</p> <p>To see the live resources, click on <code>Runtime Resources</code>.</p>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#cleanup","title":"Cleanup","text":"","tags":["tap","kubernetes","install"]},{"location":"install/basic/#delete-workload","title":"Delete Workload","text":"<p>And then we can delete our test workload if want to.</p> <pre><code>tanzu apps workload delete tanzu-java-web-app -y -n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"install/basic/#links","title":"Links","text":"<ol> <li> <p>TAP 1.5 - Prerequisites \u21a9</p> </li> <li> <p>TAP 1.5 - Supported Kubernetes versions \u21a9</p> </li> <li> <p>TAP 1.5 - Relocate images to a registry \u21a9</p> </li> <li> <p>TAP 1.5 - Cluster Essentials \u21a9\u21a9\u21a9</p> </li> <li> <p>KAPP Controller - Overview \u21a9</p> </li> <li> <p>SecretGen Controller - GitHub page \u21a9</p> </li> <li> <p>TAP 1.5 - Full profile example \u21a9</p> </li> <li> <p>TAP 1.5 - Handle TBS Depencies \u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>TAP 1.5 - Install TAP Package \u21a9</p> </li> <li> <p>TAP 1.5 - Namespace Provisioner \u21a9</p> </li> <li> <p>TAP 1.5 - Trust CA for Workload specifically \u21a9</p> </li> <li> <p>KAPP Controller - Configure Controller to Trust custom CA \u21a9</p> </li> <li> <p>Crossplane - Configure Self-signed CA Certs support \u21a9</p> </li> <li> <p>TAP 1.5 - Bitnami Services \u21a9</p> </li> <li> <p>Crossplane - The Cloud Native Control Plane Framework \u21a9</p> </li> <li> <p>Tanzu Application Platform - App Live View \u21a9</p> </li> <li> <p>Backstage - Software Catalog \u21a9\u21a9</p> </li> </ol>","tags":["tap","kubernetes","install"]},{"location":"install/gitops/","title":"TAP GitOps Install","text":"","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#goal-outcome-checks","title":"Goal, Outcome &amp; Checks","text":"<p>Goal: Complete an installation of TAP for a Customer with GitOps and namespace management ready for production usage as an Operator</p> <p>Outcome:  TAP install and updates is controlled via chages in Git only</p> <ul> <li> I am able to manage a TAP installation with GitOPs</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#raw-notes","title":"Raw Notes","text":"<ul> <li>h2o -&gt; so we use SOPS</li> <li>Prerequisites</li> <li>Accept EULAs</li> <li>Install Cluster Essentials</li> <li>Install TAP with GitOps (SOPS)</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#install-cluster-essentials","title":"Install Cluster Essentials","text":"<ul> <li>https://docs.vmware.com/en/Cluster-Essentials-for-VMware-Tanzu/1.5/cluster-essentials/deploy.html</li> <li>Login to Tanzu Network</li> <li>Download Cluster Essentials</li> <li>Upload to Jump Host</li> <li>Create Namespace</li> <li>Create ConfigMap for CA</li> </ul> <pre><code>kubectl create namespace kapp-controller\n</code></pre> <pre><code>kubectl create secret generic kapp-controller-config \\\n--namespace kapp-controller \\\n--from-file caCerts=ssl/ca.crt\n</code></pre> <pre><code>mkdir $HOME/tanzu-cluster-essentials\ntar -xvf $HOME/scripts/scripts/tanzu-cluster-essentials-linux-amd64-1.5.0.tgz -C $HOME/tanzu-cluster-essentials\n</code></pre> <pre><code>export INSTALL_BUNDLE=registry.tanzu.vmware.com/tanzu-cluster-essentials/cluster-essentials-bundle@sha256:79abddbc3b49b44fc368fede0dab93c266ff7c1fe305e2d555ed52d00361b446\nexport INSTALL_REGISTRY_HOSTNAME=registry.tanzu.vmware.com\nexport INSTALL_REGISTRY_USERNAME=TANZU-NET-USER\nexport INSTALL_REGISTRY_PASSWORD=TANZU-NET-PASSWORD\n</code></pre> <pre><code>cd $HOME/tanzu-cluster-essentials\n</code></pre> <pre><code>./install.sh --yes\n</code></pre> <p>NOTE: looks like my cluster already has the essentials installed -&gt; part of TGKm?</p>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#gitops-install","title":"GitOps Install","text":"<ul> <li>install Age -&gt; https://github.com/FiloSottile/age#installation</li> <li>create new repo in Gitea -&gt; <code>gitops-iterate-01</code></li> <li>sync it</li> <li>download Reference Implementation</li> <li>ingest reference implmentation (script)</li> <li>run cluster config init script</li> <li>update git repo</li> <li></li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#init-repo","title":"Init Repo","text":"<pre><code>mkdir -p $HOME/projects/gitops-iterate-01\ncd $HOME/projects/gitops-iterate-01\ngit config --global init.defaultBranch main\n</code></pre> <pre><code>touch README.md\ngit init\ngit checkout -b main\ngit add README.md\ngit commit -m \"first commit\"\ngit remote add origin https://gitea.services.h2o-2-9349.h2o.vmware.com/gitea/gitops-iterate-01.git\ngit push -u origin main\n</code></pre> <pre><code>scp ~/Downloads/tanzu-gitops-ri-0.1.0.tgz ubuntu@10.220.13.174:/home/ubuntu\n</code></pre> <pre><code>tar xvf $HOME/tanzu-gitops-ri-0.1.0.tgz  -C $HOME/projects/gitops-iterate-01\n</code></pre> <pre><code>./setup-repo.sh iterate sops\n</code></pre> <pre><code>git add . &amp;&amp; git commit -m \"Add iterate\"\ngit push -u origin\n</code></pre> <pre><code>cd clusters/iterate\nless docs/README.md\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#create-sops-key","title":"Create SOPS Key","text":"<pre><code>mkdir -p $HOME/tmp-enc\nchmod 700 $HOME/tmp-enc\ncd $HOME/tmp-enc\n</code></pre> <pre><code>age-keygen -o key.txt\n</code></pre> <ul> <li>create <code>tap-sensitive-values.yaml</code></li> </ul> <pre><code>---\ntap_install:\nsensitive_values:\n</code></pre> <pre><code>export SOPS_AGE_RECIPIENTS=$(cat key.txt | grep \"# public key: \" | sed 's/# public key: //')\n</code></pre> <pre><code>sops --encrypt tap-sensitive-values.yaml &gt; tap-sensitive-values.sops.yaml\n</code></pre> <pre><code>export SOPS_AGE_KEY_FILE=key.txt\n</code></pre> <pre><code>sops --decrypt tap-sensitive-values.sops.yaml\n</code></pre> <pre><code>cp tap-sensitive-values.sops.yaml $HOME/projects/gitops-iterate-01/clusters/iterate/cluster-config/values/\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#prepare-values","title":"Prepare Values","text":"<ul> <li>use iterate: https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/multicluster-reference-tap-values-iterate-sample.html</li> <li>create <code>tap-non-sensitive-values.yaml</code><ul> <li>at <code>&lt;GIT-REPO-ROOT&gt;/clusters/&lt;CLUSTER-NAME&gt;/cluster-config/values/tap-non-sensitive-values.yaml</code></li> <li><code>vim clusters/iterate/cluster-config/values/tap-non-sensitive-values.yaml</code></li> </ul> </li> </ul> <pre><code>tap_install:\nvalues:\nceip_policy_disclosed: true\nexcluded_packages:\n- policy.apps.tanzu.vmware.com\n</code></pre> <pre><code>tap_install:\nvalues:\nprofile: iterate\nceip_policy_disclosed: true\nshared:\ningress_domain: \"iterate.h2o-2-9349.h2o.vmware.com\"\nca_cert_data: | # To be passed if using custom certificates\n-----BEGIN CERTIFICATE-----\nMIID7jCCAtagAwIBAgIURv5DzXSDklERFu4gL2sQBNeRg+owDQYJKoZIhvcNAQEL\nBQAwgY4xCzAJBgNVBAYTAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAO\nBgNVBAcTB1V0cmVjaHQxFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMU\nS2Vhcm9zIFRhbnp1IFJvb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290\nIENBMB4XDTIyMDMyMzE1MzUwMFoXDTI3MDMyMjE1MzUwMFowgY4xCzAJBgNVBAYT\nAk5MMRgwFgYDVQQIEw9UaGUgTmV0aGVybGFuZHMxEDAOBgNVBAcTB1V0cmVjaHQx\nFTATBgNVBAoTDEtlYXJvcyBUYW56dTEdMBsGA1UECxMUS2Vhcm9zIFRhbnp1IFJv\nb3QgQ0ExHTAbBgNVBAMTFEtlYXJvcyBUYW56dSBSb290IENBMIIBIjANBgkqhkiG\n9w0BAQEFAAOCAQ8AMIIBCgKCAQEAyZXDL9W2vu365m//E/w8n1M189a5mI9HcTYa\n0xZhnup58Zp72PsgzujI/fQe43JEeC+aIOcmsoDaQ/uqRi8p8phU5/poxKCbe9SM\nf1OflLD9k2dwte6OV5kcSUbVOgScKL1wGEo5mdOiTFrEp5aLBUcbUeJMYz2IqLVa\nv52H0vTzGfmrfSm/PQb+5qnCE5D88DREqKtWdWW2bCW0HhxVHk6XX/FKD2Z0FHWI\nChejeaiarXqWBI94BANbOAOmlhjjyJekT5hL1gh7BuCLbiE+A53kWnXO6Xb/eyuJ\nobr+uHLJldoJq7SFyvxrDd/8LAJD4XMCEz+3gWjYDXMH7GfPWwIDAQABo0IwQDAO\nBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUfGU50Pe9\nYTv5SFvGVOz6R7ddPcUwDQYJKoZIhvcNAQELBQADggEBAHMoNDxy9/kL4nW0Bhc5\nGn0mD8xqt+qpLGgChlsMPNR0xPW04YDotm+GmZHZg1t6vE8WPKsktcuv76d+hX4A\nuhXXGS9D0FeC6I6j6dOIW7Sbd3iAQQopwICYFL9EFA+QAINeY/Y99Lf3B11JfLU8\njN9uGHKFI0FVwHX428ObVrDi3+OCNewQ3fLmrRQe6F6q2OU899huCg+eYECWvxZR\na3SlVZmYnefbA87jI2FRHUPqxp4P2mDwj/RZxhgIobhw0zz08sqC6DW0Aj1OIJe5\nsDAm0uiUdqs7FZN2uKkLKekdTgW0QkTFEJTk5Yk9t/hOrjnHoWQfB+mLhO3vPhip\nvhs=\n-----END CERTIFICATE-----\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nkp_default_repository: \"harbor.services.h2o-2-9349.h2o.vmware.com/buildservice/tbs-full-deps\"\nsupply_chain: basic\nootb_supply_chain_basic:\nregistry:\nserver: harbor.services.h2o-2-9349.h2o.vmware.com\nrepository: tap-apps\nimage_policy_webhook:\nallow_unmatched_tags: true\ncontour:\nenvoy:\nservice:\ntype: LoadBalancer cnrs:\ndomain_name: \"iterate.h2o-2-9349.h2o.vmware.com\"\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: appliveview.view.h2o-2-9349.h2o.vmware.com\nexcluded_packages:\n- policy.apps.tanzu.vmware.com\n- scanning.apps.tanzu.vmware.com\n- grype.scanning.apps.tanzu.vmware.com\n</code></pre> <pre><code>---\ntap_install:\nsensitive_values:\nshared:\nimage_registry:\nproject_path: \"harbor.services.h2o-2-9349.h2o.vmware.com/tap\"\nusername: \"\"\npassword: ''\nbuildservice:\nkp_default_repository_username: \"\"\nkp_default_repository_password: ''\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#setup-sync","title":"Setup Sync","text":"","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#git-ssh","title":"Git SSH","text":"<pre><code>ssh-keyscan gitssh.h2o-2-9349.h2o.vmware.com &gt; gitea-known-hosts.txt\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#sync-script","title":"Sync Script","text":"<pre><code>export INSTALL_REGISTRY_HOSTNAME=harbor.services.h2o-2-9349.h2o.vmware.com\nexport INSTALL_REGISTRY_USERNAME=admin\nexport INSTALL_REGISTRY_PASSWORD='VMware123!'\nexport GIT_SSH_PRIVATE_KEY=$(cat $HOME/.ssh/id_gitea)\nexport GIT_KNOWN_HOSTS=$(ssh-keyscan 1)\nexport SOPS_AGE_KEY=$(cat $HOME/tmp-enc/key.txt)\nexport TAP_PKGR_REPO=harbor.services.h2o-2-9349.h2o.vmware.com/tap/tap-packages\n</code></pre> <pre><code>./tanzu-sync/scripts/configure.sh\n</code></pre> <pre><code>git add cluster-config/ tanzu-sync/\ngit commit -m \"Configure install of TAP 1.5.0\"\ngit push\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#deploy-script","title":"Deploy Script","text":"<p>Warning</p> <p>Run this with your Kubernetes context set to your target cluster!</p> <pre><code>kubectx tap-iterate-admin@tap-iterate\n</code></pre> <pre><code>./tanzu-sync/scripts/deploy.sh\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#setup-developer-namespaces","title":"Setup Developer Namespaces","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/install-gitops-set-up-namespaces.html</li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-about.html</li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-customize-installation.html</li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-customize-installation.html#git-install</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#define-gitops-for-namespace-provisioner","title":"Define GitOps For Namespace Provisioner","text":"<ul> <li>create <code>desired-namespaces.yaml</code></li> <li>create <code>namespaces.yaml</code></li> </ul> desired-namespaces.yaml<pre><code>#@data/values\n---\nnamespaces:\n- name: dev\n- name: qa\n</code></pre> namespaces.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#! This loop will now loop over the namespace list in\n#! in ns.yaml and will create those namespaces.\n#@ for ns in data.values.namespaces:\n---\napiVersion: v1\nkind: Namespace\nmetadata:\nname: #@ ns.name\n#@ end\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#configure-namespace-provisioner-in-tap-install-values","title":"Configure Namespace Provisioner in TAP Install Values","text":"<ul> <li>update <code>cluster-config/values/tap-non-sensitive-values.yaml</code></li> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/namespace-provisioner-use-case3.html#git-private</li> </ul> <pre><code>tap_install:\nvalues:\n...\nnamespace_provisioner:\ncontroller: false\ngitops_install:\nref: origin/main\nsubPath: clusters/iterate/dev-namespaces\nurl: git@gitssh.h2o-2-9349.h2o.vmware.com:gitea/gitops-iterate-01.git\nsecretRef:\nname: sync-git-ssh\nnamespace: tanzu-sync\ncreate_export: true\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#tanzu-build-service-dependencies","title":"Tanzu Build Service Dependencies","text":"<ul> <li>Inspired by VRabbi: https://vrabbi.cloud/post/tap-1-5-gitops-installation/</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#add-tbs-dependencies-package-and-repository","title":"Add TBS Dependencies Package and Repository","text":"<ul> <li>create <code>cluster-config/config/tbs-install/package-repository.yaml</code></li> <li>create <code>cluster-config/config/tbs-install/package-install.yaml</code></li> </ul> package-repository.yaml<pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\nname: tbs-full-deps-repository\nnamespace: tap-install\nannotations:\nkapp.k14s.io/change-group: pkgr\nspec:\nfetch:\nimgpkgBundle:\nimage: harbor.services.h2o-2-9349.h2o.vmware.com/buildservice/tbs-full-deps:1.10.8\n</code></pre> package-install.yaml<pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\nname: full-tbs-deps\nnamespace: tap-install\nannotations:\nkapp.k14s.io/change-group: tbs\nkapp.k14s.io/change-rule.0: \"upsert after upserting pkgi\"\nkapp.k14s.io/change-rule.1: \"delete before deleting pkgi\"\nspec:\nserviceAccountName: tap-installer-sa\npackageRef:\nrefName: full-tbs-deps.tanzu.vmware.com\nversionSelection:\nconstraints: 1.10.8\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#test-with-workload","title":"Test With Workload","text":"<p>Now that we have a Namespace to work in, we can define a Workload.</p> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#create-workload","title":"Create Workload","text":"<p>We can then either use the CLI or the <code>Workload</code> CR to create our test workload.</p> Tanzu CLIKubernetes Manifest <pre><code>tanzu apps workload create smoke-app \\\n--git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=smoke-app \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--yes \\\n-n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre> <pre><code>echo \"apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  labels:\n    app.kubernetes.io/part-of: smoke-app\n    apps.tanzu.vmware.com/workload-type: web\n  name: smoke-app\n  namespace: ${TAP_DEVELOPER_NAMESPACE}\nspec:\n  params:\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \\\"1\\\"\n  source:\n    git:\n      ref:\n        branch: main\n      url: https://github.com/sample-accelerators/tanzu-java-web-app.git\n\" &gt; workload.yml\n</code></pre> <pre><code>kubectl apply -f workload.yml\n</code></pre> <p>To verify the status of FluxCD checkout:</p> <pre><code>kubectl get gitrepo -A\n</code></pre> <p>Use <code>kubectl wait</code> to wait for the app to be ready.</p> <pre><code>kubectl wait --for=condition=Ready Workload smoke-app --timeout=10m -n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#verify-workload","title":"Verify Workload","text":"<p>To see the logs:</p> <pre><code>tanzu apps workload tail smoke-app\n</code></pre> <p>To get the status:</p> <pre><code>tanzu apps workload get smoke-app\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#delete-workload","title":"Delete Workload","text":"<p>And then we can delete our test workload if want to.</p> <pre><code>tanzu apps workload delete smoke-app -y -n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/gitops/#links","title":"Links","text":"<ul> <li>Official Docs</li> <li>VRabbi Blog</li> <li>Mozilla SOPS</li> </ul>","tags":["tap","kubernetes","install","GitOps"]},{"location":"install/mutlicluster/","title":"Multicluster Install","text":"<p>TODO</p>","tags":["tap","kubernetes","install"]},{"location":"install/mutlicluster/#goal-outcome-checks","title":"Goal, Outcome &amp; Checks","text":"<p>Goal: Complete a multi-cluster installation of TAP for a Customer with GitOps and namespace management ready for production usage as an Operator</p> <p>Outcome:  I can do a full multi-cluster installation of TAP that is managed by GitOps</p> <ul> <li> I understand and have implemented the TAP Multi Cluster Reference Architecture</li> <li> Configure TAP GUI to read resources from multiple clusters</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"install/tap-gui-auth/","title":"TAP GUI add Auth","text":"<p>TODO</p>","tags":["tap","kubernetes","install"]},{"location":"jumphost/","title":"Jumphost Required Tools","text":"<ul> <li>needs:</li> <li>4 cpu</li> <li>8gb ram</li> <li>100gb disk</li> <li>download Ubuntu Image from Cloud Images</li> <li>https://cloud-images.ubuntu.com/jammy/20230401/</li> <li>create VM in user-workload network from ubuntu image</li> <li>paste personal SSH pub key</li> <li>add it to additional network</li> <li>set up IP</li> <li><code>ip a</code></li> <li><code>sudo netplan set ethernets.ens224.dhcp4=true</code></li> <li><code>sudo netplan apply</code></li> <li><code>ip a</code></li> <li>remove default route to alt network</li> <li><code>sudo ip r delete default via 172.16.50.1 dev ens224</code></li> <li>https://matteo-magni.github.io/tanzugym/tkgm/bastion/</li> <li>https://cloud-images.ubuntu.com/jammy/20230401/</li> <li>tools:</li> <li>go lang</li> <li>python 3?</li> <li>net-tools</li> <li>git</li> <li>yq</li> <li>jq</li> <li>govc</li> <li>kubectl</li> <li>kubectx</li> <li>kubens</li> <li>Tanzu CLI<ul> <li>TKG Plugins</li> <li>TAP Plugins</li> </ul> </li> <li>Carvel toolsuite</li> <li>curl (&amp; httpie)</li> <li>Docker</li> <li>Kind</li> <li>cfssl</li> <li>minio client</li> <li>Cosign</li> <li>age</li> <li>sops</li> <li>optional tools:<ul> <li>java</li> <li>maven</li> </ul> </li> </ul>"},{"location":"jumphost/#kind-cluster","title":"Kind Cluster","text":"<pre><code>kind create cluster --image kindest/node:v1.24.12@sha256:1e12918b8bc3d4253bc08f640a231bb0d3b2c5a9b28aa3f2ca1aee93e1e8db16 --name tkgm\n</code></pre> <pre><code>tanzu management-cluster create h2o-9349-01  --file tkgm-management-kubevip.yaml  --use-existing-bootstrap-cluster tkgm -v 6\n</code></pre>"},{"location":"jumphost/#age","title":"Age","text":"<ul> <li>https://github.com/FiloSottile/age#installation</li> </ul> <pre><code>sudo apt install age\n</code></pre>"},{"location":"jumphost/#sops","title":"Sops","text":"<ul> <li>https://github.com/mozilla/sops/releases</li> </ul> <pre><code>http --download https://github.com/mozilla/sops/releases/download/v3.7.3/sops_3.7.3_amd64.deb\nsudo apt install ./sops_3.7.3_amd64.deb\n</code></pre>"},{"location":"jumphost/#git","title":"Git","text":"<pre><code>sudo apt install net-tools git\n</code></pre> <pre><code>LOCALBIN=\"/home/ubuntu/.local/bin\"\nmkdir -p $LOCALBIN\nexport PATH=$PATH:$LOCALBIN\n</code></pre>"},{"location":"jumphost/#minio","title":"MinIO","text":"<ul> <li>MinIO: https://min.io/docs/minio/linux/reference/minio-mc.html</li> </ul> <pre><code>curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n--create-dirs \\\n-o $HOME/minio-binaries/mc\n\nchmod +x $HOME/minio-binaries/mc\nexport PATH=$PATH:$HOME/minio-binaries/\n\nmc --help\n</code></pre>"},{"location":"jumphost/#kubectl","title":"Kubectl","text":"<pre><code>curl -sSfLO \"https://dl.k8s.io/release/$(curl -sSfL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\ninstall kubectl ${LOCALBIN}/kubectl\n</code></pre>"},{"location":"jumphost/#carvel","title":"Carvel","text":"<pre><code>sudo chmod -R 777 /usr/local/bin/\nwget -O- https://carvel.dev/install.sh | bash\n</code></pre>"},{"location":"jumphost/#vcc","title":"VCC","text":"<pre><code>export VCC_USER=\"\"\nexport VCC_PASS='\n</code></pre> <pre><code># list all the available products\nvcc get products\n\n# list all the available subproducts belonging to the vmware_tanzu_kubernetes_grid product\nvcc get subproducts -p vmware_tanzu_kubernetes_grid\n\n# list all the versions for the subproduct tkg\nvcc get versions -p vmware_tanzu_kubernetes_grid -s tkg\n\n# list all the files available for version 2.1.0\n# at this point vcc does the actual login\nvcc get files -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.1\n\n# download the tanzu cli\nvcc download -p vmware_tanzu_kubernetes_grid -s tkg -v 2.1.1 -f 'tanzu-cli-bundle-linux-amd64.tar.gz' --accepteula\n</code></pre> <pre><code>tar xvzf vcc-downloads/tanzu-cli-bundle-linux-amd64.tar.gz\ninstall cli/core/v0.28.1/tanzu-core-linux_amd64 ${LOCALBIN}/tanzu\n</code></pre>"},{"location":"jumphost/#kind","title":"Kind","text":"<pre><code>curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n</code></pre>"},{"location":"jumphost/#tanzu-cli","title":"Tanzu CLI","text":"<ul> <li>Download Tanzu CLI</li> <li>login to Tanzu Network: https://network.tanzu.vmware.com/</li> <li>download CLI from TAP Download Page: https://network.tanzu.vmware.com/products/tanzu-application-platform</li> </ul> <pre><code>scp tanzu-framework-linux-amd64-v0.28.1.1.tar ubuntu@10.220.13.174:/home/ubuntu/\n</code></pre> <pre><code>mkdir -p $HOME/tanzu\ntar -xvf tanzu-framework-linux-amd64-v0.28.1.1.tar -C $HOME/tanzu\n</code></pre> <pre><code>cd $HOME/tanzu\nexport VERSION=v0.28.1.1.\n# sudo install cli/core/$VERSION/tanzu-core-linux_amd64 /usr/local/bin/tanzu\ntanzu plugin install --local cli all\n</code></pre> <pre><code>tanzu plugin list\n</code></pre> <p>This should now list both TKG and TAP plugins:</p> <pre><code>Standalone Plugins\n  NAME                DESCRIPTION                                                        TARGET      DISCOVERY  VERSION        STATUS\n  accelerator         Manage accelerators in a Kubernetes cluster                                               v1.5.0         installed\n  apps                Applications on Kubernetes                                                                v0.11.1        installed\n  external-secrets    interacts with external-secrets.io resources                                              v0.1.0-beta.4  installed\n  insight             post &amp; query image, package, source, and vulnerability data                               v1.5.0         installed\n  isolated-cluster    isolated-cluster operations                                                    default    v0.28.1        installed\n  login               Login to the platform                                                          default    v0.28.1        installed\n  pinniped-auth       Pinniped authentication operations (usually not directly invoked)              default    v0.28.1        installed\n  services            Commands for working with service instances, classes and claims                           v0.6.0         installed\n  management-cluster  Kubernetes management-cluster operations                           kubernetes  default    v0.28.1        installed\n  package             Tanzu package management                                           kubernetes  default    v0.28.1        installed\n  secret              Tanzu secret management                                            kubernetes  default    v0.28.1        installed\n  telemetry           Configure cluster-wide telemetry settings                          kubernetes  default    v0.28.1        installed\n\nPlugins from Context:  tkgm-m\n  NAME                DESCRIPTION                           TARGET      VERSION  STATUS\n  cluster             Kubernetes cluster operations         kubernetes  v0.28.1  installed\n  feature             Operate on features and featuregates  kubernetes  v0.28.1  installed\n  kubernetes-release  Kubernetes release op erations         kubernetes  v0.28.1  installed\n</code></pre>"},{"location":"jumphost/#cosign","title":"Cosign","text":"<ul> <li>https://docs.sigstore.dev/cosign/installation/</li> </ul> <pre><code>wget \"https://github.com/sigstore/cosign/releases/download/v2.0.0/cosign-linux-amd64\"\nmv cosign-linux-amd64 /usr/local/bin/cosign\nchmod +x /usr/local/bin/cosign\n</code></pre>"},{"location":"supply-chain/basic-to-test-scan/","title":"Test & Scan Supply Chain","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/getting-started-add-test-and-security.html</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#what-you-will-do","title":"What you will do","text":"<ul> <li>Install OOTB Supply Chain with Testing.</li> <li>Add a Tekton pipeline to the cluster and update the workload to point to the pipeline and resolve errors.</li> <li>Install OOTB Supply Chain with Testing and Scanning.</li> <li>Update the workload to point to the Tekton pipeline and resolve errors.</li> <li>Query for vulnerabilities and dependencies</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#steps","title":"Steps","text":"<ul> <li>Scanning Pre-requisites</li> <li>Update TAP Profile Config file</li> <li>Add Tekton Pipeline</li> <li>Add ScanPolicy</li> <li>Update Workload</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#scanning-prerequisites","title":"Scanning Prerequisites","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/tap-gui-plugins-scc-tap-gui.html#scan</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#update-profile-config","title":"Update Profile Config","text":"<ul> <li>Supply Chain</li> <li>TAP GUI Config</li> <li>enable Test and Scaning components</li> </ul> <pre><code>- supply_chain: testing\n+ supply_chain: testing_scanning\n- ootb_supply_chain_testing:\n+ ootb_supply_chain_testing_scanning:\n   registry:\n      server: \"&lt;SERVER-NAME&gt;\"\n      repository: \"&lt;REPO-NAME&gt;\"\n</code></pre> <p>One of the key components of the Scanning and Testing Supply Chain, is Grype1.</p> <p>It cross references packages from SBOM files with CVE databases.</p> <p>In restricted environments, Grype cannot retrieve these databases. So we bring it to Grype instead.</p> <p>If you want to learn how you can relocate the Grype database, read this2.</p> Grype Database Relocation/Update Script <pre><code>#!/bin/bash\nset -euo pipefail\npushd $(dirname $0)\nMINIO_HOSTNAME=${MINIO_HOSTNAME:-\"localhost\"}\necho \"&gt; Removing existing listing.json\"\nrm listing.json || true\necho \"&gt; Downloading new listing.json from Grype's databases repo\"\nhttp --download https://toolbox-data.anchore.io/grype/databases/listing.json\n\necho \"&gt; Stripping listing to latest file only\"\ncp listing.json listing_original.json\necho '{\"available\": {\"5\": [' &gt; listing_tmp.json\ncat listing_original.json | jq '.available.\"5\"[0]' &gt;&gt; listing_tmp.json\necho ']}}' &gt;&gt; listing_tmp.json\ncat listing_tmp.json | jq &gt; listing.json\n\necho \"&gt; Generate download script\"\necho \"#!/bin/bash\" &gt; grype_down.sh\necho \"set -euo pipefail\" &gt;&gt; grype_down.sh\n\ncat listing.json |jq -r '.available[] | values[].url' \\\n| awk '{print \"http --download \" $1}' &gt;&gt; grype_down.sh\nchmod +x grype_down.sh\n\necho \"&gt; Removing existing database files\"\nrm *.tar.gz || true\necho \"&gt; Downloading new database files\"\n./grype_down.sh\n\necho \"&gt; Uploading database files to MinIO\"\nmc cp *.tar.gz minio_h20/grype/databases/\n\necho \"&gt; Update listing file\"\ncp listing.json listing_copy.json\nsed -i -e \\\n\"s/https:\\/\\/toolbox-data.anchore.io\\/grype/https:\\/\\/$MINIO_HOSTNAME\\/grype/g\" \\\nlisting.json\n\necho \"&gt; Upload updated listing file\"\nmc cp listing.json minio_h20/grype/databases/\n\necho \"&gt; View folder in MinIO\"\nmc ls minio_h20/grype/databases/\n</code></pre> <p>To shortcut the configuration, we've already configured an \"offline\" storage of the Grype database. To use it, we use the snippet below.</p> <pre><code>grype:\ndb:\ndbUpdateUrl: https://minio.services.h2o-2-9349.h2o.vmware.com/grype/databases/listing.json\n</code></pre> <p>TAP GUI config for Gitea</p> <p>If you skipped the Hello World Workload Lab, you're missing the Gitea config for the TAP GUI.</p> <p>Which looks as follows:</p> <pre><code>tap_gui:\napp_config:\nbackend:\nreading:\nallow:\n- host: #@ dv.gitServer\nintegrations:\ngitea:\n- host: #@ dv.gitServer\nusername: #@ dv.gitUser\npassword: #@ dv.gitPassword\n</code></pre> <p>For completeness, we assume you want this configuration regardless.</p>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#updated-profile-template","title":"Updated Profile Template","text":"full-profile.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: full\nshared:\ningress_domain: #@ dv.domainName\nca_cert_data: #@ dv.caCert\nimage_registry:\nsecret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nkp_default_repository: #@ kpRegistry\nkp_default_repository_secret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nsupply_chain: testing_scanning\nootb_supply_chain_testing_scanning:\nregistry:\nserver: #@ dv.buildRegistry\nrepository: #@ dv.buildRepo\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: #@ \"appliveview.\"+dv.domainName\nappliveview:\ningressEnabled: true\nserver:\ntls:\nenabled: false\ntap_gui:\nservice_type: ClusterIP\napp_config:\nbackend:\nreading:\nallow:\n- host: #@ dv.gitServer\nintegrations:\ngitea:\n- host: #@ dv.gitServer\nusername: #@ dv.gitUser\npassword: #@ dv.gitPassword\nauth:\nallowGuestAccess: true\ncustomize:\ncustom_name: 'Portal McPortalFace'\norganization:\nname: 'Org McOrg Face'\ncatalog:\nlocations:\n- type: url\ntarget: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n- type: url\ntarget: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\ncrossplane:\nregistryCaBundleConfig:\nname: ca-bundle-config\nkey: ca-bundle\n#! reduces memory and CPU requirements, not recommended for production\n#! but our Lab environments have resource restrictions\ncnrs:\nlite:\nenable: true contour:\nenvoy:\nservice:\ntype: LoadBalancer\ngrype:\ndb:\ndbUpdateUrl: https://minio.services.h2o-2-9349.h2o.vmware.com/grype/databases/listing.json\nceip_policy_disclosed: true\nexcluded_packages:\n- eventing.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n- tap-telemetry.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#generate-updated-profile","title":"Generate updated profile","text":"<pre><code>export GIT_SERVER=gitea.services.h2o-2-9349.h2o.vmware.com\nexport GIT_USER=gitea\nexport GIT_PASSWORD='VMware123!'\nexport TAP_BUILD_REGISTRY_SECRET=registry-credentials\nexport BUILD_REGISTRY_REPO=tap-apps\nexport TBS_REPO=buildservice/tbs-full-deps\nexport CA_CERT=$(cat ca.crt)\nexport BUILD_REGISTRY=\nexport DOMAIN_NAME=\n</code></pre> <p>And then we run YTT to generate our Profile configuration file.</p> <pre><code>ytt -f full-profile.ytt.yaml \\\n-v buildRegistry=\"$BUILD_REGISTRY\" \\\n-v buildRegistrySecret=\"$BUILD_REGISTRY_SECRET\" \\\n-v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n-v tbsRepo=\"$TBS_REPO\" \\\n-v domainName=\"$DOMAIN_NAME\" \\\n-v caCert=\"${CA_CERT}\" \\\n-v gitUser=\"${GIT_USER}\" \\\n-v gitPassword=\"${GIT_PASSWORD}\" \\\n-v gitServer=\"${GIT_SERVER}\" \\\n&gt; \"tap-values-full.yml\"\n</code></pre> <p>We recommend you inspect the generated file:</p> <pre><code>cat tap-values-full.yml\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#install-updated-profile","title":"Install Updated Profile","text":"<p>We then update the TAP installation via the same command.</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_VERSION=1.5.0\n</code></pre> <pre><code>tanzu package install tap \\\n-p tap.tanzu.vmware.com \\\n-v $TAP_VERSION \\\n--values-file tap-values-full.yml \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#verify-components","title":"Verify Components","text":"<ul> <li>metadata-store</li> <li>scanning</li> <li>grype</li> </ul> <pre><code>tanzu package installed get metadata-store -n tap-install\ntanzu package installed get scanning -n tap-install\ntanzu package installed get grype -n tap-install\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#add-scan-policy","title":"Add Scan Policy","text":"<p>We cannot yet run our Testing &amp; Scanning pipeline, we need a Scan Policy!</p> <p>The Scan Policy contains the rules by which to judge the outcome of the vulnerability scans.</p> <p>For more information about this policy, refer to the TAP docs3.</p> <p>Let's create a Policy:</p> scan-policy.yaml<pre><code>apiVersion: scanning.apps.tanzu.vmware.com/v1beta1\nkind: ScanPolicy\nmetadata:\nname: scan-policy\nlabels:\n'app.kubernetes.io/part-of': 'enable-in-gui'\nspec:\nregoFile: |\npackage main\n# Accepted Values: \"Critical\", \"High\", \"Medium\", \"Low\", \"Negligible\", \"UnknownSeverity\"\nnotAllowedSeverities := [\"Critical\", \"High\", \"UnknownSeverity\"]\nignoreCves := []\ncontains(array, elem) = true {\narray[_] = elem\n} else = false { true }\nisSafe(match) {\nseverities := { e | e := match.ratings.rating.severity } | { e | e := match.ratings.rating[_].severity }\nsome i\nfails := contains(notAllowedSeverities, severities[i])\nnot fails\n}\nisSafe(match) {\nignore := contains(ignoreCves, match.id)\nignore\n}\ndeny[msg] {\ncomps := { e | e := input.bom.components.component } | { e | e := input.bom.components.component[_] }\nsome i\ncomp := comps[i]\nvulns := { e | e := comp.vulnerabilities.vulnerability } | { e | e := comp.vulnerabilities.vulnerability[_] }\nsome j\nvuln := vulns[j]\nratings := { e | e := vuln.ratings.rating.severity } | { e | e := vuln.ratings.rating[_].severity }\nnot isSafe(vuln)\nmsg = sprintf(\"CVE %s %s %s\", [comp.name, vuln.id, ratings])\n}\n</code></pre> <p>And apply this to the TAP Developer Namespace.</p> <pre><code>export TAP_DEVELOPER_NAMESPACE=dev\n</code></pre> <pre><code>kubectl apply -f scan-policy.yaml \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>Update The Policy To Reflect Reality</p> <p>It is better to fix the leak before attempting to clear the bucket.</p> <p>So you might want to setup lest strict rules to start with, so that people get time to resolve them.</p> <p>For example, in our test application (see next section) there are some vulnerabilities.</p> <p>As I don't care too much about those at this point in time, I will update the policy.</p> <p>First, I'll restrict the <code>notAllowedSeverities</code> to <code>Critical</code> only.</p> <p>And then I add the known vulnerabilities in that category. If new Criticals show up, it will fail, but for now, we can start the pipeline</p> <pre><code>notAllowedSeverities := [\"Critical\"]\nignoreCves := [\"CVE-2016-1000027\", \"CVE-2016-0949\",\"CVE-2017-11291\",\"CVE-2018-12805\",\"CVE-2018-4923\",\"CVE-2021-40719\",\"CVE-2018-25076\",\"GHSA-45hx-wfhj-473x\",\"GHSA-jvfv-hrrc-6q72\",\"CVE-2018-12804\",\"GHSA-36p3-wjmg-h94x\",\"GHSA-36p3-wjmg-h94x\",\"GHSA-6v73-fgf6-w5j7\"]\n</code></pre> <p>Read the Triaging and Remediating CVEs guide4 for more information.</p>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#add-tekton-pipeline","title":"Add Tekton Pipeline","text":"<p>The Test in the Testing and Scanning Supply Chain is executed by a Tekton Pipeline.</p> <p>The Supply Chain contains a PipelineRun template which assumes a Tekton Pipeline with a specific name and input parameters exist.</p> <p>This Pipeline is not included out of the box, though the docs do have an example5.</p> <p>We start with using this example (except for the image,  to avoid Dockerhub rate limiting), as it fits our example application.</p> <p>Later, if you want, we'll guide you to a more elaborate solution.</p> tekton-pipeline-test.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: developer-defined-tekton-pipeline\nlabels:\napps.tanzu.vmware.com/pipeline: test     # (!) required\nspec:\nparams:\n- name: source-url                       # (!) required\n- name: source-revision                  # (!) required\ntasks:\n- name: test\nparams:\n- name: source-url\nvalue: $(params.source-url)\n- name: source-revision\nvalue: $(params.source-revision)\ntaskSpec:\nparams:\n- name: source-url\n- name: source-revision\nsteps:\n- name: test\nimage: harbor.services.h2o-2-9349.h2o.vmware.com/dockerhub/library/gradle\nscript: |-\ncd `mktemp -d`\nwget -qO- $(params.source-url) | tar xvz -m\n./mvnw test\n</code></pre> <p>Once you've created the file, apply it to the cluster. As it is a Namespaced resource, add it to your TAP Developer Namespace.</p> <pre><code>kubectl apply -f tekton-pipeline-test.yaml \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#update-workload","title":"Update Workload","text":"<p>Before, the Basic Supply Chain would pick any Workload. The Testing &amp; Scanning Supply Chain only picks up Workloads that contain the label <code>apps.tanzu.vmware.com/has-tests=true</code>.</p> <p>if you look at your existing Workloads, you'll see that they no longer match a valid Supply Chain:</p> <pre><code>tanzu apps workload list \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>Which shows:</p> <pre><code>NAME                 TYPE   APP                  READY                 AGE\ntanzu-java-web-app   web    tanzu-java-web-app   SupplyChainNotFound   13h\ntap-demo-04          web    tap-demo-04          SupplyChainNotFound   165m\n</code></pre> <p>Let's change this, by adding the missing label to our Workload.</p> Existing Workload with Tanzu CLIExisting Workload with ManifestNew Sample Workload with Tanzu CLI <pre><code>export APP_NAME=\n</code></pre> <pre><code>tanzu apps workload update $APP_NAME \\\n-n $TAP_DEVELOPER_NAMESPACE \\\n--label \"apps.tanzu.vmware.com/has-tests=true\"\n</code></pre> config/workload.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\n  name: tap-demo-04\n  labels:\n    apps.tanzu.vmware.com/workload-type: web\n    apps.tanzu.vmware.com/has-tests: \"true\"\napps.tanzu.vmware.com/auto-configure-actuators: \"true\"\napp.kubernetes.io/part-of: tap-demo-04\nspec:\n  build:\n    env:\n      - name: BP_JVM_VERSION\n        value: \"17\"\nparams:\n  - name: gitops_ssh_secret\n    value: ssh-credentials\n  - name: annotations\n    value:\n      autoscaling.knative.dev/minScale: \"1\"\nsource:\n    git:\n      url: ssh://git@gitssh.h2o-2-9349.h2o.vmware.com/lab02/tap-demo-04.git\n      ref:\n        branch: main\n</code></pre> <pre><code>tanzu apps workload create smoke-app \\\n--git-repo https://github.com/sample-accelerators/tanzu-java-web-app.git \\\n--git-branch main \\\n--type web \\\n--label app.kubernetes.io/part-of=smoke-app \\\n--label apps.tanzu.vmware.com/has-tests=true \\\n--annotation autoscaling.knative.dev/minScale=1 \\\n--yes \\\n-n \"$TAP_DEVELOPER_NAMESPACE\"\n</code></pre> <p>Once this is done, verify our Tekton Pipeline was run correctly:</p> <pre><code>kubectl get pipelinerun,taskrun \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>Which should yield something like this:</p> <pre><code>NAME                                       SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\npipelinerun.tekton.dev/tap-demo-04-f62k8   True        Succeeded   3m53s       2m28s\n\nNAME                                                        SUCCEEDED   REASON                   STARTTIME   COMPLETIONTIME\ntaskrun.tekton.dev/scan-tap-demo-04-9xjcf                   False       CouldntGetTask           54s         53s\ntaskrun.tekton.dev/tap-demo-04-f62k8-test                   True        Succeeded                3m53s       2m28s\n</code></pre> <p>And verify the scans:</p> <pre><code>kubectl get sourcescan,imagescan \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <p>Which should yield something like this:</p> <pre><code>NAME                                                    PHASE       SCANNEDREVISION                                 SCANNEDREPOSITORY                                                                                                                              AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN   CVETOTAL\nsourcescan.scanning.apps.tanzu.vmware.com/smoke-app     Completed   main/6db88c7a7e7dec1843809b058195b68480c4c12a   http://fluxcd-source-controller.flux-system.svc.cluster.local./gitrepository/dev/smoke-app/6db88c7a7e7dec1843809b058195b68480c4c12a.tar.gz     16m   1          0      0        0     0         1\nsourcescan.scanning.apps.tanzu.vmware.com/tap-demo-04   Completed   main/c3a1da9983f56c0ff3d595077b2f12ff238cb92a   http://fluxcd-source-controller.flux-system.svc.cluster.local./gitrepository/dev/tap-demo-04/c3a1da9983f56c0ff3d595077b2f12ff238cb92a.tar.gz   10h   1          0      0        0     0         1\nNAME                                                   PHASE       SCANNEDIMAGE                                                                                                                                 AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN   CVETOTAL\nimagescan.scanning.apps.tanzu.vmware.com/smoke-app     Completed   harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/smoke-app-dev@sha256:9c67da3d7b6ff8bc4e622aa6f213e4a8acd3c602fa79c06dd517e3d1c50b80a1     14m   1          8      10       16    0         35\nimagescan.scanning.apps.tanzu.vmware.com/tap-demo-04   Completed   harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/tap-demo-04-dev@sha256:c63c3003e8f57c0bd6583d90e2f35a28025c85a93ddb9a2c67542f39cc07ffff   16m   0          3      5        16    0         24\n</code></pre> <p>SourceScan Error with no Data</p> <p>Unfortunately, the ScanTemplate setup doesn't always handle to secrets correctly.</p> <p>The Scan Templates are generated by TAP when you install the profile.</p> <p>They need to contact the Metadata Store, and need to trust its Certificate.</p> <p>It does so by importing the <code>app-tls-cert</code> from the <code>metadata-store</code> Namespace.</p> <p>Sometimes this fails, and then the Source Scan returns this data:</p> <pre><code>kubectl get sourcescan -n $TAP_DEVELOPER_NAMESPACE\n</code></pre> <pre><code>NAME          PHASE   SCANNEDREVISION   SCANNEDREPOSITORY   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN   CVETOTAL\ntap-demo-04   Error                                         19m\n</code></pre> <p>To remedy this, we can do the secret export and import ourselves with the SecretGen Controller:</p> <p>Note, this is a temporary solution, as TAP will remove the secret in the <code>dev</code> namespace within a few minutes.</p> <p>Read this paragraph for a more permanent solution.</p> metadata-tls-secret-import-and-export.yaml<pre><code>---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretExport\nmetadata:\nname: app-tls-cert\nnamespace: metadata-store\nspec:\ntoNamespace: dev\n---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretImport\nmetadata:\nname: app-tls-cert\nnamespace: dev\nspec:\nfromNamespace: metadata-store\n</code></pre> <p>And then apply it to the cluster.</p> <pre><code>kubectl apply -f metadata-tls-secret-import-and-export.yaml \\\n-n $TAP_DEVELOPER_NAMESPACE\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#query-for-vulnerabilities","title":"Query For Vulnerabilities","text":"<p>If there are CVE's found, you might want to investigate them.</p> <p>You can do so via the Insight plugin for the Tanzu CLI.</p> <p>It is an optional excercise, for which I recommend following the docs6</p>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#enable-cve-scan-results-in-tap-gui","title":"Enable CVE scan results in TAP GUI","text":"<p>If you have looked at the Supply Chain view on TAP GUI for your Workloads, you might have noticed there are no scan or CVE results there.</p> <p>Either in the Security Analysis screen, or the Source Scanner and Image Scanner steps, the CVE tables are empty.</p> <p>That is because the TAP GUI does not have access7 to the Metadata Store by default.</p> <p>Let us enable this!</p>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#obtain-read-token","title":"Obtain Read Token","text":"<p>We can either create a new write token9,  or we can use the already created read token8</p> <p>I prefer not to create new secrets, which we'll have to manage, unless absolutely necessary.</p> <p>So we retrieve the existing token.</p> <p>We do so, via this command:</p> <pre><code>export METADATA_TOKEN=$(kubectl get secret metadata-store-read-write-client \\\n-n metadata-store -o jsonpath=\"{.data.token}\"\\\n| base64 -d)\necho \"METADATA_TOKEN=$METADATA_TOKEN\"\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#update-tap-profile","title":"Update TAP Profile","text":"<pre><code>tap_gui:\napp_config:\nproxy:\n/metadata-store:\ntarget: https://metadata-store-app.metadata-store:8443/api/v1\nchangeOrigin: true\nsecure: false\nheaders:\nAuthorization: \"Bearer ACCESS-TOKEN\"\nX-Custom-Source: project-star\n</code></pre> <p>Replace ACCESS-TOKEN</p> <p>If you manually update the Values file, remember to replace <code>ACCESS-TOKEN</code> with the actual token!</p> full-profile.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: full\nshared:\ningress_domain: #@ dv.domainName\nca_cert_data: #@ dv.caCert\nimage_registry:\nsecret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nkp_default_repository: #@ kpRegistry\nkp_default_repository_secret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nsupply_chain: testing_scanning\nootb_supply_chain_testing_scanning:\nregistry:\nserver: #@ dv.buildRegistry\nrepository: #@ dv.buildRepo\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: #@ \"appliveview.\"+dv.domainName\nappliveview:\ningressEnabled: true\nserver:\ntls:\nenabled: false\ntap_gui:\nservice_type: ClusterIP\napp_config:\nproxy:\n/metadata-store:\ntarget: https://metadata-store-app.metadata-store:8443/api/v1\nchangeOrigin: true\nsecure: false\nheaders:\nAuthorization: #@ \"Bearer \"+dv.metadataToken\nX-Custom-Source: project-star\nbackend:\nreading:\nallow:\n- host: #@ dv.gitServer\nintegrations:\ngitea:\n- host: #@ dv.gitServer\nusername: #@ dv.gitUser\npassword: #@ dv.gitPassword\nauth:\nallowGuestAccess: true\ncustomize:\ncustom_name: 'Portal McPortalFace'\norganization:\nname: 'Org McOrg Face'\ncatalog:\nlocations:\n- type: url\ntarget: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n- type: url\ntarget: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\ncrossplane:\nregistryCaBundleConfig:\nname: ca-bundle-config\nkey: ca-bundle\n#! reduces memory and CPU requirements, not recommended for production\n#! but our Lab environments have resource restrictions\ncnrs:\nlite:\nenable: true contour:\nenvoy:\nservice:\ntype: LoadBalancer\ngrype:\ndb:\ndbUpdateUrl: https://minio.services.h2o-2-9349.h2o.vmware.com/grype/databases/listing.json\nceip_policy_disclosed: true\nexcluded_packages:\n- eventing.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n- tap-telemetry.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#generate-updated-profile_1","title":"Generate updated profile","text":"<pre><code>export GIT_SERVER=gitea.services.h2o-2-9349.h2o.vmware.com\nexport GIT_USER=gitea\nexport GIT_PASSWORD='VMware123!'\nexport TAP_BUILD_REGISTRY_SECRET=registry-credentials\nexport BUILD_REGISTRY_REPO=tap-apps\nexport TBS_REPO=buildservice/tbs-full-deps\nexport CA_CERT=$(cat ca.crt)\nexport BUILD_REGISTRY=\nexport DOMAIN_NAME=\n</code></pre> <p>And then we run YTT to generate our Profile configuration file.</p> <pre><code>ytt -f full-profile.ytt.yaml \\\n-v buildRegistry=\"$BUILD_REGISTRY\" \\\n-v buildRegistrySecret=\"$BUILD_REGISTRY_SECRET\" \\\n-v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n-v tbsRepo=\"$TBS_REPO\" \\\n-v domainName=\"$DOMAIN_NAME\" \\\n-v caCert=\"${CA_CERT}\" \\\n-v gitUser=\"${GIT_USER}\" \\\n-v gitPassword=\"${GIT_PASSWORD}\" \\\n-v gitServer=\"${GIT_SERVER}\" \\\n-v metadataToken=\"${METADATA_TOKEN}\" \\\n&gt; \"tap-values-full.yml\"\n</code></pre> <p>We recommend you inspect the generated file:</p> <pre><code>cat tap-values-full.yml\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#install-updated-profile_1","title":"Install Updated Profile","text":"<p>We then update the TAP installation via the same command.</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_VERSION=1.5.0\n</code></pre> <pre><code>tanzu package install tap \\\n-p tap.tanzu.vmware.com \\\n-v $TAP_VERSION \\\n--values-file tap-values-full.yml \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#view-security-scan-data-in-tap-gui","title":"View Security Scan Data in TAP GUI","text":"<p>Go to your TAP GUI, and either open a Scan step in a Supply Chain, or open the Security Analysis screen (shield with magnifying glass icon).</p> <p>You should now see the scan result details.</p>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#create-tls-app-cert-secret-separately","title":"Create TLS App Cert Secret Separately","text":"<p>Using the SecretGen's SecretImport and SecretExport capability solves the missing <code>app-tls-cert</code> temporarily.</p> <p>For a permanent solution, we need to do the following:</p> <ul> <li>copy the secret to another namespace</li> <li>update the Grype app config to use this new secret</li> <li>generate the new values file</li> <li>update our TAP install</li> </ul>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#copy-the-certificate-secret","title":"Copy the Certificate Secret","text":"<p>Let's copy the secret into a file.</p> <pre><code>kubectl get secret -n metadata-store app-tls-cert \\\n-o yaml &gt; metadata-store-app-tls-cert.yaml\n</code></pre> <p>Remove any values we can't use:</p> <pre><code>yq e -i '.metadata = {\"name\": \"metadata-store-app-tls-cert\"}' metadata-store-app-tls-cert.yaml\n</code></pre> <p>And then apply it to our target namespace:</p> <pre><code>kubectl apply -f metadata-store-app-tls-cert.yaml \\\n-n ${TAP_DEVELOPER_NAMESPACE}\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#update-tap-profile_1","title":"Update TAP Profile","text":"<p>The Grype config to change, is the following:</p> <pre><code>grype:\nnamespace: dev\nmetadataStore:\ncaSecret:\nname: metadata-store-app-tls-cert\n</code></pre> full-profile.ytt.yaml<pre><code>#@ load(\"@ytt:data\", \"data\")\n#@ dv = data.values\n#@ kpRegistry = \"{}/{}\".format(dv.buildRegistry, dv.tbsRepo)\n---\nprofile: full\nshared:\ningress_domain: #@ dv.domainName\nca_cert_data: #@ dv.caCert\nimage_registry:\nsecret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nbuildservice:\npull_from_kp_default_repo: true\nexclude_dependencies: true\nkp_default_repository: #@ kpRegistry\nkp_default_repository_secret:\nname: #@ dv.buildRegistrySecret\nnamespace: tap-install\nsupply_chain: testing_scanning\nootb_supply_chain_testing_scanning:\nregistry:\nserver: #@ dv.buildRegistry\nrepository: #@ dv.buildRepo\nappliveview_connector:\nbackend:\nsslDeactivated: true\ningressEnabled: true\nhost: #@ \"appliveview.\"+dv.domainName\nappliveview:\ningressEnabled: true\nserver:\ntls:\nenabled: false\ntap_gui:\nservice_type: ClusterIP\napp_config:\nproxy:\n/metadata-store:\ntarget: https://metadata-store-app.metadata-store:8443/api/v1\nchangeOrigin: true\nsecure: false\nheaders:\nAuthorization: #@ \"Bearer \"+dv.metadataToken\nX-Custom-Source: project-star\nbackend:\nreading:\nallow:\n- host: #@ dv.gitServer\nintegrations:\ngitea:\n- host: #@ dv.gitServer\nusername: #@ dv.gitUser\npassword: #@ dv.gitPassword\nauth:\nallowGuestAccess: true\ncustomize:\ncustom_name: 'Portal McPortalFace'\norganization:\nname: 'Org McOrg Face'\ncatalog:\nlocations:\n- type: url\ntarget: https://github.com/joostvdg/tap-catalog/blob/main/catalog-info.yaml\n- type: url\ntarget: https://github.com/joostvdg/tap-hello-world/blob/main/catalog/catalog-info.yaml\ncrossplane:\nregistryCaBundleConfig:\nname: ca-bundle-config\nkey: ca-bundle\n#! reduces memory and CPU requirements, not recommended for production\n#! but our Lab environments have resource restrictions\ncnrs:\nlite:\nenable: true contour:\nenvoy:\nservice:\ntype: LoadBalancer\ngrype:\ndb:\ndbUpdateUrl: https://minio.services.h2o-2-9349.h2o.vmware.com/grype/databases/listing.json\nnamespace: dev\nmetadataStore:\ncaSecret:\nname: metadata-store-app-tls-cert\nceip_policy_disclosed: true\nexcluded_packages:\n- eventing.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n- tap-telemetry.tanzu.vmware.com #! not used, so removing to reduce memory/cpu footprint\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#generate-updated-profile_2","title":"Generate updated profile","text":"<pre><code>export GIT_SERVER=gitea.services.h2o-2-9349.h2o.vmware.com\nexport GIT_USER=gitea\nexport GIT_PASSWORD='VMware123!'\nexport TAP_BUILD_REGISTRY_SECRET=registry-credentials\nexport BUILD_REGISTRY_REPO=tap-apps\nexport TBS_REPO=buildservice/tbs-full-deps\nexport CA_CERT=$(cat ca.crt)\nexport BUILD_REGISTRY=\nexport DOMAIN_NAME=\n</code></pre> <p>And then we run YTT to generate our Profile configuration file.</p> <pre><code>ytt -f full-profile.ytt.yaml \\\n-v buildRegistry=\"$BUILD_REGISTRY\" \\\n-v buildRegistrySecret=\"$BUILD_REGISTRY_SECRET\" \\\n-v buildRepo=\"$BUILD_REGISTRY_REPO\" \\\n-v tbsRepo=\"$TBS_REPO\" \\\n-v domainName=\"$DOMAIN_NAME\" \\\n-v caCert=\"${CA_CERT}\" \\\n-v gitUser=\"${GIT_USER}\" \\\n-v gitPassword=\"${GIT_PASSWORD}\" \\\n-v gitServer=\"${GIT_SERVER}\" \\\n-v metadataToken=\"${METADATA_TOKEN}\" \\\n&gt; \"tap-values-full.yml\"\n</code></pre> <p>We recommend you inspect the generated file:</p> <pre><code>cat tap-values-full.yml\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#install-updated-profile_2","title":"Install Updated Profile","text":"<p>We then update the TAP installation via the same command.</p> <pre><code>export TAP_INSTALL_NAMESPACE=tap-install\nexport TAP_VERSION=1.5.0\n</code></pre> <pre><code>tanzu package install tap \\\n-p tap.tanzu.vmware.com \\\n-v $TAP_VERSION \\\n--values-file tap-values-full.yml \\\n-n ${TAP_INSTALL_NAMESPACE}\n</code></pre> metadata-tls-secret-import-and-export.yaml<pre><code>---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretExport\nmetadata:\nname: metadata-store-app-tls-cert\nnamespace: metadata-store-secrets\nspec:\ntoNamespace: dev\n---\napiVersion: secretgen.carvel.dev/v1alpha1\nkind: SecretImport\nmetadata:\nname: metadata-store-app-tls-cert\nnamespace: dev\nspec:\nfromNamespace: metadata-store-secrets\n</code></pre> <pre><code>kubectl apply -f metadata-tls-secret-import-and-export.yaml\n</code></pre>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/basic-to-test-scan/#references","title":"References","text":"<ol> <li> <p>Grype \u21a9</p> </li> <li> <p>Joostvdg's Blog - How relocate Grype CVE Database with Minio \u21a9</p> </li> <li> <p>TAP 1.5 - Testing and Scanning Supply Chain \u21a9</p> </li> <li> <p>TAP 1.5 - Triage and Remidate CVEs for Supply Chain \u21a9</p> </li> <li> <p>TAP 1.5 - Tekton Pipeline Example for Testing and Scanning Supply Chain \u21a9</p> </li> <li> <p>TAP 1.5 - Triage Security Vulnerabilities \u21a9</p> </li> <li> <p>TAP 1.5 - Connect TAP GUI to Metadata Store \u21a9</p> </li> <li> <p>TAP 1.5 - Retrieve Metadata Store Read Token \u21a9</p> </li> <li> <p>TAP 1.5 - Create Metadata Store Write Token \u21a9</p> </li> </ol>","tags":["tap","kubernetes","install"]},{"location":"supply-chain/custom-notes/","title":"Notes On Workshop","text":""},{"location":"supply-chain/custom-notes/#cartographer","title":"Cartographer","text":"<ul> <li>harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/tap-demo-03-dev@sha256:5e7615dd7e9e02dc32b96e089a87870c19d55d143ecb9eea3eb701ad8cba8013 git</li> <li>ghcr.io/joostvdg/go-demo:v2.1.16</li> <li>https://github.com/joostvdg/go-demo/blob/main/k8s/deployment.yaml</li> </ul> <p>To construct and use a Supply Chain, we need the following ingredients:</p> <ol> <li>One or more Resources, usually Cartographer templates</li> <li>A SupplyChain definition, using those Resources</li> <li>A Workload definition that selects the Supply Chain</li> </ol>"},{"location":"supply-chain/custom-notes/#my-first-template","title":"My First Template","text":"<p>Relies on Workload Definition</p> <p>Cartographer works from a Workload definition.</p> <p>So any template, like the ClusterTemplate below, works because the trigger is a Workload CR.</p> cluster-template-app-deploy.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy\nspec:\ntemplate:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\nlabels:\napp: $(workload.metadata.name)$\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: $(workload.metadata.name)$\ntemplate:\nmetadata:\nlabels:\napp: $(workload.metadata.name)$\nspec:\ncontainers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\n</code></pre> <p>For the Workload definition, it needs to supply the following fields:</p> <ul> <li><code>metadata.name</code></li> <li><code>spec.image</code></li> </ul>"},{"location":"supply-chain/custom-notes/#supply-chain-definition","title":"Supply Chain Definition","text":"<p>The supply chain has three top level fields in its spec, the resources, a service account reference and a selector for workloads.</p> <ol> <li>Resources: Which CRs are used (and how)</li> <li>ServiceAccount Reference: reference to a ServiceAccount that can use the Templates/Resources</li> <li>Selector: Set of Kubernetes Labels which you have to specify on a Workload to trigger this Supply Chain</li> </ol> my-first-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: my-first-supply-chain\nspec:\nresources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy\nserviceAccountRef:\nname: cartographer-pre-built-sa\nnamespace: default\nselector:\nworkload-type: pre-built\n</code></pre>"},{"location":"supply-chain/custom-notes/#workload-definition","title":"Workload Definition","text":"<p>Remember, we need to specify the following fields:</p> <ul> <li><code>metadata.name</code></li> <li><code>spec.image</code></li> </ul> workload-pre-built-hello.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello\nlabels:\nworkload-type: pre-built\nspec:\nimage: ghcr.io/joostvdg/go-demo:v2.1.16\n</code></pre>"},{"location":"supply-chain/custom-notes/#rbac","title":"RBAC","text":"rbac.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cartographer-pre-built-sa\nnamespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: deploy-image-role\nrules:\n- apiGroups:\n- apps\nresources:\n- deployments\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-prebuilt-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: deploy-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-pre-built-sa\n</code></pre>"},{"location":"supply-chain/custom-notes/#putting-it-together","title":"Putting it together","text":"<pre><code>kubectl apply -f rbac.yaml\nkubectl apply -f cluster-template-app-deploy.yaml\nkubectl apply -f my-first-supply-chain.yaml\nkuebctl apply -f workload-pre-built-hello.yaml\n</code></pre> <pre><code>kubectl tree workload hello\n</code></pre> <pre><code>NAMESPACE  NAME                                       READY  REASON  AGE\ndefault    Workload/hello                             True   Ready   98m\ndefault    \u2514\u2500Deployment/hello-deployment              -              98m\ndefault      \u2514\u2500ReplicaSet/hello-deployment-cfdf74d6   -              98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-kjjp5  True           98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-wgtvl  True           98m\ndefault        \u2514\u2500Pod/hello-deployment-cfdf74d6-x2pmc  True           98m\n</code></pre> <pre><code>kubectl port-forward deployment/hello-deployment 8080:8080\n</code></pre> <pre><code>curl http://localhost:8080/\n</code></pre>"},{"location":"supply-chain/custom-notes/#with-params","title":"With Params","text":"<ul> <li>https://cartographer.sh/docs/v0.7.0/tutorials/using-params/</li> </ul> <pre><code>kubectl port-forward deployment/hello-deployment 8080:8080\n</code></pre> <pre><code>curl http://localhost:8080/\n</code></pre>"},{"location":"supply-chain/custom-notes/#extending","title":"Extending","text":"<ul> <li>https://cartographer.sh/docs/v0.7.0/tutorials/extending-a-supply-chain/</li> <li>https://github.com/paketo-buildpacks/go-build</li> <li>https://github.com/joostvdg/go-demo/</li> <li>https://github.com/pivotal/kpack/blob/main/docs/image.md</li> </ul> <pre><code>tanzu apps workload tail source-code-01 --namespace dev --timestamp --since 1h\n</code></pre>"},{"location":"supply-chain/custom-notes/#tekton-lifecycle","title":"Tekton LifeCycle","text":""},{"location":"supply-chain/custom-notes/#ootb-supply-chain","title":"OOTB Supply Chain","text":""},{"location":"supply-chain/custom-notes/#build-cluster","title":"Build Cluster","text":"<pre><code>kubectl tree workload -n dev tap-demo-03   </code></pre> <pre><code>NAMESPACE  NAME                                                READY  REASON              AGE\ndev        Workload/tap-demo-03                                True   Ready               5d2h\ndev        \u251c\u2500ConfigMap/tap-demo-03                             -                          5d\ndev        \u251c\u2500ConfigMap/tap-demo-03-deliverable                 -                          5d2h\ndev        \u251c\u2500ConfigMap/tap-demo-03-with-api-descriptors        -                          5d\ndev        \u251c\u2500ConfigMap/tap-demo-03-with-claims                 -                          5d\ndev        \u251c\u2500GitRepository/tap-demo-03                         True   Succeeded           5d2h\ndev        \u251c\u2500Image/tap-demo-03                                 True                       5d1h\ndev        \u2502 \u251c\u2500Build/tap-demo-03-build-1                       -                          5d1h\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-build-1-build-pod             False  PodCompleted        5d1h\ndev        \u2502 \u251c\u2500Build/tap-demo-03-build-2                       -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-build-2-build-pod             False  PodCompleted        5d\ndev        \u2502 \u251c\u2500Build/tap-demo-03-build-3                       -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-build-3-build-pod             False  PodCompleted        5d\ndev        \u2502 \u251c\u2500PersistentVolumeClaim/tap-demo-03-cache         -                          5d1h\ndev        \u2502 \u2514\u2500SourceResolver/tap-demo-03-source               True                       5d1h\ndev        \u251c\u2500ImageScan/tap-demo-03                             -                          5d1h\ndev        \u2502 \u251c\u2500TaskRun/scan-tap-demo-03-29z7x                  -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/scan-tap-demo-03-29z7x-pod                False  PodCompleted        5d\ndev        \u2502 \u251c\u2500TaskRun/scan-tap-demo-03-2hwn7                  -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/scan-tap-demo-03-2hwn7-pod                False  PodCompleted        5d\ndev        \u2502 \u251c\u2500TaskRun/scan-tap-demo-03-bzrb9                  -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/scan-tap-demo-03-bzrb9-pod                False  PodCompleted        5d\ndev        \u2502 \u2514\u2500TaskRun/scan-tap-demo-03-phsmz                  -                          5d1h\ndev        \u2502   \u2514\u2500Pod/scan-tap-demo-03-phsmz-pod                False  PodCompleted        5d1h\ndev        \u251c\u2500PodIntent/tap-demo-03                             True   ConventionsApplied  5d\ndev        \u251c\u2500Runnable/tap-demo-03                              True   Ready               5d2h\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-46rnd                   -                          5d\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-16560cea6b          -                          5d\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-46rnd-fetch-repository    -                          5d\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-46rnd-fetch-repository-pod  False  PodCompleted        5d\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-46rnd-maven-run           -                          5d\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-46rnd-maven-run-pod         False  PodCompleted        5d\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-66fp8                   -                          5d\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-8bd0367a05          -                          5d\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-66fp8-fetch-repository    -                          5d\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-66fp8-fetch-repository-pod  False  PodCompleted        5d\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-66fp8-maven-run           -                          5d\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-66fp8-maven-run-pod         False  PodCompleted        5d\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-6xtlf                   -                          5d\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-8bd736ceef          -                          5d\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-6xtlf-fetch-repository    -                          5d\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-6xtlf-fetch-repository-pod  False  PodCompleted        5d\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-6xtlf-maven-run           -                          5d\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-6xtlf-maven-run-pod         False  PodFailed           5d\ndev        \u2502 \u251c\u2500PipelineRun/tap-demo-03-ff5dn                   -                          5d1h\ndev        \u2502 \u2502 \u251c\u2500PersistentVolumeClaim/pvc-b5da4a07af          -                          5d1h\ndev        \u2502 \u2502 \u251c\u2500TaskRun/tap-demo-03-ff5dn-fetch-repository    -                          5d1h\ndev        \u2502 \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-ff5dn-fetch-repository-pod  False  PodCompleted        5d1h\ndev        \u2502 \u2502 \u2514\u2500TaskRun/tap-demo-03-ff5dn-maven-run           -                          5d1h\ndev        \u2502 \u2502   \u2514\u2500Pod/tap-demo-03-ff5dn-maven-run-pod         False  PodFailed           5d1h\ndev        \u2502 \u2514\u2500PipelineRun/tap-demo-03-wfswt                   -                          5d2h\ndev        \u2502   \u251c\u2500PersistentVolumeClaim/pvc-29e80c6f81          -                          5d2h\ndev        \u2502   \u251c\u2500TaskRun/tap-demo-03-wfswt-fetch-repository    -                          5d2h\ndev        \u2502   \u2502 \u2514\u2500Pod/tap-demo-03-wfswt-fetch-repository-pod  False  PodCompleted        5d2h\ndev        \u2502   \u2514\u2500TaskRun/tap-demo-03-wfswt-maven-run           -                          5d2h\ndev        \u2502     \u2514\u2500Pod/tap-demo-03-wfswt-maven-run-pod         False  PodCompleted        5d2h\ndev        \u251c\u2500Runnable/tap-demo-03-config-writer                True   Ready               5d\ndev        \u2502 \u251c\u2500TaskRun/tap-demo-03-config-writer-gllk2         -                          5d\ndev        \u2502 \u2502 \u2514\u2500Pod/tap-demo-03-config-writer-gllk2-pod       False  PodCompleted        5d\ndev        \u2502 \u2514\u2500TaskRun/tap-demo-03-config-writer-qgsk5         -                          5d\ndev        \u2502   \u2514\u2500Pod/tap-demo-03-config-writer-qgsk5-pod       False  PodCompleted        5d\ndev        \u2514\u2500SourceScan/tap-demo-03                            -                          5d1h\ndev          \u251c\u2500TaskRun/scan-tap-demo-03-7dnjg                  -                          5d\ndev          \u2502 \u2514\u2500Pod/scan-tap-demo-03-7dnjg-pod                False  PodCompleted        5d\ndev          \u251c\u2500TaskRun/scan-tap-demo-03-9vlxv                  -                          5d\ndev          \u2502 \u2514\u2500Pod/scan-tap-demo-03-9vlxv-pod                False  PodCompleted        5d\ndev          \u251c\u2500TaskRun/scan-tap-demo-03-kv2sv                  -                          5d\ndev          \u2502 \u2514\u2500Pod/scan-tap-demo-03-kv2sv-pod                False  PodCompleted        5d\ndev          \u2514\u2500TaskRun/scan-tap-demo-03-nf6j6                  -                          5d1h\ndev            \u2514\u2500Pod/scan-tap-demo-03-nf6j6-pod                False  PodCompleted        5d1h\n</code></pre>"},{"location":"supply-chain/custom-notes/#run-cluster","title":"Run Cluster","text":"<pre><code>kubectl tree deliverable tap-demo-03 -n apps\n</code></pre> <pre><code>NAMESPACE  NAME                                    READY  REASON  AGE\napps       Deliverable/tap-demo-03                 True   Ready   4d23h\napps       \u251c\u2500App/tap-demo-03                       -              4d23h\napps       \u2514\u2500ImageRepository/tap-demo-03-delivery  True   Ready   4d23h\n</code></pre>"},{"location":"supply-chain/custom-notes/#iterate-cluster","title":"Iterate Cluster","text":"<pre><code>kubectl tree workload -n dev smoke-app\n</code></pre> <pre><code>NAMESPACE  NAME                                         READY  REASON              AGE\ndev        Workload/smoke-app                           True   Ready               2d23h\ndev        \u251c\u2500ConfigMap/smoke-app                        -                          2d23h\ndev        \u251c\u2500ConfigMap/smoke-app-with-api-descriptors   -                          2d23h\ndev        \u251c\u2500ConfigMap/smoke-app-with-claims            -                          2d23h\ndev        \u251c\u2500Deliverable/smoke-app                      True   Ready               2d23h\ndev        \u2502 \u251c\u2500App/smoke-app                            -                          2d23h\ndev        \u2502 \u2514\u2500ImageRepository/smoke-app-delivery       True   Ready               2d23h\ndev        \u251c\u2500GitRepository/smoke-app                    True   Succeeded           2d23h\ndev        \u251c\u2500Image/smoke-app                            True                       2d23h\ndev        \u2502 \u251c\u2500Build/smoke-app-build-1                  -                          2d23h\ndev        \u2502 \u2502 \u2514\u2500Pod/smoke-app-build-1-build-pod        False  PodCompleted        2d23h\ndev        \u2502 \u251c\u2500PersistentVolumeClaim/smoke-app-cache    -                          2d23h\ndev        \u2502 \u2514\u2500SourceResolver/smoke-app-source          True                       2d23h\ndev        \u251c\u2500PodIntent/smoke-app                        True   ConventionsApplied  2d23h\ndev        \u2514\u2500Runnable/smoke-app-config-writer           True   Ready               2d23h\ndev          \u2514\u2500TaskRun/smoke-app-config-writer-wdlzc    -                          2d23h\ndev            \u2514\u2500Pod/smoke-app-config-writer-wdlzc-pod  False  PodCompleted        2d23h\n</code></pre> <pre><code>kubectl tree deliverable -n dev smoke-app\n</code></pre> <pre><code>NAMESPACE  NAME                                  READY  REASON  AGE\ndev        Deliverable/smoke-app                 True   Ready   2d23h\ndev        \u251c\u2500App/smoke-app                       -              2d23h\ndev        \u2514\u2500ImageRepository/smoke-app-delivery  True   Ready   2d23h\n</code></pre>"},{"location":"supply-chain/custom-notes/#modify-ootb-supply-chains","title":"Modify OOTB Supply Chains","text":"<ul> <li>https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/scc-authoring-supply-chains.html#modifying-an-out-of-the-box-supply-chain-2</li> </ul> <p>To change the shape of a supply chain or the template that it points to, do the following:</p> <ol> <li>Copy one of the reference supply chains.</li> <li>Remove the old supply chain. See preventing Tanzu Application Platform supply chains from being installed.</li> <li>Edit the supply chain object.</li> <li>Submit the modified supply chain to the cluster</li> </ol>"},{"location":"supply-chain/custom-notes/#tekton-tutorial","title":"Tekton Tutorial","text":"<ul> <li>https://tekton.dev/docs/getting-started/</li> <li>https://tekton.dev/docs/getting-started/tasks/</li> <li>https://cartographer.sh/docs/v0.7.0/tutorials/lifecycle/</li> <li>https://tekton.dev/docs/results/</li> <li>https://github.com/tektoncd/catalog/tree/main/task</li> <li>Adding custom behavior to Supply Chains: https://docs.vmware.com/en/VMware-Tanzu-Application-Platform/1.5/tap/scc-authoring-supply-chains.html#adding-custom-behavior-to-supply-chains-7</li> </ul>"},{"location":"supply-chain/custom-notes/#tekton-hello-world","title":"Tekton Hello World","text":"<pre><code>export TAP_DEVELOPMENT_NAMESPACE=dev\n</code></pre> <pre><code>kubectl apply -f 01-task-hello.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl apply -f 02-task-run.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl get taskrun hello-task-run -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl -n ${TAP_DEVELOPMENT_NAMESPACE} logs hello-task-run-pod -c step-echo\n</code></pre> <pre><code>kubectl delete -f 01-task-hello.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\nkubectl delete -f 02-task-run.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre>"},{"location":"supply-chain/custom-notes/#tekton-pipeline","title":"Tekton Pipeline","text":"<pre><code>kubectl -n dev logs hello-goodbye-run-goodbye-pod -c step-goodbye\n</code></pre> <pre><code>kubectl create -f 04-pipeline-run-dynamic.yaml -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl get taskrun,pipelinerun -n dev\n</code></pre>"},{"location":"supply-chain/custom-notes/#tekton-pipeline-with-workspace","title":"Tekton Pipeline With Workspace","text":"<pre><code>kubectl get taskrun,pipelinerun -n dev\n</code></pre> <pre><code>kubectl get pod  -n ${TAP_DEVELOPMENT_NAMESPACE}\n</code></pre> <pre><code>kubectl -n ${TAP_DEVELOPMENT_NAMESPACE} logs  clone-git-next-tag-run-z5vrr-git-next-tag-pod\n</code></pre> <p>``sh kubectl get taskrun -n dev -l tekton.dev/task=git-next-tag <pre><code>```sh\nkubectl get taskrun -n dev -l tekton.dev/task=git-next-tag -ojson | jq '.items | map(.status.taskResults)'\n</code></pre></p>"},{"location":"supply-chain/custom-notes/#app-deploy-04","title":"App Deploy 04","text":"<pre><code>kubectl tree workload -n dev hello-again\n</code></pre> <pre><code>tanzu apps workload tail hello-04 --namespace dev --timestamp --since 1h\n</code></pre> <pre><code>kubectl port-forward deployment/hello-deployment 8080:8080\n</code></pre> <pre><code>curl http://localhost:8080/\n</code></pre>"},{"location":"supply-chain/custom-notes/#examples","title":"Examples","text":"<ul> <li>ClusterSupplyChain</li> <li>ClusterSourceTemplate<ul> <li>ClusterRunTemplate</li> <li>PipelineRun<ul> <li>Task</li> <li>TaskRun (Generated)<ul> <li>Pod (Generated)</li> <li>InitContainer -&gt; Shell (Generated)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nspec:\nresources:\n- name: source-tester\nsources:\n- name: source\nresource: source-provider\ntemplateRef:\nkind: ClusterSourceTemplate\nname: testing-pipeline-workspace\n</code></pre> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSourceTemplate\nspec:\nytt: ...\n</code></pre> ClusterRunTemplate-tektonsource-pipelinerun.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterRunTemplate\nmetadata:\nannotations:\nkapp.k14s.io/identity: v1;/carto.run/ClusterRunTemplate/tekton-source-pipelinerun;carto.run/v1alpha1\nkapp.k14s.io/original: '{\"apiVersion\":\"carto.run/v1alpha1\",\"kind\":\"ClusterRunTemplate\",\"metadata\":{\"labels\":{\"kapp.k14s.io/app\":\"1683272492779471078\",\"kapp.k14s.io/association\":\"v1.72b4cf08dac7ed1e3cac533f6a62ff76\"},\"name\":\"tekton-source-pipelinerun\"},\"spec\":{\"outputs\":{\"revision\":\"spec.params[?(@.name==\\\"source-revision\\\")].value\",\"url\":\"spec.params[?(@.name==\\\"source-url\\\")].value\"},\"template\":{\"apiVersion\":\"tekton.dev/v1beta1\",\"kind\":\"PipelineRun\",\"metadata\":{\"generateName\":\"$(runnable.metadata.name)$-\",\"labels\":\"$(runnable.metadata.labels)$\"},\"spec\":{\"params\":\"$(runnable.spec.inputs.tekton-params)$\",\"pipelineRef\":{\"name\":\"$(selected.metadata.name)$\"}}}}}'\nkapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3\ncreationTimestamp: \"2023-05-05T07:41:34Z\"\ngeneration: 1\nlabels:\nkapp.k14s.io/app: \"1683272492779471078\"\nkapp.k14s.io/association: v1.72b4cf08dac7ed1e3cac533f6a62ff76\nname: tekton-source-pipelinerun\nresourceVersion: \"10224684\"\nuid: 34c133ce-228d-4f0d-85df-fe60bae905f7\nspec:\noutputs:\nrevision: spec.params[?(@.name==\"source-revision\")].value\nurl: spec.params[?(@.name==\"source-url\")].value\ntemplate:\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: $(runnable.metadata.name)$-\nlabels: $(runnable.metadata.labels)$\nspec:\nparams: $(runnable.spec.inputs.tekton-params)$\npipelineRef:\nname: $(selected.metadata.name)$\n</code></pre>"},{"location":"supply-chain/custom/","title":"Create Supply Chain","text":"","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#questions","title":"Questions","text":"<ul> <li>FluxCD Webhook instead of Polling</li> <li>Polling must die</li> <li>Git metadata (e.g., Git Webhook data)?</li> <li>Other branches, tags, PRs?</li> <li>Handle multiple commits to same revision in sequence?</li> <li>Build caching?</li> <li>Tekton GUI?</li> <li>Carto Live Editor?</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#goals-outcomes","title":"Goals &amp; Outcomes","text":"<ul> <li> I have a basic understanding of Cartographer Supply Chain</li> <li> I have a basic understanding of Tekton</li> <li> I've written and implemented my own Tekton Task</li> <li> I've written and implemented my own Tekton Pipeline</li> <li> I've written and implemented my own Custom Supply Chain on a Cluster</li> <li> I've written and implemented my own Custom Supply Chain on a Cluster, using Tekton</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#steps","title":"Steps","text":"<ul> <li>Cartographer Introduction</li> <li>Tekton Introduction</li> <li>Integrating Tekton into Cartographer</li> <li>Compare with OOTB Supply Chains</li> </ul> <p>We start with exploring the core components of Cartographer. First with hardcoded values, then with some templating involved.</p> <p>Eventually, for a Supply Chain, we need something to run commands in a container. For this, we rely on Tekton.</p> <p>Before we add the Tekton bits to the Cartographer Supply Chain, we take a look a Tekton's core components. We then join the two in a more elaborate Supply Chain.</p> <p>And lastly, you should now be able to read and comprehend the OOTB Supply Chains.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#prerequisites","title":"Prerequisites","text":"<ul> <li>TAP Profile installed which includes Tekton and Cartographer</li> <li>Full, Iterate, or Build</li> <li>TAP Developer Namespace setup</li> <li>for RBAC and Repository Credentials</li> <li>Checkout this repository</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#cartographer-introduction","title":"Cartographer Introduction","text":"<p>The Cartographer docs have a good tutorial1, which we'll follow in condensed form.</p> <p>If you'd rather follow the tutorial from there, the files included in this repository have minor changes, it is recommend to compare them. Just make sure to apply the files into a namespace that is configured as a TAP Developer Namespace.</p> <p>We'll start with a the minimal configuration and then build it out:</p> <ol> <li>My First Supply Chain</li> <li>Include Parameters</li> <li>Git Checkout and Image Build</li> </ol> <p>To construct and use a Supply Chain, we need the following ingredients:</p> <ol> <li>One or more Resources, usually Cartographer templates</li> <li>A SupplyChain definition, using those Resources</li> <li>A Workload definition that selects the Supply Chain</li> </ol>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#my-first-template","title":"My First Template","text":"<p>Relies on Workload Definition</p> <p>Cartographer works from a Workload definition.</p> <p>So any template, like the ClusterTemplate below, works because the trigger is a Workload CR.</p> <p>As the name implies, a ClusterTemplate is a resource that generates another resource. You define that other resource via the <code>spec.template</code> field.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy\nspec:\ntemplate:\n</code></pre> <p>In the case of our example, we will templatize a Deployment.</p> <p>Parameters for the template come from upstream resources, such as the Workload that triggers a Supply Chain. We define the parameter as <code>$(source.propery)$</code>, note the double <code>$</code>.</p> <pre><code>template:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\n</code></pre> <p>A Workload is a namespaced Kubernetes CR, which we can leverage to fill in all required (and unique) fields of a Deployment.</p> ClusterTemplate resources/cartographer/app-deploy-01/01-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy\nspec:\ntemplate:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\nlabels:\napp: $(workload.metadata.name)$\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: $(workload.metadata.name)$\ntemplate:\nmetadata:\nlabels:\napp: $(workload.metadata.name)$\nspec:\ncontainers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\n</code></pre> <p>Note the property <code>spec.image</code>. This is something we must define within the Workload to make this Template work.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#supply-chain-definition","title":"Supply Chain Definition","text":"<p>The supply chain has three top level fields in its spec, the resources, a service account reference and a selector for workloads.</p> <ol> <li>Resources: Which CRs are used (and how)</li> <li>ServiceAccount Reference: reference to a ServiceAccount that can use the Templates/Resources</li> <li>Selector: Set of Kubernetes Labels which you have to specify on a Workload to trigger this Supply Chain</li> </ol> <p>The most minimal Supply Chain contains a single Resource. The Cluster Template we just created is one of those resources.</p> <p>There are five types of Resources2:</p> <ul> <li>ClusterTemplate: instructs the supply chain to instantiate a Kubernetes object that has no outputs to be supplied to other objects in the chain</li> <li>ClusterSourceTemplates: indicates how the supply chain could instantiate an object responsible for providing source code</li> <li>ClusterImageTemplates: instructs how the supply chain should instantiate an object responsible for supplying container images</li> <li>ClusterDeploymentTemplate: indicates how the delivery should configure the environment (namespace/cluster)</li> <li>ClusterConfigTemplates: Instructs the supply chain how to instantiate a Kubernetes object that knows how to make Kubernetes configurations available to further resources in the chain</li> </ul> <p>You can find the details of each resource in the docs3</p> <p>One of the goals of Cartographer is to be the abstraction layer above CI/CD Pipelines. As such, the majority of the resources make the most sense with a Cluster scope.</p> <p>To make the disctintion clear, all Cluster scoped CRs start with <code>Cluster</code>. This includes the Supply Chain, so our empty Supply Chain looks like this:</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: my-first-supply-chain\nspec:\nresources: []\nserviceAccountRef: {}\nselector: {}\n</code></pre> <p>Each resource has a <code>name</code> and <code>templateRef</code> field. And the <code>templateRef</code> field maps to a Kubernetes CR with <code>kind</code> and <code>name</code>.</p> <p>For example, here we map our Cluster Template from earlier:</p> <pre><code>resources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy\n</code></pre> <p>The <code>serviceAccountRef</code> requires the <code>name</code> and <code>namespace</code> of the ServiceAccount Cartographer uses.</p> <p>The <code>selector</code> contains a Kubernetes Label:</p> <pre><code>selector:\nworkload-type: pre-built\n</code></pre> <p>See the complete example below.</p> Supply Chain resources/cartographer/app-deploy-01/02-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: my-first-supply-chain\nspec:\nresources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy\nserviceAccountRef:\nname: cartographer-pre-built-sa\nnamespace: default\nselector:\nworkload-type: pre-built\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#workload-definition","title":"Workload Definition","text":"<p>Remember, we need to specify the <code>spec.image</code> field.</p> <p>And if we want to trigger our Supply Chain, our Workload needs to contain the Label <code>workload-type=pre-built</code>.</p> workload-pre-built-hello.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello\nlabels:\nworkload-type: pre-built\nspec:\nimage: ghcr.io/joostvdg/go-demo:v2.1.16\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#rbac","title":"RBAC","text":"<p>Before we apply the resources to the cluster, we need to make sure our ServiceAccount exists and has the required permissions.</p> ServiceAccount, Roles, and RoleBindings resources/cartographer/rbac.yaml<pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cartographer-pre-built-sa\nnamespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: deploy-image-role\nrules:\n- apiGroups:\n- apps\nresources:\n- deployments\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-prebuilt-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: deploy-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-pre-built-sa\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-1","title":"Excercise 1","text":"<p>We should now have four files, which you can apply to the Cluster and the appropriate Namespace.</p> <pre><code>export DEV_NAMESPACE=dev\n</code></pre> <pre><code>kubectl apply -f resources/cartographer/rbac.yaml -n ${DEV_NAMESPACE}\nkubectl apply -f resources/cartographer/app-deploy-01/01-cluster-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-01/02-supply-chain.yaml\nkuebctl apply -f resources/cartographer/app-deploy-01/03-workload.yaml -n ${DEV_NAMESPACE}\n</code></pre> <p>Once you have applied the resources, the workload should be valid:</p> <pre><code>kubectl get workload -n $DEV_NAMESPACE\n</code></pre> <p>And if you have the <code>kubectl tree</code> plugin, you can see the related resources:</p> <pre><code>kubectl tree workload hello -n ${DEV_NAMESPACE}\n</code></pre> <p>Which should show something like this:</p> <pre><code>NAMESPACE  NAME                                       READY  REASON  AGE\ndefault    Workload/hello                             True   Ready   98m\ndefault    \u2514\u2500Deployment/hello-deployment              -              98m\ndefault      \u2514\u2500ReplicaSet/hello-deployment-cfdf74d6   -              98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-kjjp5  True           98m\ndefault        \u251c\u2500Pod/hello-deployment-cfdf74d6-wgtvl  True           98m\ndefault        \u2514\u2500Pod/hello-deployment-cfdf74d6-x2pmc  True           98m\n</code></pre> <p>To test the application, you can use <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward deployment/hello-deployment -n $DEV_NAMESPACE 8080:8080\n</code></pre> <p>And then Curl:</p> <pre><code>curl http://localhost:8080/\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#adding-additional-parameters","title":"Adding Additional Parameters","text":"<p>A next step is to add more templating to our Supply Chain4.</p> <p>One of the ways we can do this is via additional parameters.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#add-paramters-to-clustertemplate","title":"Add Paramters To ClusterTemplate","text":"<p>We extend the ClusterTemplate from the previous example.</p> <p>I tend the rename updated examples:</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-02\n</code></pre> <p>Parameters are a top level spec resource (<code>params</code>), which you can verify with <code>kubectl explain</code>.</p> <pre><code>kubectl explain ClusterTemplate.spec\n</code></pre> <p>Which lists among other things:</p> <pre><code>params  &lt;[]Object&gt;\n  Additional parameters. See:\n  https://cartographer.sh/docs/latest/architecture/#parameter-hierarchy\n</code></pre> <p>We'll add a single environment variable to our Deployment. For this, we create two <code>params</code> entries:</p> <pre><code>params:\n- name: env_key\ndefault: \"FOO\"\n- name: env_value\ndefault: \"BAR\"\n</code></pre> <p>Which we can use in our <code>spec.template</code> via <code>$(params.&lt;nameOfParam&gt;)$</code>:</p> <pre><code>containers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\nenv:\n- name: $(params.env_key)$\nvalue: $(params.env_value)$\n</code></pre> Updated ClusterTemplate resources/cartographer/app-deploy-02/01-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-02\nspec:\ntemplate:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\nlabels:\napp: $(workload.metadata.name)$\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: $(workload.metadata.name)$\ntemplate:\nmetadata:\nlabels:\napp: $(workload.metadata.name)$\nspec:\ncontainers:\n- name: $(workload.metadata.name)$\nimage: $(workload.spec.image)$\nenv:\n- name: $(params.env_key)$\nvalue: $(params.env_value)$\nparams:\n- name: env_key\ndefault: \"FOO\"\n- name: env_value\ndefault: \"BAR\"\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-supply-chain-to-use-updated-template","title":"Update Supply Chain To Use Updated Template","text":"<p>The parameters we added to the ClusterTemplate need to be supplied by the Workload.</p> <p>So technically we do not have to change our ClusterSupplyChain. I do so anyway, so we can have both Supply Chains in the cluster at the same time.</p> <p>We change the Template Ref:</p> <pre><code>- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-02\n</code></pre> <p>If we want Workloads to select the new Supply Chain, we should also update the Selector:</p> <pre><code>selector:\nworkload-type: pre-built-02\n</code></pre> Updated Supply Chain resources/cartographer/app-deploy-02/02-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: supply-chain-02\nspec:\nresources:\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-02\nserviceAccountRef:\nname: cartographer-pre-built-sa\nnamespace: default\nselector:\nworkload-type: pre-built-02\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#add-parameters-to-workload","title":"Add Parameters To Workload","text":"<p>In the Workload we change a few more things.</p> <p>We rename it, to <code>hello-02</code>, to differentiate from our previous Workload.</p> <p>We update the Label to <code>workload-type: pre-built-02</code> to reflect the updated Supply Chain.</p> <p>And, last but not least, we add the parameters!</p> <p>We do so similarly as we did for the Cluster Template:</p> <pre><code>params:\n- name: env_key\nvalue: \"K_SERVICE\"\n- name: env_value\nvalue: \"carto-hello\"\n</code></pre> <p>Tip</p> <p>For those wondering what this parameter does.</p> <p>When run via Knative Serving (as is done in TAP) the <code>k_SERVICE</code> env variable is printed in the http get on the <code>/</code> endpoint.</p> <p>So setting this, let's us directly see our value in the output.</p> <pre><code>Chart Version: ; Image Version: ; Release: unknown, SemVer: , GitCommit: ,Host: hello-04-deployment-7645c7c549-dpn8v, Revision: , Service: carto-hello-source\n</code></pre> Updated Workload resources/cartographer/app-deploy-02/03-workload.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello-02\nlabels:\nworkload-type: pre-built-02\nspec:\nimage: ghcr.io/joostvdg/go-demo:v2.1.16\nparams:\n- name: env_key\nvalue: \"K_SERVICE\"\n- name: env_value\nvalue: \"carto-hello\"\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-2","title":"Excercise 2","text":"<pre><code>export DEV_NAMESPACE=dev\n</code></pre> <p>Technically, we do not have to update the RBAC configuration, but its included for completeness.</p> <pre><code>kubectl apply -f resources/cartographer/rbac.yaml -n ${DEV_NAMESPACE}\nkubectl apply -f resources/cartographer/app-deploy-02/01-cluster-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-02/02-supply-chain.yaml\nkubectl apply -f resources/cartographer/app-deploy-02/03-workload.yaml  -n ${DEV_NAMESPACE}\n</code></pre> <p>Once you have applied the resources, the workload should be valid:</p> <pre><code>kubectl get workload -n $DEV_NAMESPACE\n</code></pre> <p>To test the application, you can use <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward deployment/hello-02-deployment -n $DEV_NAMESPACE 8080:8080\n</code></pre> <p>And then Curl:</p> <pre><code>curl http://localhost:8080/\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#extending-the-supply-chain","title":"Extending The Supply Chain","text":"<p>So far our Supply Chain deploys our application by generating a Deployment.</p> <p>Now it is time to add more steps to our Supply Chain5, so we, you know, a Chain of steps instead of just one.</p> <p>To stay in that theme, the Deployment needs to be supplied with a (container) Image. So our next goal is to build that Image and supply it to the Deployment.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#add-image-template","title":"Add Image Template","text":"<p>As doing CI/CD in Kubernetes often involves creating Container Images, Cartographer has a first-class support for this.</p> <p>This capability is supplied by the CR ClusterImageTemplate6.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterImageTemplate\nmetadata:\nname: image-builder-01\nspec:\ntemplate: {}\n</code></pre> <p>Cartographer doesn't actually build the image for us, it relies on third party tools. One such tool, which matches really well with Cartographer, is KPack7.</p> KPack Image CR example <pre><code>apiVersion: kpack.io/v1alpha1\nkind: Image\nmetadata:\nname: example-image\nnamespace: default\nspec:\ntag: &lt;DOCKER-IMAGE-TAG&gt;\nserviceAccount: &lt;SERVICE-ACCOUNT&gt;\nbuilder:\nname: &lt;BUILDER&gt;\nkind: Builder\nsource:\ngit:\nurl: &lt;APPLICATION-SOURCE-REPOSITORY&gt;\nrevision: &lt;APPLICATION-SOURCE-REVISION&gt;\n</code></pre> <p>KPack has an Image CR, that requires us the provide the following values:</p> <ul> <li>Tag: the name of the Image to build</li> <li>Builder: a KPack Builder, which is a Kubernetes CR that configures Cloud Native Buildpacks8</li> <li>Source: the source code input for the image building process</li> </ul> <p>The Tag speaks for itself, let's look at the Builder.</p> <p>When installing TAP, it also includes Tanzu Build Service (TBS). TBS installs and configures KPack, and thus we already have a set of KPack Builders available.</p> <p>We can retrieve them as follows:</p> <pre><code>kubectl get clusterbuilder\n</code></pre> <p>Which in my environment returns the following (some values are abbreviated):</p> <pre><code>NAME         LATESTIMAGE                                                    READY\nbase         h.s.h.v.com/b../tbs-full-deps:clusterbuilder-base@s.......85   True\nbase-jammy   h.s.h.v.com/b../tbs-full-deps:clusterbuilder-base-jammy@..70   True\ndefault      h.s.h.v.com/b../tbs-full-deps:clusterbuilder-default@.....57   True\nfull         h.s.h.v.com/b../tbs-full-deps:clusterbuilder-full@........2c   True\nfull-jammy   h.s.h.v.com/b../tbs-full-deps:clusterbuilder-full-jammy@..d0   True\ntiny         h.s.h.v.com/b../tbs-full-deps:clusterbuilder-tiny@........96   True\ntiny-jammy   h.s.h.v.com/b../tbs-full-deps:clusterbuilder-tiny-jammy@..9f   True\n</code></pre> <p>I like small Images (and I cannot lie), so I choose <code>tiny</code>:</p> <pre><code>builder:\nkind: ClusterBuilder\nname: tiny\n</code></pre> <p>Missing API Schema</p> <p>Unfortunately, the KPack CRs do not contain the OpenAPI schema.</p> <p>So we cannot use <code>kubectl explain</code> to explore the schema.</p> <p>For the source, we assume it is a Git repository, and require the Workload to specify it.</p> <pre><code>source:\ngit:\nurl: $(workload.spec.source.git.url)$\nrevision: $(workload.spec.source.git.ref.branch)$\n</code></pre> <p>For the ClusterImageTemplate itself, we need to specify two more things:</p> <ul> <li>params*: an input parameter for the <code>image_prefix</code> for the **Tag</li> <li><code>tag: $(params.image_prefix)$$(workload.metadata.name)$</code></li> <li>imagePath: we need to specify where the template can retrieve the URI of the built Image, to supply it to the next steps</li> </ul> <pre><code>params:\n- name: image_prefix\ndefault: harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/test\nimagePath: .status.latestImage\n</code></pre> Full ClusterImageTemplate resources/cartographer/app-deploy-03/01-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterImageTemplate\nmetadata:\nname: image-builder-01\nspec:\ntemplate:\napiVersion: kpack.io/v1alpha2\nkind: Image\nmetadata:\nname: $(workload.metadata.name)$\nspec:\ntag: $(params.image_prefix)$$(workload.metadata.name)$\nbuilder:\nkind: ClusterBuilder\nname: tiny\nsource:\ngit:\nurl: $(workload.spec.source.git.url)$\nrevision: $(workload.spec.source.git.ref.branch)$\nparams:\n- name: image_prefix\ndefault: harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/test\nimagePath: .status.latestImage\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-cluster-template","title":"Update Cluster Template","text":"<p>As with the other examples, we rename the ClusterTemplate so it does not replace our previous efforts.</p> resources/cartographer/app-deploy-03/01-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-from-sc-image-01\n</code></pre> <p>We want our Image to come from within the Supply Chain, and no longer supplied by our Workload.</p> <p>Later we'll declare in the Supply Chain how and where the Image comes from. For now, let's assume we'll end up with the value supplied in the struct <code>images.built-image.image</code>.</p> <pre><code>containers:\n- name: $(workload.metadata.name)$\nimage: $(images.built-image.image)$\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-rbac","title":"Update RBAC","text":"<p>The SA we use needs permissions to use the KPack resources.</p> <p>So we need to update our RBAC configuration:</p> Updated RBAC Config resources/cartographer/app-deploy-03/00-rbac.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: cartographer-from-source-sa\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: deploy-image-role\nrules:\n- apiGroups:\n- apps\nresources:\n- deployments\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-deploy-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: deploy-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-from-source-sa\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nname: build-image-role\nrules:\n- apiGroups:\n- kpack.io\nresources:\n- images\nverbs:\n- list\n- create\n- update\n- delete\n- patch\n- watch\n- get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\nname: cartographer-build-image-role-binding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: Role\nname: build-image-role\nsubjects:\n- kind: ServiceAccount\nname: cartographer-from-source-sa\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-supply-chain","title":"Update Supply Chain","text":"<p>Oke. so we created a second Cartographer Resource, the ClusterImageTemplate.</p> <p>Let's add it to our Supply Chain.</p> <p>We add it to the list of Resources:</p> <pre><code>resources:\n- name: build-image\ntemplateRef:\nkind: ClusterImageTemplate\nname: image-builder-01\n</code></pre> <p>Our Deployment Template now needs to receive the Image URI from the Supply Chain. So we need to add that information to the Template's Resource definition.</p> <p>In case you don't remember, we assumed it would be defined within <code>images.built-image.image</code>.</p> <p>You can see below how we can express that. We add the <code>images</code> list, and link it to the output <code>built-image</code> from the Resource <code>build-image</code>.</p> <p>The Resource <code>build-image</code> is the Resource name of our ClusterImageTemplate. </p> <pre><code>- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-from-sc-image-01\nimages:\n- resource: build-image\nname: built-image\n</code></pre> <p>Below is the complete updated Supply Chain.</p> Updated Supply Chain resources/cartographer/app-deploy-03/02-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: source-code-supply-chain-01\nspec:\nresources:\n- name: build-image\ntemplateRef:\nkind: ClusterImageTemplate\nname: image-builder-01\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-from-sc-image-01\nimages:\n- resource: build-image\nname: built-image\nserviceAccountRef:\nname: cartographer-from-source-sa\nnamespace: dev\nselector:\nworkload-type: source-code-01\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-workload","title":"Update Workload","text":"<p>As the Image for our Deployment is now generated by the Supply Chain, we remove it from our Workload manifest.</p> <p>Instead, we now need to supply the (Git) Source of our Workload.</p> <pre><code>source:\ngit:\nref:\nbranch: main\nurl: https://github.com/joostvdg/go-demo\n</code></pre> <p>Below is the complete updated Workload manifest:</p> Updated Workload resources/cartographer/app-deploy-03/03-workload.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: source-code-01\nlabels:\nworkload-type: source-code-01\nspec:\nparams:\n- name: env_key\nvalue: \"K_SERVICE\"\n- name: env_value\nvalue: \"carto-hello-source\"\nsource:\ngit:\nref:\nbranch: main\nurl: https://github.com/joostvdg/go-demo\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-3","title":"Excercise 3","text":"<pre><code>export DEV_NAMESPACE=dev\n</code></pre> <p>We can now apply all the Resources to the Cluster:</p> <pre><code>kubectl apply -f resources/cartographer/app-deploy-03/00-rbac.yaml -n ${DEV_NAMESPACE}\nkubectl apply -f resources/cartographer/app-deploy-03/01-cluster-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-03/02-supply-chain.yaml\nkubectl apply -f resources/cartographer/app-deploy-03/03-workload.yaml  -n ${DEV_NAMESPACE}\n</code></pre> <p>Info</p> <p>The <code>01-cluster-template.yaml</code> file contains both the ClusterTemplate and the ClusterImageTemplate.</p> <p>Once you have applied the resources, the workload should be valid:</p> <pre><code>kubectl get workload -n $DEV_NAMESPACE\n</code></pre> <p>Because we're now doing a build, you might want to follow allong with the logs:</p> <pre><code>tanzu apps workload tail source-code-01 --namespace dev --timestamp --since 1h\n</code></pre> <p>To test the application, you can use <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward deployment/source-code-01-deployment -n $DEV_NAMESPACE 8080:8080\n</code></pre> <p>And then Curl:</p> <pre><code>curl http://localhost:8080/\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-introduction","title":"Tekton Introduction","text":"<p>At some point a Supply Chain needs to perform actions that are specific to your application or tech stack.</p> <p>Usually by means of running a particular Container with some shell commands.</p> <p>While you could use the ClusterTemplate to run a Pod with specific commands, there is a better tool for the job: Tekton9</p> <p>Tekton is an open-source cloud native CICD (Continuous Integration and Continuous Delivery/Deployment) solution</p> <p>We'll examine the core resources of Tekton and how to use them. So that later we can include them in our Cartographer Supply Chain.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#core-resources","title":"Core Resources","text":"<p>For those new to Tekton, here is a brief explanation of the core resources:</p> <ul> <li>Step: definition of a command to execute in a container image, part of the Task definition</li> <li>Task: template for a series of Steps with Workspaces, Outputs and (input) Parameters, more on those below</li> <li>TaskRun: a one-time runtime instantiation of a Task with its Workspaces and Parameters defined</li> <li>Pipeline: a template for a series of Tasks, with Workspaces and Parameters</li> <li>PipelineRun: a one-time runtime instantiation of a Pipeline with its Workspaces and Parameters defined</li> <li>Workspace: storage definition, essentially Kubernetes Volume definitions, conceptually in Task and Pipeline, specified in TaskRun and PipelineRun</li> <li>Results: Tasks can have outputs, named Results, which is a way of exporting information from a Task into the Pipeline, so it can be used by other Tasks</li> <li>Params: input parameters for a Task</li> </ul>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#task-taskrun","title":"Task &amp; TaskRun","text":"<p>A Tekton Task is a Kubernetes CR that contains one or more Steps, and lot of other optional configuration10.</p> <p>We will ignore most the optional configuration for now, and focus on the Steps.</p> <p>A Step has a <code>name</code>, <code>image</code>, and then either <code>args</code> and/or a <code>command</code> or a <code>script</code>, depending on the Image used.</p> <p>Below is an example of a Task using an Image to execute a shell command:</p> resources/tekton/task/01-task-hello.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: hello\nspec:\nsteps:\n- name: echo\nimage: harbor.services.h2o-2-9349.h2o.vmware.com/library/alpine@sha256:c0669ef34cdc14332c0f1ab0c2c01acb91d96014b172f1a76f3a39e63d1f0bda\nscript: |\n#!/bin/sh\necho \"Hello World\"\n</code></pre> <p>You can add this to your cluster as follows:</p> <pre><code>kubectl apply -f resources/tekton/task/01-task-hello.yaml -n $DEV_NAMESPACE\n</code></pre> <p>You can verify it exists, by running:</p> <pre><code>kubectl get task -n $DEV_NAMESPACE\n</code></pre> <p>Which returns something like this:</p> <pre><code>NAME            AGE\nhello           24h\n</code></pre> <p>You might wonder, now what?</p> <p>Nothing, a Task is a template, it doesn't do anything by itself.</p> <p>For that we need either a PipelineRun (when the Task is part of a Pipeline) or a TaskRun.</p> <p>A TaskRun refers to a Task, satisfies its requirements and then instantiates a version of that Task.</p> <p>In our case, a TaskRun looks like this:</p> resources/tekton/task/02-task-run.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\nname: hello-task-run\nspec:\ntaskRef:\nname: hello\n</code></pre> <p>We can add this to the cluster as follows:</p> <pre><code>kubectl apply -f resources/tekton/task/02-task-run.yaml -n $DEV_NAMESPACE\n</code></pre> <p>And then verify its status:</p> <pre><code>kubectl get taskrun -n $DEV_NAMESPACE\n</code></pre> <p>Which initially returns this:</p> <pre><code>NAME              SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-task-run    Unknown     Pending     5s\n</code></pre> <p>And then this:</p> <pre><code>NAME              SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-task-run    True        Succeeded   53s         47s\n</code></pre> <p>Because our TaskRun has a fixed name, we can loot at the Pod and request the logs of the container. The container is named after the step, so our step <code>echo</code> becomes <code>step-echo</code>:</p> <pre><code>kubectl -n ${DEV_NAMESPACE} logs hello-task-run-pod -c step-echo\n</code></pre> <p>Which should return the following:</p> <pre><code>Hello World\n</code></pre> <p>Feel free to clean them up:</p> <pre><code>kubectl delete -f resources/tekton/task/02-task-run.yaml -n $DEV_NAMESPACE\nkubectl delete -f resources/tekton/task/01-task-hello.yaml -n $DEV_NAMESPACE\n</code></pre> <p>Let us look at Pipeline and PipelineRun next.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#pipeline-pipelinerun","title":"Pipeline &amp; PipelineRun","text":"<p>Having a single Task with a fixed named TaskRun is a bit limiting.</p> <p>If you need to support more flows, workspaces, some logic, or want to re-use existing Tasks you need a Pipeline.</p> <p>A Tekton Pipeline11 is a collection of Tasks with inputs, outputs, workspaces and built-in logic for CI/CD workflows.</p> <p>We won't go into too much detail, but feel free to explore the options a Pipeline offers11.</p> <p>For now, we'll stick the ability to combine a series of Tasks and supply them with their requirements (e.g., input parameters).</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: hello-goodbye\nspec:\nparams: []\ntasks: []\n</code></pre> <p>That's our Pipeline skeleton, let us add some tasks to it:</p> <pre><code>spec:\ntasks:\n- name: hello\ntaskRef:\nname: hello\n- name: goodbye\nrunAfter:\n- hello\ntaskRef:\nname: goodbye\nparams:\n- name: username\nvalue: $(params.username)\n</code></pre> <p>As you can see, we can re-use our <code>hello</code> Task. And there's is a new Task, called <code>goodbye</code>.</p> <p>There are two new things here:</p> <ul> <li><code>runAfter</code>: Tasks might rely on outputs (Results) from other tasks, or have a logical sequential order, so we can specify Task B runs after Task A completed (successfully)</li> <li><code>params</code>: our second Task, <code>goodbye</code>, requires an input parameter named <code>username</code></li> </ul> <p>As you can see in the <code>params</code> section of the <code>goodbye</code> Task, we supply it a value from the Pipeline <code>params</code> object. Let's add that to our Pipeline:</p> <pre><code>spec:\nparams:\n- name: username\ntype: string\n</code></pre> <p>Our Pipeline now looks like this:</p> resources/tekton/pipeline/03-pipeline.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: hello-goodbye\nspec:\nparams:\n- name: username\ntype: string\ntasks:\n- name: hello\ntaskRef:\nname: hello\n- name: goodbye\nrunAfter:\n- hello\ntaskRef:\nname: goodbye\nparams:\n- name: username\nvalue: $(params.username)\n</code></pre> <p>We can add our two Tasks and the Pipeline to the cluster, and Tekton verifies all Tasks are accounted for.</p> <pre><code>kubectl apply -f resources/tekton/pipeline/01-task-hello.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline/02-task-goodbye.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline/03-pipeline.yaml -n $DEV_NAMESPACE\n</code></pre> <p>And verify the Pipeline is healthy:</p> <pre><code>kubectl get pipeline -n $DEV_NAMESPACE\n</code></pre> <p>Which should result in:</p> <pre><code>NAME                 AGE\nhello-goodbye        24h\n</code></pre> <p>Now that we have a Pipeline, we have to Run it. It is probably not a surpise we do this via a PipelineRun.</p> <p>Ignoring all the other bells and whistles Tekton offers, instantiating a Pipeline is straightforward. We create a PipelineRun manifest, reference the Pipeline and supply it its requirements (e.g., <code>params</code>).</p> resources/tekton/pipeline/04-pipeline-run-static.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\nname: hello-goodbye-run\nspec:\npipelineRef:\nname: hello-goodbye\nparams:\n- name: username\nvalue: \"Tekton\"\n</code></pre> <p>As you can see, we refer to our Pipeline via <code>pipelineRef.name</code>. And we supply the input parameters via <code>params</code>.</p> <p>Let's apply this to the cluster, and initiate our first Pipeline Run.</p> <pre><code>kubectl apply \\\n-f resources/tekton/pipeline/04-pipeline-run-static.yaml \\\n-n $DEV_NAMESPACE\n</code></pre> <p>And verify the state:</p> <pre><code>kubectl get pipelinerun -n $DEV_NAMESPACE\n</code></pre> <p>Which should yield something like this at first:</p> <pre><code>NAME                           SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-goodbye-run              Unknown     Running     6s\n</code></pre> <p>And then:</p> <pre><code>NAME                           SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-goodbye-run              True        Succeeded   64s         45s\n</code></pre> <p>And if we look for the TaskRuns:</p> <pre><code>kubectl get taskrun -n $DEV_NAMESPACE\n</code></pre> <p>We should see the following</p> <pre><code>NAME                           SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\nhello-goodbye-run-goodbye      True        Succeeded   76s         63s\nhello-goodbye-run-hello        True        Succeeded   82s         76s\n</code></pre> <p>There is a downside to running Pipelines like this, we can not have more than one Run.</p> <p>The simple solution to this, is the replace the <code>metadata.name</code> property by <code>metadata.generateName</code>. Where the convention is to end in a <code>-</code>, so a generated hash is included.</p> resources/tekton/pipeline/05-pipeline-run-dynamic.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: hello-goodbye-run-\nspec:\npipelineRef:\nname: hello-goodbye\nparams:\n- name: username\nvalue: \"Tekton\"\n</code></pre> <p>We cannot apply this resource to the cluster, we have to use <code>kubectl create</code> instead (because of the <code>generatedName</code>):</p> <pre><code>kubectl create -f  resources/tekton/pipeline/05-pipeline-run-dynamic.yaml -n $DEV_NAMESPACE\n</code></pre> <p>If we now look at the PipelineRun and TaskRuns:</p> <pre><code>kubectl get taskrun,pipelinerun -n $DEV_NAMESPACE\n</code></pre> <p>We get the following:</p> <pre><code>NAME                                                            SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\ntaskrun.tekton.dev/hello-goodbye-run-goodbye                    True        Succeeded   6m54s       6m41s\ntaskrun.tekton.dev/hello-goodbye-run-hello                      True        Succeeded   7m          6m54s\ntaskrun.tekton.dev/hello-goodbye-run-vhswg-goodbye              True        Succeeded   29s         21s\ntaskrun.tekton.dev/hello-goodbye-run-vhswg-hello                True        Succeeded   36s         29s\n\nNAME                                                  SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\npipelinerun.tekton.dev/hello-goodbye-run              True        Succeeded   7m          6m41s\npipelinerun.tekton.dev/hello-goodbye-run-vhswg        True        Succeeded   36s         21s\n</code></pre> <p>The Run objects now include a hash in their name, allowing more than one to exists at once.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-pipeline-with-workspace","title":"Tekton Pipeline With Workspace","text":"<p>One common thing CI/CD Pipelines share, is the need to share data between Tasks. For example, you might have a Git Clone task to collect your source code, and then re-use that in the rest of the pipeline.</p> <p>Before going into how we work with Workspaces, let's highlight another concept. Tasks are re-usable templates, as such, there is a large collection of them maintained by the community. This is called the Tekton Catalog12, in which you'll find common tasks such a GitClone, building an Image with Kaniko and so on.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#setting-up-the-tasks","title":"Setting Up The Tasks","text":"<p>We will re-use the existing GitClone Task13, although with one minor modification.</p> <p>Which is included in the resources:</p> <pre><code>kubectl apply \\\n-f resources/tekton/pipeline-w-workspace/02-task-git-clone-0.10.yaml \\\n-n $DEV_NAMESPACE\n</code></pre> <p>This Task let's us Clone a Git repository, with practically all common Git clone configuration options.</p> <p>It defines a list of Workspaces:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: git-clone\nspec:\nworkspaces:\n- name: output\ndescription: The git repo will be cloned onto the volume backing this Workspace.\n- name: ssh-directory\noptional: true\ndescription: |\nA .ssh directory with private key, known_hosts, config, etc. Copied to\nthe user's home before git commands are executed. Used to authenticate\nwith the git remote when performing the clone. Binding a Secret to this\nWorkspace is strongly recommended over other volume types.\n- name: basic-auth\noptional: true\ndescription: |\nA Workspace containing a .gitconfig and .git-credentials file. These\nwill be copied to the user's home before any git commands are run. Any\nother files in this Workspace are ignored. It is strongly recommended\nto use ssh-directory over basic-auth whenever possible and to bind a\nSecret to this Workspace over other volume types.\n- name: ssl-ca-directory\noptional: true\ndescription: |\nA workspace containing CA certificates, this will be used by Git to\nverify the peer with when fetching or pushing over HTTPS.\n</code></pre> <p>Most of which have <code>optional: true</code>, except for <code>output</code>. Which means we'll need to supply that in our Pipeline later.</p> <p>It also has many paramaters, of which <code>url</code> and <code>revision</code> are required. We'll need to supply those as well.</p> <p>Next up is a Task that uses this Git checkout, to determine the next Git tag (e.g., application's release version).</p> <pre><code>kubectl apply -f resources/tekton/pipeline-w-workspace/01-task-git-next-tag.yaml -n $DEV_NAMESPACE\n</code></pre> <p>This task also required a Workspace:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: git-next-tag\nspec:\nworkspaces:\n- name: source\n</code></pre> <p>It requires one parameter, <code>base</code>, which is the SemVer base (e.g., <code>1.0.*</code>).</p> <p>And it provided an Output:</p> <pre><code>spec:\nresults:\n- name: NEXT_TAG\ndescription: Next version for Git Tag.\n</code></pre> <p>We'll look at this result later.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#creating-the-pipeline-with-workspace","title":"Creating the Pipeline with Workspace","text":"<p>So far, our two Tasks have a \"shopping list\" of required items:</p> <ul> <li>Workspace (<code>source</code> for git-next-tag, and <code>output</code> for git-clone)</li> <li>Parameters: (Git) <code>url</code>, (Git) <code>revision</code>, and (SemVer Tag) <code>base</code></li> </ul> <p>This is how our Pipeline looks like, satisfying those requirements:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: clone-git-next-tag\nspec:\nparams:\n- name: repo-url\ntype: string\n- name: base\ntype: string\n- name: gitrevision\nworkspaces:\n- name: shared-data\n</code></pre> <p>Next, we add the Tasks and configure their requirements via these properties:</p> <pre><code>spec:\ntasks:\n- name: fetch-source\ntaskRef:\nname: git-clone\nworkspaces:\n- name: output\nworkspace: shared-data\nparams:\n- name: url\nvalue: $(params.repo-url)\n- name: revision\nvalue: $(params.gitrevision)\n- name: git-next-tag\nrunAfter: [\"fetch-source\"]\ntaskRef:\nname: git-next-tag\nworkspaces:\n- name: source\nworkspace: shared-data\nparams:\n- name: base\nvalue: $(params.base)\n</code></pre> <p>As you might have spotted, we re-use the Workspace. Supplying the same single Workspace we defined in the Pipeline to both Tasks!</p> <p>This is intended, else we can't re-use our Git clone.</p> <p>Apply the Pipeline to the cluster.</p> <pre><code>kubectl apply \\\n-f resources/tekton/pipeline-w-workspace/03-pipeline.yaml \\\n-n $DEV_NAMESPACE\n</code></pre> Complete Pipeline Example resources/tekton/pipeline-w-workspace/03-pipeline.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: clone-git-next-tag\nspec:\ndescription: |\nThis pipeline clones a git repo, then echoes the README file to the stdout.\nparams:\n- name: repo-url\ntype: string\ndescription: The git repo URL to clone from.\n- name: base\ndescription: version Base to query Git tags for (e.g., v2.1.*)\ntype: string\n- name: gitrevision\ndescription: git revision to checkout\nworkspaces:\n- name: shared-data\ndescription: |\nThis workspace contains the cloned repo files, so they can be read by the\nnext task.\ntasks:\n- name: fetch-source\ntaskRef:\nname: git-clone\nworkspaces:\n- name: output\nworkspace: shared-data\nparams:\n- name: url\nvalue: $(params.repo-url)\n- name: revision\nvalue: $(params.gitrevision)\n- name: git-next-tag\nrunAfter: [\"fetch-source\"]\ntaskRef:\nname: git-next-tag\nworkspaces:\n- name: source\nworkspace: shared-data\nparams:\n- name: base\nvalue: $(params.base)\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#create-the-pipelinerun","title":"Create the PipelineRun","text":"<p>To instantiate our Pipeline, we'll create a PipelineRun.</p> <p>In this PipelineRun, we need to do the following:</p> <ul> <li>reference the Pipeline</li> <li>supply a Workspace</li> <li>supply the Parameters</li> </ul> <p>To make it re-usable, we'll start with a <code>generateName</code> style:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: clone-git-next-tag-run-\n</code></pre> <p>To ensure all containers can read the filesystem of the volume, we set a specific <code>fsGroup</code>:</p> <pre><code>spec:\n  podTemplate:\n    securityContext:\n      fsGroup: 65532\n</code></pre> <p>And then we create a Workspace via a <code>volumeClaimTemplate</code>:</p> <pre><code>spec:\nworkspaces:\n- name: shared-data\nvolumeClaimTemplate:\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 50Mi\nvolumeMode: Filesystem\n</code></pre> <p>Note, the name <code>shared-data</code> is the name specified in the Pipeline. The Pipeline definition ensures each Task gets it supplied as however it named it.</p> <p>And the parameters:</p> <pre><code>spec:\nparams:\n- name: repo-url\nvalue: https://github.com/joostvdg/go-demo.git\n- name: base\nvalue: \"v2.1\"\n- name: gitrevision\nvalue: main\n</code></pre> Full PipelineRun Example <pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: clone-git-next-tag-run-\nspec:\npipelineRef:\nname: clone-git-next-tag\npodTemplate:\nsecurityContext:\nfsGroup: 65532\nworkspaces:\n- name: shared-data\nvolumeClaimTemplate:\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 50Mi\nvolumeMode: Filesystem\nparams:\n- name: repo-url\nvalue: https://github.com/joostvdg/go-demo.git\n- name: base\nvalue: \"v2.1\"\n- name: gitrevision\nvalue: main\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#excercise-4","title":"Excercise 4","text":"<pre><code>export DEV_NAMESPACE=dev\n</code></pre> <p>Just in case you haven't applied all files yet, here's the whole list again:</p> <pre><code>kubectl apply -f resources/tekton/pipeline-w-workspace/02-task-git-clone-0.10.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline-w-workspace/01-task-git-next-tag.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/tekton/pipeline-w-workspace/03-pipeline.yaml -n $DEV_NAMESPACE\n</code></pre> <p>Verify the Pipeline is valid, and then create the PipelineRun:</p> <pre><code>kubectl create \\\n-f resources/tekton/pipeline-w-workspace/04-pipeline-run.yaml \\\n-n $DEV_NAMESPACE\n</code></pre> <p>Once created, you can verify the status:</p> <pre><code>kubectl get taskrun,pipelinerun -n $DEV_NAMESPACE\n</code></pre> <p>If you want the logs, you'll now have to find the appropriate Pod name, as its name is generated.</p> <pre><code>kubectl get pod  -n ${DEV_NAMESPACE}\n</code></pre> <pre><code>POD_NAME=\n</code></pre> <pre><code>kubectl -n ${DEV_NAMESPACE} logs ${POD_NAME}\n</code></pre> <p>You can also use the (automatic) Labels to query them:</p> <pre><code>kubectl get taskrun -n dev -l tekton.dev/task=git-next-tag\n</code></pre> <p>And then you can find the output of the <code>Result</code> from the <code>git-next-tag</code> Task in its <code>status.taskResults</code> field:</p> <pre><code>kubectl get taskrun -n dev \\\n-l tekton.dev/task=git-next-tag -ojson \\\n| jq '.items | map(.status.taskResults)'\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-in-supply-chain","title":"Tekton In Supply Chain","text":"<p>Now that we know how to build a Cartographer Supply Chain and a Tekton Pipeline, let's combine the two!</p> <p>As with the other steps, we're reusing an existing tutorial14.</p> <p>The goal is to create a Supply Chain that validates source code, then builds an Image and then deploys that Image.</p> <p>It is in the Source Code Validation step we'll use Tekton.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-task-markdown-lint","title":"Tekton Task Markdown Lint","text":"<p>The Markdown Lint Task is available in the Tekton Catalog15, and only needs a Workspace to do its work.</p> <pre><code>workspaces:\n- name: shared-workspace\ndescription: A workspace that contains the fetched git repository.\n</code></pre> <p>The task doesn't contain anything new, so we'll go on to the Pipeline.</p> Markdown Lint Task resources/cartographer/app-deploy-04/01-tekton-task-markdown-lint-0.1.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: markdown-lint\nlabels:\napp.kubernetes.io/version: \"0.1\"\nannotations:\ntekton.dev/pipelines.minVersion: \"0.12.1\"\ntekton.dev/categories: Code Quality\ntekton.dev/tags: linter\ntekton.dev/displayName: \"Markdown linter\"\ntekton.dev/platforms: \"linux/amd64\"\nspec:\ndescription: &gt;-\nThis task can be used to perform lint check on Markdown files\nworkspaces:\n- name: shared-workspace\ndescription: A workspace that contains the fetched git repository.\nparams:\n- name: args\ntype: array\ndescription: extra args needs to append\ndefault: [\"--help\"]\nsteps:\n- name: lint-markdown-files\nimage: harbor.services.h2o-2-9349.h2o.vmware.com/other/markdownlint@sha256:399a199c92f89f42cf3a0a1159bd86ca5cdc293fcfd39f87c0669ddee9767724 #tag: 0.11.0\nworkingDir: $(workspaces.shared-workspace.path)\ncommand:\n- mdl\nargs:\n- $(params.args)\n</code></pre> <p>Info</p> <p>You might be wondering: \"If they are all available in the Catalog, why not use them from there?\".</p> <p>Which is a valid question to ask.  The answer: the images used are relocated and the Task in the resources uses that relocated Image.</p>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-pipeline","title":"Tekton Pipeline","text":"<p>The Pipeline uses two tasks. The Markdown Lint15 Task and the Git Clone16 Task. The tutorial uses Git Clone version <code>0.3</code>, so we've included that in the Resources.</p> <p>As the Markdown Lint task requires the Source Code, it has to run after the Git Clone (i.e., <code>fetch-repository</code>) Task.</p> <pre><code>spec:\ntasks:\n- name: fetch-repository\ntaskRef:\nname: git-clone\n- name: md-lint-run\ntaskRef:\nname: markdown-lint\nrunAfter:\n- fetch-repository\n</code></pre> <p>As with our previous Pipeline, we need to supply the Git Repository URL and the (Git) Revision. So we specify appropriate <code>params</code> at <code>spec.params</code> for the Pipeline.</p> <p>We also have to specify a Workspace, which is then given to both tasks.</p> <pre><code>spec:\nparams:\n- name: repository\ntype: string\n- name: revision\ntype: string\nworkspaces:\n- name: shared-workspace\n</code></pre> <p>We then make sure the Workspace and the Parameters are applied to the Tasks.</p> <p>See the complete example below.</p> Complete Pipeline Example resources/cartographer/app-deploy-04/02-tekton-pipeline-markdown-lint.yaml<pre><code>apiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\nname: linter-pipeline\nspec:\nparams:\n- name: repository\ntype: string\n- name: revision\ntype: string\nworkspaces:\n- name: shared-workspace\ntasks:\n- name: fetch-repository\ntaskRef:\nname: git-clone\nworkspaces:\n- name: output\nworkspace: shared-workspace\nparams:\n- name: url\nvalue: $(params.repository)\n- name: revision\nvalue: $(params.revision)\n- name: subdirectory\nvalue: \"\"\n- name: deleteExisting\nvalue: \"true\"\n- name: md-lint-run #lint markdown\ntaskRef:\nname: markdown-lint\nrunAfter:\n- fetch-repository\nworkspaces:\n- name: shared-workspace\nworkspace: shared-workspace\nparams:\n- name: args\nvalue: [\".\"]\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#tekton-pipelinerun-in-cartographer","title":"Tekton PipelineRun in Cartographer","text":"<p>For Cartographer to instantiate a Tekton Pipeline, it needs to produce a PipelineRun.</p> <p>As you probably expect by now, we use one of the Template types of Cartographer.</p> <p>Perhaps a bit counterintuitive, but as the Pipeline does a Git checkout we use the <code>ClusterSourceTemplate</code>.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSourceTemplate\nmetadata:\nname: source-linter\nspec:\ntemplate: {}\n</code></pre> <p>The Supply Chain runs more than once. In order for that to work well with the Tekton resources we need to do the following:</p> <ul> <li>use the <code>metadata.generateName</code> way of naming the Tekton resources</li> <li>set the ClusterSourceTemplate's <code>lifecycle</code> property to <code>tekton</code></li> </ul> <pre><code>spec:\nlifecycle: tekton\n</code></pre> <p>In the <code>spec.template</code>, we write the PipelineRun.</p> <pre><code>spec:\ntemplate:\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: linter-pipeline-run-\nspec:\npipelineRef:\nname: linter-pipeline\n</code></pre> <p>Our Pipeline requires a Workspace, which we'll define in the way we've done before:</p> <pre><code>spec:\ntemplate:\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nspec:\nworkspaces:\n- name: shared-workspace\nvolumeClaimTemplate:\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 256Mi\n</code></pre> <p>The Pipeline also requires two parameters. The Git Repository URL (<code>repository</code>) and Revision (<code>revision</code>).</p> <pre><code>spec:\ntemplate:\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nspec:\nparams:\n- name: repository\nvalue: $(workload.spec.source.git.url)$\n- name: revision\nvalue: $(workload.spec.source.git.ref.branch)$\n</code></pre> <p>Here there is a change.</p> <p>We specify the Parameters so they are copied from the Workload.</p> <p>The ClusterSourceTemplate also expects to ouput the Source URL and Revision. Which also allows us to show how you retrieve values from the Tekton PipelineRun.</p> <pre><code>spec:\nurlPath: .status.pipelineSpec.tasks[0].params[0].value\nrevisionPath: .status.pipelineSpec.tasks[0].params[1].value\n</code></pre> <p>That is our ClusterSourceTemplate.</p> Complete ClusterSourceTemplate resources/cartographer/app-deploy-04/03-cluster-source-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSourceTemplate\nmetadata:\nname: source-linter\nspec:\ntemplate:\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\ngenerateName: linter-pipeline-run-\nspec:\npipelineRef:\nname: linter-pipeline\nparams:\n- name: repository\nvalue: $(workload.spec.source.git.url)$\n- name: revision\nvalue: $(workload.spec.source.git.ref.branch)$\nworkspaces:\n- name: shared-workspace\nvolumeClaimTemplate:\nspec:\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 256Mi\nurlPath: .status.pipelineSpec.tasks[0].params[0].value\nrevisionPath: .status.pipelineSpec.tasks[0].params[1].value\nlifecycle: tekton\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#image-building-templates","title":"Image Building Templates","text":"<p>As we're extending the previous Supply Chain, we're reusing the templates from before. Namely, the ClusterTemplate and the ClusterImageTemplate.</p> <p>As you probably expect from me, I've renamed them, so they can be used next to the other ones.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-from-sc-image-04\n</code></pre> <p>And:</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterImageTemplate\nmetadata:\nname: image-builder-04\n</code></pre> <p>For the rest they are the same.</p> Complete Templates resources/cartographer/app-deploy-04/04-cluster-template.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterTemplate\nmetadata:\nname: app-deploy-from-sc-image-04\nspec:\ntemplate:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: $(workload.metadata.name)$-deployment\nlabels:\napp: $(workload.metadata.name)$\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: $(workload.metadata.name)$\ntemplate:\nmetadata:\nlabels:\napp: $(workload.metadata.name)$\nspec:\ncontainers:\n- name: $(workload.metadata.name)$\nimage: $(images.built-image.image)$\nenv:\n- name: $(params.env_key)$\nvalue: $(params.env_value)$\nparams:\n- name: env_key\ndefault: \"FOO\"\n- name: env_value\ndefault: \"BAR\"\n---\napiVersion: carto.run/v1alpha1\nkind: ClusterImageTemplate\nmetadata:\nname: image-builder-04\nspec:\ntemplate:\napiVersion: kpack.io/v1alpha2\nkind: Image\nmetadata:\nname: $(workload.metadata.name)$\nspec:\ntag: $(params.image_prefix)$$(workload.metadata.name)$\nbuilder:\nkind: ClusterBuilder\nname: tiny\nsource:\ngit:\nurl: $(workload.spec.source.git.url)$\nrevision: $(workload.spec.source.git.ref.branch)$\nparams:\n- name: image_prefix\ndefault: harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/test\nimagePath: .status.latestImage\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-supply-chain_1","title":"Update Supply Chain","text":"<p>We're almost there, let's update (and rename) the Supply Chain.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: source-code-supply-chain-04\nspec:\nselector:\nworkload-type: source-code-04\n</code></pre> <p>We now have three resources:</p> <ul> <li>ClusterSourceTemplate: the Tekton PipelineRun</li> <li>ClusterImageTemplate: our KPack Image build</li> <li>ClusterTemplate: our deployment</li> </ul> <pre><code>  resources:\n- name: lint-source\ntemplateRef:\nkind: ClusterSourceTemplate\nname: source-linter\n- name: build-image\ntemplateRef:\nkind: ClusterImageTemplate\nname: image-builder-04\nsources:\n- resource: lint-source\nname: source\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-from-sc-image-04\nimages:\n- resource: build-image\nname: built-image\n</code></pre> <p>We also need to update the permission for our SA, wich we'll do next. The SA config itself has not changed.</p> <pre><code>spec:\nserviceAccountRef:\nname: cartographer-from-source-sa\nnamespace: dev\n</code></pre> Complete Supply Chain resources/cartographer/app-deploy-04/05-supply-chain.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: ClusterSupplyChain\nmetadata:\nname: source-code-supply-chain-04\nspec:\nselector:\nworkload-type: source-code-04\nresources:\n- name: lint-source\ntemplateRef:\nkind: ClusterSourceTemplate\nname: source-linter\n- name: build-image\ntemplateRef:\nkind: ClusterImageTemplate\nname: image-builder-04\nsources:\n- resource: lint-source\nname: source\n- name: deploy\ntemplateRef:\nkind: ClusterTemplate\nname: app-deploy-from-sc-image-04\nimages:\n- resource: build-image\nname: built-image\nserviceAccountRef:\nname: cartographer-from-source-sa\nnamespace: dev\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#update-workload_1","title":"Update Workload","text":"<p>The only change to the Workload is the name.</p> <pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello-04\nlabels:\nworkload-type: source-code-04\n</code></pre> Updated Workload resources/cartographer/app-deploy-04/06-workload.yaml<pre><code>apiVersion: carto.run/v1alpha1\nkind: Workload\nmetadata:\nname: hello-04\nlabels:\nworkload-type: source-code-04\nspec:\nparams:\n- name: env_key\nvalue: \"K_SERVICE\"\n- name: env_value\nvalue: \"carto-hello-source\"\nsource:\ngit:\nref:\nbranch: main\nurl: https://github.com/joostvdg/go-demo\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#exercise-5","title":"Exercise 5","text":"<pre><code>export DEV_NAMESPACE=dev\n</code></pre> <p>Just in case you haven't applied all files yet, here's the whole list again:</p> <pre><code>kubectl apply -f resources/cartographer/app-deploy-04/00-rbac.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/cartographer/app-deploy-04/01-tekton-task-git-clone-0.3.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/cartographer/app-deploy-04/01-tekton-task-markdown-lint-0.1.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/cartographer/app-deploy-04/02-tekton-pipeline-markdown-lint.yaml -n $DEV_NAMESPACE\nkubectl apply -f resources/cartographer/app-deploy-04/03-cluster-source-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-04/04-cluster-template.yaml\nkubectl apply -f resources/cartographer/app-deploy-04/05-supply-chain.yaml\nkubectl apply -f resources/cartographer/app-deploy-04/06-workload.yaml -n $DEV_NAMESPACE\n</code></pre> <p>Once created, you can verify the status:</p> <pre><code>kubectl get workload -n $DEV_NAMESPACE\n</code></pre> <p>As there's a few things going on, we'll see the Workload have several different statusses.</p> <p>For example:</p> <pre><code>NAME             SOURCE                               SUPPLYCHAIN                   READY   REASON                                                AGE\nhello-04         https://github.com/joostvdg/go-demo  source-code-supply-chain-04   False   SetOfImmutableStampedObjectsIncludesNoHealthyObject   11s\n</code></pre> <p>The <code>Unknown</code> status with Reason <code>MissingValueAtPath</code> is quite common, it usually means a Resource B expects a value from another Resource A. While the Resource A is not finished with its work, Resource B cannot read the value and thus reports <code>MissingValueAtPath</code>.</p> <pre><code>NAME             SOURCE                               SUPPLYCHAIN                   READY     REASON                        AGE\nhello-04         https://github.com/joostvdg/go-demo  source-code-supply-chain-04   Unknown   MissingValueAtPath            68s\n</code></pre> <p>Eventually the Workload will be finished successfully.</p> <pre><code>NAME             SOURCE                               SUPPLYCHAIN                   READY     REASON                        AGE\nhello-04         https://github.com/joostvdg/go-demo  source-code-supply-chain-04   True      Ready                         6m2s\n</code></pre> <p>We can then also take a look at the Tekton resources:</p> <pre><code>kubectl get taskrun,pipelinerun -n $DEV_NAMESPACE\n</code></pre> <pre><code>NAME                                                            SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\ntaskrun.tekton.dev/linter-pipeline-run-xn5vg-fetch-repository   True        Succeeded   6m55s       6m40s\ntaskrun.tekton.dev/linter-pipeline-run-xn5vg-md-lint-run        True        Succeeded   6m40s       6m34s\n\nNAME                                                  SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME\npipelinerun.tekton.dev/linter-pipeline-run-xn5vg      True        Succeeded   6m55s       6m34s\n</code></pre> <p>For more details, see the commands from the previous Exercises.</p> <p>Info</p> <p>In case you are wondering how this hierarchy now looks:</p> <pre><code>* ClusterSupplyChain\n  * ClusterSourceTemplate\n    * ClusterRunTemplate\n      * PipelineRun\n        * Task\n          * TaskRun (Generated)\n* Pod (Generated)\n* InitContainer -&gt; Shell (Generated)\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#ootb-pipeline-appendix","title":"OOTB Pipeline Appendix","text":"<p>Source in TAP</p> <p>In TAP, we don't have to clone our sources from Git, we can download them from FluxCD.</p> <p>The way TAP works, it that the trigger for a SupplyChain goes through FluxCD's GitRepository management. So below is a way of codifying that process into a Tekton Task:</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\nname: fluxcd-repo-download\nspec:\nparams:\n- name: source-url\ntype: string\ndescription: |\nthe source url to download the code from, \nin the form of a FluxCD repository checkout .tar.gz\nworkspaces:\n- name: output\ndescription: The git repo will be cloned onto the volume backing this Workspace.\nsteps:\n- name: download-source\nimage: public.ecr.aws/docker/library/gradle:jdk17-focal\nscript: |\n#!/usr/bin/env sh\ncd $(workspaces.output.path)\nwget -qO- $(params.source-url) | tar xvz -m\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/custom/#references","title":"References","text":"<ol> <li> <p>Cartographer - First Supply Chain Tutorial \u21a9</p> </li> <li> <p>Cartographer - Types of Resources \u21a9</p> </li> <li> <p>Cartographer - Resource Definitions \u21a9</p> </li> <li> <p>Cartographer - Parameters Tutorial \u21a9</p> </li> <li> <p>Cartographer - Extend A Supply Chain Tutorial \u21a9</p> </li> <li> <p>Cartographer - ClusterImageTemplate resource spec \u21a9</p> </li> <li> <p>KPack - Kubernetes solution for running Cloud Native Buildpacks \u21a9</p> </li> <li> <p>Cloud Native Build Packs \u21a9</p> </li> <li> <p>Tekton - Open-source cloud native CICD \u21a9</p> </li> <li> <p>Tekton - Task details \u21a9</p> </li> <li> <p>Tekton - Pipeline details \u21a9\u21a9</p> </li> <li> <p>Tekton - Task Catalog \u21a9</p> </li> <li> <p>Tekton Catalog - Git Clone 0.3 \u21a9</p> </li> <li> <p>Cartographer - Supply Chain with Tekton Pipeline \u21a9</p> </li> <li> <p>Tekton Catalog - Markdown Lint 0.1 \u21a9\u21a9</p> </li> <li> <p>Tekton Catalog - Git Clone 0.3 \u21a9</p> </li> <li> <p>Cartographer - Lifecycle Tutorial \u21a9</p> </li> </ol>","tags":["tap","kubernetes","cartographer","tekton","supplychain"]},{"location":"supply-chain/debug/","title":"TAP Debug Supply Chains","text":"","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#debug-workload","title":"Debug Workload","text":"","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#source","title":"Source","text":"<pre><code>kubectl get gitrepo -A\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#workload","title":"Workload","text":"<pre><code>kubectl get workload -A\n</code></pre> <pre><code>tanzu apps workload get smoke-app --namespace dev\n</code></pre>","tags":["tap","kubernetes","cartographer","tekton","debug"]},{"location":"supply-chain/debug/#image-failed","title":"Image Failed","text":"<pre><code>\ud83d\udce1 Overview\n   name:        smoke-app\n   type:        web\n   namespace:   dev\n\n\ud83d\udcbe Source\n   type:     git\n   url:      https://github.com/sample-accelerators/tanzu-java-web-app.git\n   branch:   main\n\n\ud83d\udce6 Supply Chain\n   name:   source-to-url\n\nNAME               READY   HEALTHY   UPDATED   RESOURCE\n   source-provider    True    True      9m49s     gitrepositories.source.toolkit.fluxcd.io/smoke-app\n   image-provider     False   False     9m40s     images.kpack.io/smoke-app\n   config-provider    False   Unknown   9m57s     not found\n   app-config         False   Unknown   9m56s     not found\n   service-bindings   False   Unknown   9m56s     not found\n   api-descriptors    False   Unknown   9m56s     not found\n   config-writer      False   Unknown   9m56s     not found\n\n\ud83d\ude9a Delivery\n   name:   delivery-basic\n\nNAME              READY   HEALTHY   UPDATED   RESOURCE\n   source-provider   False   False     9m45s     imagerepositories.source.apps.tanzu.vmware.com/smoke-app-delivery\n   deployer          False   Unknown   9m49s     not found\n\n\ud83d\udcac Messages\n   Workload [HealthyConditionRule]:   condition status: False, message: Unable to find builder default.\n   Deliverable [HealthyConditionRule]:   Unable to resolve image with tag \"harbor.services.h2o-2-9349.h2o.vmware.com/tap-apps/smoke-app-dev-bundle:ed5561a7-5bfc-45fb-be33-90b2aeb0a9a1\" to a digest: HEAD https://harbor.services.h2o-2-9349.h2o.vmware.com/v2/tap-apps/smoke-app-dev-bundle/manifests/ed5561a7-5bfc-45fb-be33-90b2aeb0a9a1: unexpected status code 404 Not Found (HEAD responses have no body, use GET for details)\n</code></pre> <pre><code>kubectl get ClusterImageTemplate kpack-template -o yaml | yq\n</code></pre> <pre><code>kubectl describe img -n dev smoke-app\n</code></pre> <pre><code>Status:\n  Conditions:\n    Last Transition Time:  2023-05-05T12:07:52Z\n    Message:               Unable to find builder default.\n    Reason:                BuilderNotFound\n    Status:                False\n    Type:                  Ready\n  Observed Generation:     1\nEvents:                    &lt;none&gt;\n</code></pre> <p>This means we're missing Tanzu Buildservice components!</p>","tags":["tap","kubernetes","cartographer","tekton","debug"]}]}